[{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Time Series Class Conversion","text":"time series landscape R vast, deep, complex causing many inconsistencies data attributes formats ultimately making difficult coerce different data structures. zoo xts packages solved number issues dealing various classes (ts, zoo, xts, irts, msts, list goes …). However, packages deal classes data frame, issues conversion tbl time series object classes still present. timetk package provides tools solve issues conversion, maximizing attribute extensibility (required data attributes retained conversion primary time series classes). following tools available coerce retrieve key information: Conversion functions: tk_tbl, tk_ts, tk_xts, tk_zoo, tk_zooreg. functions coerce time-based tibbles tbl main time-series data types xts, zoo, zooreg, ts, maintaining time-based index. Index function: tk_index returns index. argument, timetk_idx = TRUE, time-based index (non-regularized index) forecast objects, models, ts objects returned present. Refer tk_ts() learn non-regularized index persistence conversion process. vignette includes brief case study conversion issues detailed explanation timetk function conversion time-based tbl objects several primary time series classes (xts, zoo, zooreg ts).","code":""},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"prerequisites","dir":"Articles","previous_headings":"","what":"Prerequisites","title":"Time Series Class Conversion","text":"get started, load following packages.","code":"library(dplyr) library(timetk)"},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Time Series Class Conversion","text":"’ll use “Q10” dataset - first ID sample quarterly datasets (see m4_quarterly) M4 Competition. return structure tibble, conducive many popular time series analysis packages including quantmod, TTR, forecast many others.","code":"q10_quarterly <- m4_quarterly %>% dplyr::filter(id == \"Q10\") q10_quarterly ## # A tibble: 59 × 3 ##    id    date       value ##    <fct> <date>     <dbl> ##  1 Q10   2000-01-01 2329  ##  2 Q10   2000-04-01 2350. ##  3 Q10   2000-07-01 2333. ##  4 Q10   2000-10-01 2382. ##  5 Q10   2001-01-01 2383. ##  6 Q10   2001-04-01 2405  ##  7 Q10   2001-07-01 2411  ##  8 Q10   2001-10-01 2428. ##  9 Q10   2002-01-01 2392. ## 10 Q10   2002-04-01 2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"case-study-conversion-issues-with-ts","dir":"Articles","previous_headings":"","what":"Case Study: Conversion issues with ts()","title":"Time Series Class Conversion","text":"ts object class roots stats package many popular packages use time series data structure including popular forecast package. said, ts data structure difficult coerce back forth default contain time-based index. Rather uses regularized index computed using start frequency arguments. Conversion ts done using ts() function stats library, results various problems.","code":""},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"problems","dir":"Articles","previous_headings":"Case Study: Conversion issues with ts()","what":"Problems","title":"Time Series Class Conversion","text":"First, numeric columns get coerced. user forgets add [,\"pct\"] drop “date” column, ts() returns dates numeric format user wants. correct method call specific column desired. However, presents new issue. date index lost, different “regularized” index built using start frequency attributes. can see structure (using str() function) regularized time series present, date index retained. can get index using index() function zoo package. index retained regular sequence numeric values. many cases, regularized values coerced back original time-base date date time data contains significantly information (.e. year-month-day, hour-minute-second, timezone attributes) data may regularized interval (frequency).","code":"# date column gets coerced to numeric ts(q10_quarterly, start = c(2000, 1), freq = 4) %>%     head() ##         id  date  value ## 2000 Q1  1 10957 2329.0 ## 2000 Q2  1 11048 2349.9 ## 2000 Q3  1 11139 2332.9 ## 2000 Q4  1 11231 2381.5 ## 2001 Q1  1 11323 2382.6 ## 2001 Q2  1 11413 2405.0 q10_quarterly_ts <- ts(q10_quarterly$value, start = c(2000, 1), freq  = 4) q10_quarterly_ts ##        Qtr1   Qtr2   Qtr3   Qtr4 ## 2000 2329.0 2349.9 2332.9 2381.5 ## 2001 2382.6 2405.0 2411.0 2428.5 ## 2002 2391.6 2418.5 2406.5 2418.5 ## 2003 2420.4 2438.6 2448.7 2470.6 ## 2004 2484.5 2495.9 2492.5 2521.6 ## 2005 2538.1 2549.7 2587.2 2585.0 ## 2006 2602.6 2615.3 2654.0 2680.8 ## 2007 2665.4 2645.1 2647.5 2719.2 ## 2008 2677.0 2650.9 2667.8 2660.2 ## 2009 2554.7 2522.7 2510.0 2541.7 ## 2010 2499.1 2527.9 2519.0 2536.3 ## 2011 2493.2 2542.1 2501.6 2516.3 ## 2012 2510.5 2548.4 2548.6 2530.7 ## 2013 2497.1 2520.4 2516.9 2505.5 ## 2014 2513.9 2549.9 2555.3 # No date index attribute str(q10_quarterly_ts) ##  Time-Series [1:59] from 2000 to 2014: 2329 2350 2333 2382 2383 ... # Regularized numeric sequence zoo::index(q10_quarterly_ts) ##  [1] 2000.00 2000.25 2000.50 2000.75 2001.00 2001.25 2001.50 2001.75 2002.00 ## [10] 2002.25 2002.50 2002.75 2003.00 2003.25 2003.50 2003.75 2004.00 2004.25 ## [19] 2004.50 2004.75 2005.00 2005.25 2005.50 2005.75 2006.00 2006.25 2006.50 ## [28] 2006.75 2007.00 2007.25 2007.50 2007.75 2008.00 2008.25 2008.50 2008.75 ## [37] 2009.00 2009.25 2009.50 2009.75 2010.00 2010.25 2010.50 2010.75 2011.00 ## [46] 2011.25 2011.50 2011.75 2012.00 2012.25 2012.50 2012.75 2013.00 2013.25 ## [55] 2013.50 2013.75 2014.00 2014.25 2014.50"},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"solution","dir":"Articles","previous_headings":"Case Study: Conversion issues with ts()","what":"Solution","title":"Time Series Class Conversion","text":"timetk package contains new function, tk_ts(), enables maintaining original date index attribute. repeat tbl ts conversion process using new function, tk_ts(), can see differences. First, numeric columns get coerced, prevents unintended consequences due R conversion rules (e.g. dates getting unintentionally converted characters causing homogeneous data structure converting numeric values character). column dropped, user gets warning. Second, data returned additional attributes. important numeric attribute, “index”, contains original date information number. ts() function preserve index tk_ts() preserve index numeric form along time zone class.","code":"# date automatically dropped and user is warned q10_quarterly_ts_timetk <- tk_ts(q10_quarterly, start = 2000, freq  = 4) ## Warning: Non-numeric columns being dropped: id, date q10_quarterly_ts_timetk ##        Qtr1   Qtr2   Qtr3   Qtr4 ## 2000 2329.0 2349.9 2332.9 2381.5 ## 2001 2382.6 2405.0 2411.0 2428.5 ## 2002 2391.6 2418.5 2406.5 2418.5 ## 2003 2420.4 2438.6 2448.7 2470.6 ## 2004 2484.5 2495.9 2492.5 2521.6 ## 2005 2538.1 2549.7 2587.2 2585.0 ## 2006 2602.6 2615.3 2654.0 2680.8 ## 2007 2665.4 2645.1 2647.5 2719.2 ## 2008 2677.0 2650.9 2667.8 2660.2 ## 2009 2554.7 2522.7 2510.0 2541.7 ## 2010 2499.1 2527.9 2519.0 2536.3 ## 2011 2493.2 2542.1 2501.6 2516.3 ## 2012 2510.5 2548.4 2548.6 2530.7 ## 2013 2497.1 2520.4 2516.9 2505.5 ## 2014 2513.9 2549.9 2555.3 # More attributes including time index, time class, time zone str(q10_quarterly_ts_timetk) ##  Time-Series [1:59, 1] from 2000 to 2014: 2329 2350 2333 2382 2383 ... ##  - attr(*, \"dimnames\")=List of 2 ##   ..$ : NULL ##   ..$ : chr \"value\" ##  - attr(*, \"index\")= num [1:59] 9.47e+08 9.55e+08 9.62e+08 9.70e+08 9.78e+08 ... ##   ..- attr(*, \"tzone\")= chr \"UTC\" ##   ..- attr(*, \"tclass\")= chr \"Date\""},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"advantages-of-conversion-with-tk_tbl","dir":"Articles","previous_headings":"Case Study: Conversion issues with ts()","what":"Advantages of conversion with tk_tbl()","title":"Time Series Class Conversion","text":"Since used tk_ts() conversion, can extract original index date format using tk_index(timetk_idx = TRUE) (default timetk_idx = FALSE returns default regularized index). Next, tk_tbl() function argument timetk_idx also can used select index return. First, show conversion using default index. Notice index returned “regularized” meaning actually numeric index rather time-based index. can now get original date index using tk_tbl() argument timetk_idx = TRUE. can see case (cases) can get data frame began .","code":"# Can now retrieve the original date index timetk_index <- q10_quarterly_ts_timetk %>%     tk_index(timetk_idx = TRUE) ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent head(timetk_index) ## [1] \"2000-01-01\" \"2000-04-01\" \"2000-07-01\" \"2000-10-01\" \"2001-01-01\" ## [6] \"2001-04-01\" class(timetk_index) ## [1] \"Date\" # Conversion back to tibble using the default index (regularized) q10_quarterly_ts_timetk %>%     tk_tbl(index_rename = \"date\", timetk_idx = FALSE) ## # A tibble: 59 × 2 ##    index     value ##    <yearqtr> <dbl> ##  1 2000 Q1   2329  ##  2 2000 Q2   2350. ##  3 2000 Q3   2333. ##  4 2000 Q4   2382. ##  5 2001 Q1   2383. ##  6 2001 Q2   2405  ##  7 2001 Q3   2411  ##  8 2001 Q4   2428. ##  9 2002 Q1   2392. ## 10 2002 Q2   2418. ## # ℹ 49 more rows # Conversion back to tibble now using the timetk index (date / date-time) q10_quarterly_timetk <- q10_quarterly_ts_timetk %>%     tk_tbl(timetk_idx = TRUE) %>%     rename(date = index) ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent q10_quarterly_timetk ## # A tibble: 59 × 2 ##    date       value ##    <date>     <dbl> ##  1 2000-01-01 2329  ##  2 2000-04-01 2350. ##  3 2000-07-01 2333. ##  4 2000-10-01 2382. ##  5 2001-01-01 2383. ##  6 2001-04-01 2405  ##  7 2001-07-01 2411  ##  8 2001-10-01 2428. ##  9 2002-01-01 2392. ## 10 2002-04-01 2418. ## # ℹ 49 more rows # Comparing the coerced tibble with the original tibble identical(q10_quarterly_timetk, q10_quarterly %>% select(-id)) ## [1] TRUE"},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"conversion-methods","dir":"Articles","previous_headings":"","what":"Conversion Methods","title":"Time Series Class Conversion","text":"Using q10_quarterly, ’ll go various conversion methods using tk_tbl, tk_xts, tk_zoo, tk_zooreg, tk_ts.","code":""},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"from-tbl","dir":"Articles","previous_headings":"Conversion Methods","what":"From tbl","title":"Time Series Class Conversion","text":"starting point q10_quarterly. coerce xts, zoo, zooreg ts classes.","code":"# Start: q10_quarterly ## # A tibble: 59 × 3 ##    id    date       value ##    <fct> <date>     <dbl> ##  1 Q10   2000-01-01 2329  ##  2 Q10   2000-04-01 2350. ##  3 Q10   2000-07-01 2333. ##  4 Q10   2000-10-01 2382. ##  5 Q10   2001-01-01 2383. ##  6 Q10   2001-04-01 2405  ##  7 Q10   2001-07-01 2411  ##  8 Q10   2001-10-01 2428. ##  9 Q10   2002-01-01 2392. ## 10 Q10   2002-04-01 2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"to-xts","dir":"Articles","previous_headings":"Conversion Methods > From tbl","what":"to xts","title":"Time Series Class Conversion","text":"Use tk_xts(). default “date” used date index “date” column dropped output. numeric columns coerced avoid unintentional conversion issues. Use select argument specify columns drop. Use date_var argument specify column use date index. Notice message warning longer present. Also, alternative, can set silent = TRUE bypass warnings since default dropping “date” column desired. Notice warnings messages.","code":"# End q10_quarterly_xts <- tk_xts(q10_quarterly) ## Warning: Non-numeric columns being dropped: id, date ## Using column `date` for date_var. head(q10_quarterly_xts) ##             value ## 2000-01-01 2329.0 ## 2000-04-01 2349.9 ## 2000-07-01 2332.9 ## 2000-10-01 2381.5 ## 2001-01-01 2382.6 ## 2001-04-01 2405.0 # End - Using `select` and `date_var` args tk_xts(q10_quarterly, select = -(id:date), date_var = date) %>%     head() ##             value ## 2000-01-01 2329.0 ## 2000-04-01 2349.9 ## 2000-07-01 2332.9 ## 2000-10-01 2381.5 ## 2001-01-01 2382.6 ## 2001-04-01 2405.0 # End - Using `silent` to silence warnings tk_xts(q10_quarterly, silent = TRUE) %>%     head() ##             value ## 2000-01-01 2329.0 ## 2000-04-01 2349.9 ## 2000-07-01 2332.9 ## 2000-10-01 2381.5 ## 2001-01-01 2382.6 ## 2001-04-01 2405.0"},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"to-zoo","dir":"Articles","previous_headings":"Conversion Methods > From tbl","what":"to zoo","title":"Time Series Class Conversion","text":"Use tk_zoo(). coercing xts, non-numeric “date” column automatically dropped index automatically selected date column.","code":"# End q10_quarterly_zoo <- tk_zoo(q10_quarterly, silent = TRUE)  head(q10_quarterly_zoo) ##             value ## 2000-01-01 2329.0 ## 2000-04-01 2349.9 ## 2000-07-01 2332.9 ## 2000-10-01 2381.5 ## 2001-01-01 2382.6 ## 2001-04-01 2405.0"},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"to-zooreg","dir":"Articles","previous_headings":"Conversion Methods > From tbl","what":"to zooreg","title":"Time Series Class Conversion","text":"Use tk_zooreg(). coercing xts, non-numeric “date” column automatically dropped. regularized index built function arguments start freq. original time-based index retained can accessed using tk_index(timetk_idx = TRUE).","code":"# End q10_quarterly_zooreg <- tk_zooreg(q10_quarterly, start = 2000, freq = 4, silent = TRUE)  head(q10_quarterly_zooreg) ##          value ## 2000 Q1 2329.0 ## 2000 Q2 2349.9 ## 2000 Q3 2332.9 ## 2000 Q4 2381.5 ## 2001 Q1 2382.6 ## 2001 Q2 2405.0 # Retrieve original time-based index tk_index(q10_quarterly_zooreg, timetk_idx = TRUE) %>%     str() ##  Date[1:59], format: \"2000-01-01\" \"2000-04-01\" \"2000-07-01\" \"2000-10-01\" \"2001-01-01\" ..."},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"to-ts","dir":"Articles","previous_headings":"Conversion Methods > From tbl","what":"to ts","title":"Time Series Class Conversion","text":"Use tk_ts(). non-numeric “date” column automatically dropped. regularized index built function arguments. original time-based index retained can accessed using tk_index(timetk_idx = TRUE).","code":"# End q10_quarterly_ts <- tk_ts(q10_quarterly, start = 2000, freq = 4, silent = TRUE)  q10_quarterly_ts ##        Qtr1   Qtr2   Qtr3   Qtr4 ## 2000 2329.0 2349.9 2332.9 2381.5 ## 2001 2382.6 2405.0 2411.0 2428.5 ## 2002 2391.6 2418.5 2406.5 2418.5 ## 2003 2420.4 2438.6 2448.7 2470.6 ## 2004 2484.5 2495.9 2492.5 2521.6 ## 2005 2538.1 2549.7 2587.2 2585.0 ## 2006 2602.6 2615.3 2654.0 2680.8 ## 2007 2665.4 2645.1 2647.5 2719.2 ## 2008 2677.0 2650.9 2667.8 2660.2 ## 2009 2554.7 2522.7 2510.0 2541.7 ## 2010 2499.1 2527.9 2519.0 2536.3 ## 2011 2493.2 2542.1 2501.6 2516.3 ## 2012 2510.5 2548.4 2548.6 2530.7 ## 2013 2497.1 2520.4 2516.9 2505.5 ## 2014 2513.9 2549.9 2555.3 # Retrieve original time-based index tk_index(q10_quarterly_ts, timetk_idx = TRUE) %>%     str() ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent ##  Date[1:59], format: \"2000-01-01\" \"2000-04-01\" \"2000-07-01\" \"2000-10-01\" \"2001-01-01\" ..."},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"to-tbl","dir":"Articles","previous_headings":"Conversion Methods","what":"To tbl","title":"Time Series Class Conversion","text":"Going back tibble just easy using tk_tbl().","code":""},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"from-xts","dir":"Articles","previous_headings":"Conversion Methods > To tbl","what":"From xts","title":"Time Series Class Conversion","text":"Notice loss data going back tbl.","code":"# Start head(q10_quarterly_xts) ##             value ## 2000-01-01 2329.0 ## 2000-04-01 2349.9 ## 2000-07-01 2332.9 ## 2000-10-01 2381.5 ## 2001-01-01 2382.6 ## 2001-04-01 2405.0 # End tk_tbl(q10_quarterly_xts) ## # A tibble: 59 × 2 ##    index      value ##    <date>     <dbl> ##  1 2000-01-01 2329  ##  2 2000-04-01 2350. ##  3 2000-07-01 2333. ##  4 2000-10-01 2382. ##  5 2001-01-01 2383. ##  6 2001-04-01 2405  ##  7 2001-07-01 2411  ##  8 2001-10-01 2428. ##  9 2002-01-01 2392. ## 10 2002-04-01 2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"from-zoo","dir":"Articles","previous_headings":"Conversion Methods > To tbl","what":"From zoo","title":"Time Series Class Conversion","text":"Notice loss data going back tbl.","code":"# Start head(q10_quarterly_zoo) ##             value ## 2000-01-01 2329.0 ## 2000-04-01 2349.9 ## 2000-07-01 2332.9 ## 2000-10-01 2381.5 ## 2001-01-01 2382.6 ## 2001-04-01 2405.0 # End tk_tbl(q10_quarterly_zoo) ## # A tibble: 59 × 2 ##    index      value ##    <date>     <dbl> ##  1 2000-01-01 2329  ##  2 2000-04-01 2350. ##  3 2000-07-01 2333. ##  4 2000-10-01 2382. ##  5 2001-01-01 2383. ##  6 2001-04-01 2405  ##  7 2001-07-01 2411  ##  8 2001-10-01 2428. ##  9 2002-01-01 2392. ## 10 2002-04-01 2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"from-zooreg","dir":"Articles","previous_headings":"Conversion Methods > To tbl","what":"From zooreg","title":"Time Series Class Conversion","text":"Notice index regularized numeric sequence default. timetk_idx = TRUE index original date sequence. result original tbl started !","code":"# Start head(q10_quarterly_zooreg) ##          value ## 2000 Q1 2329.0 ## 2000 Q2 2349.9 ## 2000 Q3 2332.9 ## 2000 Q4 2381.5 ## 2001 Q1 2382.6 ## 2001 Q2 2405.0 # End - with default regularized index tk_tbl(q10_quarterly_zooreg) ## # A tibble: 59 × 2 ##    index     value ##    <yearqtr> <dbl> ##  1 2000 Q1   2329  ##  2 2000 Q2   2350. ##  3 2000 Q3   2333. ##  4 2000 Q4   2382. ##  5 2001 Q1   2383. ##  6 2001 Q2   2405  ##  7 2001 Q3   2411  ##  8 2001 Q4   2428. ##  9 2002 Q1   2392. ## 10 2002 Q2   2418. ## # ℹ 49 more rows # End - with timetk index that is the same as original time-based index tk_tbl(q10_quarterly_zooreg, timetk_idx = TRUE) ## # A tibble: 59 × 2 ##    index      value ##    <date>     <dbl> ##  1 2000-01-01 2329  ##  2 2000-04-01 2350. ##  3 2000-07-01 2333. ##  4 2000-10-01 2382. ##  5 2001-01-01 2383. ##  6 2001-04-01 2405  ##  7 2001-07-01 2411  ##  8 2001-10-01 2428. ##  9 2002-01-01 2392. ## 10 2002-04-01 2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"from-ts","dir":"Articles","previous_headings":"Conversion Methods > To tbl","what":"From ts","title":"Time Series Class Conversion","text":"Notice index regularized numeric sequence default. timetk_idx = TRUE index original date sequence. result original tbl started !","code":"# Start q10_quarterly_ts ##        Qtr1   Qtr2   Qtr3   Qtr4 ## 2000 2329.0 2349.9 2332.9 2381.5 ## 2001 2382.6 2405.0 2411.0 2428.5 ## 2002 2391.6 2418.5 2406.5 2418.5 ## 2003 2420.4 2438.6 2448.7 2470.6 ## 2004 2484.5 2495.9 2492.5 2521.6 ## 2005 2538.1 2549.7 2587.2 2585.0 ## 2006 2602.6 2615.3 2654.0 2680.8 ## 2007 2665.4 2645.1 2647.5 2719.2 ## 2008 2677.0 2650.9 2667.8 2660.2 ## 2009 2554.7 2522.7 2510.0 2541.7 ## 2010 2499.1 2527.9 2519.0 2536.3 ## 2011 2493.2 2542.1 2501.6 2516.3 ## 2012 2510.5 2548.4 2548.6 2530.7 ## 2013 2497.1 2520.4 2516.9 2505.5 ## 2014 2513.9 2549.9 2555.3 # End - with default regularized index tk_tbl(q10_quarterly_ts) ## # A tibble: 59 × 2 ##    index     value ##    <yearqtr> <dbl> ##  1 2000 Q1   2329  ##  2 2000 Q2   2350. ##  3 2000 Q3   2333. ##  4 2000 Q4   2382. ##  5 2001 Q1   2383. ##  6 2001 Q2   2405  ##  7 2001 Q3   2411  ##  8 2001 Q4   2428. ##  9 2002 Q1   2392. ## 10 2002 Q2   2418. ## # ℹ 49 more rows # End - with timetk index  tk_tbl(q10_quarterly_ts, timetk_idx = TRUE) ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent ## # A tibble: 59 × 2 ##    index      value ##    <date>     <dbl> ##  1 2000-01-01 2329  ##  2 2000-04-01 2350. ##  3 2000-07-01 2333. ##  4 2000-10-01 2382. ##  5 2001-01-01 2383. ##  6 2001-04-01 2405  ##  7 2001-07-01 2411  ##  8 2001-10-01 2428. ##  9 2002-01-01 2392. ## 10 2002-04-01 2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"testing-if-an-object-has-a-timetk-index","dir":"Articles","previous_headings":"","what":"Testing if an object has a timetk index","title":"Time Series Class Conversion","text":"function has_timetk_idx() can used test whether toggling timetk_idx argument tk_index() tk_tbl() functions effect output. several examples using ten year treasury data used case study:","code":""},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"tk_ts","dir":"Articles","previous_headings":"Testing if an object has a timetk index","what":"tk_ts()","title":"Time Series Class Conversion","text":"tk_ts() function returns object “timetk index” attribute. toggle timetk_idx = TRUE retrieving index tk_index(), get index dates rather regularized time series. toggle timetk_idx = TRUE conversion tbl using tk_tbl(), get index dates rather regularized index returned tbl.","code":"# Data coerced with tk_ts() has timetk index has_timetk_idx(q10_quarterly_ts) ## [1] TRUE tk_index(q10_quarterly_ts, timetk_idx = TRUE) ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent ##  [1] \"2000-01-01\" \"2000-04-01\" \"2000-07-01\" \"2000-10-01\" \"2001-01-01\" ##  [6] \"2001-04-01\" \"2001-07-01\" \"2001-10-01\" \"2002-01-01\" \"2002-04-01\" ## [11] \"2002-07-01\" \"2002-10-01\" \"2003-01-01\" \"2003-04-01\" \"2003-07-01\" ## [16] \"2003-10-01\" \"2004-01-01\" \"2004-04-01\" \"2004-07-01\" \"2004-10-01\" ## [21] \"2005-01-01\" \"2005-04-01\" \"2005-07-01\" \"2005-10-01\" \"2006-01-01\" ## [26] \"2006-04-01\" \"2006-07-01\" \"2006-10-01\" \"2007-01-01\" \"2007-04-01\" ## [31] \"2007-07-01\" \"2007-10-01\" \"2008-01-01\" \"2008-04-01\" \"2008-07-01\" ## [36] \"2008-10-01\" \"2009-01-01\" \"2009-04-01\" \"2009-07-01\" \"2009-10-01\" ## [41] \"2010-01-01\" \"2010-04-01\" \"2010-07-01\" \"2010-10-01\" \"2011-01-01\" ## [46] \"2011-04-01\" \"2011-07-01\" \"2011-10-01\" \"2012-01-01\" \"2012-04-01\" ## [51] \"2012-07-01\" \"2012-10-01\" \"2013-01-01\" \"2013-04-01\" \"2013-07-01\" ## [56] \"2013-10-01\" \"2014-01-01\" \"2014-04-01\" \"2014-07-01\" tk_tbl(q10_quarterly_ts, timetk_idx = TRUE) ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent ## # A tibble: 59 × 2 ##    index      value ##    <date>     <dbl> ##  1 2000-01-01 2329  ##  2 2000-04-01 2350. ##  3 2000-07-01 2333. ##  4 2000-10-01 2382. ##  5 2001-01-01 2383. ##  6 2001-04-01 2405  ##  7 2001-07-01 2411  ##  8 2001-10-01 2428. ##  9 2002-01-01 2392. ## 10 2002-04-01 2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"testing-other-data-types","dir":"Articles","previous_headings":"Testing if an object has a timetk index","what":"Testing other data types","title":"Time Series Class Conversion","text":"timetk_idx argument effect objects use regularized time series. Therefore, has_timetk_idx() returns FALSE object types (e.g. tbl, xts, zoo) since toggling argument effect classes. Toggling timetk_idx argument effect output. Output timetk_idx = TRUE timetk_idx = FALSE.","code":"has_timetk_idx(q10_quarterly_xts) ## [1] FALSE tk_index(q10_quarterly_xts, timetk_idx = TRUE) ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent ##  [1] \"2000-01-01\" \"2000-04-01\" \"2000-07-01\" \"2000-10-01\" \"2001-01-01\" ##  [6] \"2001-04-01\" \"2001-07-01\" \"2001-10-01\" \"2002-01-01\" \"2002-04-01\" ## [11] \"2002-07-01\" \"2002-10-01\" \"2003-01-01\" \"2003-04-01\" \"2003-07-01\" ## [16] \"2003-10-01\" \"2004-01-01\" \"2004-04-01\" \"2004-07-01\" \"2004-10-01\" ## [21] \"2005-01-01\" \"2005-04-01\" \"2005-07-01\" \"2005-10-01\" \"2006-01-01\" ## [26] \"2006-04-01\" \"2006-07-01\" \"2006-10-01\" \"2007-01-01\" \"2007-04-01\" ## [31] \"2007-07-01\" \"2007-10-01\" \"2008-01-01\" \"2008-04-01\" \"2008-07-01\" ## [36] \"2008-10-01\" \"2009-01-01\" \"2009-04-01\" \"2009-07-01\" \"2009-10-01\" ## [41] \"2010-01-01\" \"2010-04-01\" \"2010-07-01\" \"2010-10-01\" \"2011-01-01\" ## [46] \"2011-04-01\" \"2011-07-01\" \"2011-10-01\" \"2012-01-01\" \"2012-04-01\" ## [51] \"2012-07-01\" \"2012-10-01\" \"2013-01-01\" \"2013-04-01\" \"2013-07-01\" ## [56] \"2013-10-01\" \"2014-01-01\" \"2014-04-01\" \"2014-07-01\""},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"working-with-zooyearmon-and-zooyearqtr-index","dir":"Articles","previous_headings":"","what":"Working with zoo::yearmon and zoo::yearqtr index","title":"Time Series Class Conversion","text":"zoo package yearmon yearqtr classes working regularized monthly quarterly data, respectively. “timetk index” tracks format conversion. ’s example yearqtr. can coerce xts yearqtr class intact. can coerce ts , although “timetk index” hidden, yearqtr class intact. Coercing ts tbl using timetk_idx = TRUE shows original index maintained conversion steps.","code":"yearqtr_tbl <- q10_quarterly %>%     mutate(date = zoo::as.yearqtr(date)) yearqtr_tbl ## # A tibble: 59 × 3 ##    id    date      value ##    <fct> <yearqtr> <dbl> ##  1 Q10   2000 Q1   2329  ##  2 Q10   2000 Q2   2350. ##  3 Q10   2000 Q3   2333. ##  4 Q10   2000 Q4   2382. ##  5 Q10   2001 Q1   2383. ##  6 Q10   2001 Q2   2405  ##  7 Q10   2001 Q3   2411  ##  8 Q10   2001 Q4   2428. ##  9 Q10   2002 Q1   2392. ## 10 Q10   2002 Q2   2418. ## # ℹ 49 more rows yearqtr_xts <- tk_xts(yearqtr_tbl) ## Warning: Non-numeric columns being dropped: id, date ## Using column `date` for date_var. yearqtr_xts %>% head() ##          value ## 2000 Q1 2329.0 ## 2000 Q2 2349.9 ## 2000 Q3 2332.9 ## 2000 Q4 2381.5 ## 2001 Q1 2382.6 ## 2001 Q2 2405.0 yearqtr_ts <- tk_ts(yearqtr_xts, start = 1997, freq = 4) yearqtr_ts %>% head() ##        Qtr1   Qtr2   Qtr3   Qtr4 ## 1997 2329.0 2349.9 2332.9 2381.5 ## 1998 2382.6 2405.0 yearqtr_ts %>% tk_tbl(timetk_idx = TRUE) ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent ## # A tibble: 59 × 2 ##    index     value ##    <yearqtr> <dbl> ##  1 2000 Q1   2329  ##  2 2000 Q2   2350. ##  3 2000 Q3   2333. ##  4 2000 Q4   2382. ##  5 2001 Q1   2383. ##  6 2001 Q2   2405  ##  7 2001 Q3   2411  ##  8 2001 Q4   2428. ##  9 2002 Q1   2392. ## 10 2002 Q2   2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html","id":"learning-more","dir":"Articles","previous_headings":"","what":"Learning More","title":"Time Series Class Conversion","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/TK01_Working_With_Time_Series_Index.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Calendar Features","text":"time series index consists collection time-based values define observation occurred, important part time series object. index gives user lot information simple timestamp. Consider datetime “2016-01-01 00:00:00”. timestamp, can decompose date time information get signature, consists year, quarter, month, day, day year, day month, hour, minute, second occurrence single observation. , difference two observations frequency can obtain even information periodicity data whether observations regular interval. information critical provides basis performance time finance, decay rates biology, growth rates economics, . vignette user exposed : Time Series Index Time Series Signature Time Series Summary","code":""},{"path":"https://business-science.github.io/timetk/articles/TK01_Working_With_Time_Series_Index.html","id":"prerequisites","dir":"Articles","previous_headings":"","what":"Prerequisites","title":"Calendar Features","text":"get started, load following packages.","code":"library(dplyr) library(timetk)"},{"path":"https://business-science.github.io/timetk/articles/TK01_Working_With_Time_Series_Index.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Calendar Features","text":"’ll use Facebook stock prices FANG data set. historical stock prices (open, high, low, close, volume, adjusted) “FB” stock 2013 2016. simplify tutorial, select “date” “volume” columns. FB_vol_date data frame, can see “date” column observations daily beginning second day 2013.","code":"data(\"FANG\")  FB_tbl <- FANG %>% dplyr::filter(symbol == \"FB\") FB_tbl ## # A tibble: 1,008 × 8 ##    symbol date        open  high   low close    volume adjusted ##    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl> ##  1 FB     2013-01-02  27.4  28.2  27.4  28    69846400     28   ##  2 FB     2013-01-03  27.9  28.5  27.6  27.8  63140600     27.8 ##  3 FB     2013-01-04  28.0  28.9  27.8  28.8  72715400     28.8 ##  4 FB     2013-01-07  28.7  29.8  28.6  29.4  83781800     29.4 ##  5 FB     2013-01-08  29.5  29.6  28.9  29.1  45871300     29.1 ##  6 FB     2013-01-09  29.7  30.6  29.5  30.6 104787700     30.6 ##  7 FB     2013-01-10  30.6  31.5  30.3  31.3  95316400     31.3 ##  8 FB     2013-01-11  31.3  32.0  31.1  31.7  89598000     31.7 ##  9 FB     2013-01-14  32.1  32.2  30.6  31.0  98892800     31.0 ## 10 FB     2013-01-15  30.6  31.7  29.9  30.1 173242600     30.1 ## # ℹ 998 more rows FB_vol_date <- FB_tbl %>% select(date, volume) FB_vol_date ## # A tibble: 1,008 × 2 ##    date          volume ##    <date>         <dbl> ##  1 2013-01-02  69846400 ##  2 2013-01-03  63140600 ##  3 2013-01-04  72715400 ##  4 2013-01-07  83781800 ##  5 2013-01-08  45871300 ##  6 2013-01-09 104787700 ##  7 2013-01-10  95316400 ##  8 2013-01-11  89598000 ##  9 2013-01-14  98892800 ## 10 2013-01-15 173242600 ## # ℹ 998 more rows"},{"path":"https://business-science.github.io/timetk/articles/TK01_Working_With_Time_Series_Index.html","id":"time-series-index","dir":"Articles","previous_headings":"","what":"Time Series Index","title":"Calendar Features","text":"can analyze index, need extract object. function tk_index() extracts index time series object including data frame (tbl), xts, zoo, etc. index always returned native date, datetime, yearmon, yearqtr format. Note index must one time-based classes extraction work: datetimes: Must inherit POSIXt dates: Must inherit Date yearmon: Must inherit yearmon zoo package yearqtr: Must inherit yearqtr zoo package Extract index using tk_index(). structure shown see output format, vector dates.","code":"# idx_date idx_date <- tk_index(FB_vol_date) str(idx_date) ##  Date[1:1008], format: \"2013-01-02\" \"2013-01-03\" \"2013-01-04\" \"2013-01-07\" \"2013-01-08\" ..."},{"path":"https://business-science.github.io/timetk/articles/TK01_Working_With_Time_Series_Index.html","id":"time-series-signature","dir":"Articles","previous_headings":"","what":"Time Series Signature","title":"Calendar Features","text":"index can decomposed signature. time series signature unique set properties time series values describe time series.","code":""},{"path":"https://business-science.github.io/timetk/articles/TK01_Working_With_Time_Series_Index.html","id":"get-functions---turning-an-index-into-information","dir":"Articles","previous_headings":"Time Series Signature","what":"Get Functions - Turning an Index into Information","title":"Calendar Features","text":"function tk_get_timeseries_signature() can used convert index tibble containing following values (columns): index: index value decomposed index.num: numeric value index seconds. base “1970-01-01 00:00:00” (Execute \"1970-01-01 00:00:00\" %>% ymd_hms() %>% .numeric() see value returned zero). Every time series value date can converted numeric value seconds. diff: difference seconds previous numeric index value. year: year component index. year.iso: ISO year number year (Monday start). half: half component index. quarter: quarter component index. month: month component index base 1. month.xts: month component index base 0, xts implements. month.lbl: month label ordered factor begining January ending December. day: day component index. hour: hour component index. minute: minute component index. second: second component index. hour12: hour component 12 hour scale. .pm: Morning () = 1, Afternoon (PM) = 2. wday: day week base 1. Sunday = 1 Saturday = 7. wday.xts: day week base 0, xts implements. Sunday = 0 Saturday = 6. wday.lbl: day week label ordered factor begining Sunday ending Saturday. mday: day month. qday: day quarter. yday: day year. mweek: week month. week: week number year (Sunday start). week.iso: ISO week number year (Monday start). week2: modulus bi-weekly frequency. week3: modulus tri-weekly frequency. week4: modulus quad-weekly frequency. mday7: integer division day month seven, returns first, second, third, … instance day appeared month. Values begin 1. example, first Saturday month mday7 = 1. second mday7 = 2.","code":"# idx_date signature tk_get_timeseries_signature(idx_date) ## # A tibble: 1,008 × 29 ##    index       index.num   diff  year year.iso  half quarter month month.xts ##    <date>          <dbl>  <dbl> <int>    <int> <int>   <int> <int>     <int> ##  1 2013-01-02 1357084800     NA  2013     2013     1       1     1         0 ##  2 2013-01-03 1357171200  86400  2013     2013     1       1     1         0 ##  3 2013-01-04 1357257600  86400  2013     2013     1       1     1         0 ##  4 2013-01-07 1357516800 259200  2013     2013     1       1     1         0 ##  5 2013-01-08 1357603200  86400  2013     2013     1       1     1         0 ##  6 2013-01-09 1357689600  86400  2013     2013     1       1     1         0 ##  7 2013-01-10 1357776000  86400  2013     2013     1       1     1         0 ##  8 2013-01-11 1357862400  86400  2013     2013     1       1     1         0 ##  9 2013-01-14 1358121600 259200  2013     2013     1       1     1         0 ## 10 2013-01-15 1358208000  86400  2013     2013     1       1     1         0 ## # ℹ 998 more rows ## # ℹ 20 more variables: month.lbl <ord>, day <int>, hour <int>, minute <int>, ## #   second <int>, hour12 <int>, am.pm <int>, wday <int>, wday.xts <int>, ## #   wday.lbl <ord>, mday <int>, qday <int>, yday <int>, mweek <int>, ## #   week <int>, week.iso <int>, week2 <int>, week3 <int>, week4 <int>, ## #   mday7 <int>"},{"path":"https://business-science.github.io/timetk/articles/TK01_Working_With_Time_Series_Index.html","id":"augment-functions-adding-many-features-to-a-data-frame","dir":"Articles","previous_headings":"Time Series Signature","what":"Augment Functions (Adding Many Features to a Data Frame)","title":"Calendar Features","text":"’s usually important keep index signature values (e.g. volume example). can use expedited approach tk_augment_timeseries_signature(), adds signature end time series object. Modeling now much easier. example, can use linear regression model using lm() function month year predictor volume.","code":"# Augmenting a data frame FB_vol_date_signature <- FB_vol_date %>% tk_augment_timeseries_signature(.date_var = date) FB_vol_date_signature ## # A tibble: 1,008 × 30 ##    date          volume  index.num   diff  year year.iso  half quarter month ##    <date>         <dbl>      <dbl>  <dbl> <int>    <int> <int>   <int> <int> ##  1 2013-01-02  69846400 1357084800     NA  2013     2013     1       1     1 ##  2 2013-01-03  63140600 1357171200  86400  2013     2013     1       1     1 ##  3 2013-01-04  72715400 1357257600  86400  2013     2013     1       1     1 ##  4 2013-01-07  83781800 1357516800 259200  2013     2013     1       1     1 ##  5 2013-01-08  45871300 1357603200  86400  2013     2013     1       1     1 ##  6 2013-01-09 104787700 1357689600  86400  2013     2013     1       1     1 ##  7 2013-01-10  95316400 1357776000  86400  2013     2013     1       1     1 ##  8 2013-01-11  89598000 1357862400  86400  2013     2013     1       1     1 ##  9 2013-01-14  98892800 1358121600 259200  2013     2013     1       1     1 ## 10 2013-01-15 173242600 1358208000  86400  2013     2013     1       1     1 ## # ℹ 998 more rows ## # ℹ 21 more variables: month.xts <int>, month.lbl <ord>, day <int>, hour <int>, ## #   minute <int>, second <int>, hour12 <int>, am.pm <int>, wday <int>, ## #   wday.xts <int>, wday.lbl <ord>, mday <int>, qday <int>, yday <int>, ## #   mweek <int>, week <int>, week.iso <int>, week2 <int>, week3 <int>, ## #   week4 <int>, mday7 <int> # Example Benefit 2: Modeling is easier fit <- lm(volume ~ year + month.lbl, data = FB_vol_date_signature) summary(fit) ##  ## Call: ## lm(formula = volume ~ year + month.lbl, data = FB_vol_date_signature) ##  ## Residuals: ##       Min        1Q    Median        3Q       Max  ## -51042223 -13528407  -4588594   8296073 304011277  ##  ## Coefficients: ##                Estimate Std. Error t value Pr(>|t|)     ## (Intercept)   2.494e+10  1.414e+09  17.633  < 2e-16 *** ## year         -1.236e+07  7.021e+05 -17.604  < 2e-16 *** ## month.lbl.L  -9.589e+06  2.740e+06  -3.499 0.000488 *** ## month.lbl.Q   7.348e+06  2.725e+06   2.697 0.007122 **  ## month.lbl.C  -9.773e+06  2.711e+06  -3.605 0.000328 *** ## month.lbl^4  -2.885e+06  2.720e+06  -1.060 0.289176     ## month.lbl^5  -2.994e+06  2.749e+06  -1.089 0.276428     ## month.lbl^6   3.169e+06  2.753e+06   1.151 0.249851     ## month.lbl^7   6.000e+05  2.721e+06   0.221 0.825514     ## month.lbl^8   8.281e+03  2.702e+06   0.003 0.997555     ## month.lbl^9   9.504e+06  2.704e+06   3.515 0.000459 *** ## month.lbl^10 -5.911e+06  2.701e+06  -2.188 0.028888 *   ## month.lbl^11 -4.738e+06  2.696e+06  -1.757 0.079181 .   ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ##  ## Residual standard error: 24910000 on 995 degrees of freedom ## Multiple R-squared:  0.2714, Adjusted R-squared:  0.2626  ## F-statistic: 30.89 on 12 and 995 DF,  p-value: < 2.2e-16"},{"path":"https://business-science.github.io/timetk/articles/TK01_Working_With_Time_Series_Index.html","id":"time-series-summary","dir":"Articles","previous_headings":"","what":"Time Series Summary","title":"Calendar Features","text":"next index analysis tool summary metrics, can retrieved using tk_get_timeseries_summary() function. summary reports following attributes single-row tibble. General Summary: first six columns general summary information. n.obs: total number observations start: start appropriate time class end: end appropriate time class units: label describes unit index value independent frequency (.e. date class always “days” whereas datetime class always “seconds”). Values can days, hours, mins, secs. scale: label describes median difference (frequency) observations. Values can quarter, month, day, hour, minute, second. tzone: timezone index. Differences Summary: next group values differences summary (.e. summary frequency). values seconds: diff.minimum: minimum difference index values. diff.q1: first quartile index differences. diff.median: median difference index values (.e. common frequency). diff.mean: average difference index values. diff.q3: third quartile index differences. diff.maximum: maximum difference index values. differences provide information regularity frequency. Generally speaking difference values equal, index regular. However, scales beyond “day” never theoretically regular since differences seconds equivalent. However, conceptually monthly, quarterly yearly data can thought regular index contains consecutive months, quarters, years, respectively. Therefore, difference attributes meaningful daily lower time scales difference summary always indicates level regularity. second group (differences summary), immediately recognize mean different median therefore index irregular (meaning certain days missing). can see maximum difference 345,600 seconds, indicating maximum difference 4 days (345,600 seconds / 86400 seconds/day).","code":"# idx_date: First six columns, general summary tk_get_timeseries_summary(idx_date)[,1:6] ## # A tibble: 1 × 6 ##   n.obs start      end        units scale tzone ##   <int> <date>     <date>     <chr> <chr> <chr> ## 1  1008 2013-01-02 2016-12-30 days  day   UTC # idx_date: Last six columns, difference summary tk_get_timeseries_summary(idx_date)[,7:12] ## # A tibble: 1 × 6 ##   diff.minimum diff.q1 diff.median diff.mean diff.q3 diff.maximum ##          <dbl>   <dbl>       <dbl>     <dbl>   <dbl>        <dbl> ## 1        86400   86400       86400   125096.   86400       345600"},{"path":"https://business-science.github.io/timetk/articles/TK01_Working_With_Time_Series_Index.html","id":"learning-more","dir":"Articles","previous_headings":"","what":"Learning More","title":"Calendar Features","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/TK02_Time_Series_Date_Sequences.html","id":"prerequisites","dir":"Articles","previous_headings":"","what":"Prerequisites","title":"Intelligent Date & Time Sequences","text":"get started, load following packages.","code":"library(dplyr) library(timetk)"},{"path":"https://business-science.github.io/timetk/articles/TK02_Time_Series_Date_Sequences.html","id":"making-a-time-series-sequence","dir":"Articles","previous_headings":"","what":"Making a Time Series Sequence","title":"Intelligent Date & Time Sequences","text":"tk_make_timeseries() improves seq.Date() seq.POSIXt() functions simplifying 1 function. Intelligently handles character dates logical assumptions based user inputs. Day Can use = \"day\" leave blank. include_endpoints = FALSE removes last value series 7 observations. 2 Seconds Can use = \"2 sec\" adjust interval width. include_endpoints = TRUE keeps last value series ends 6th second. Length = 1 year 6 months length_out = \"1 year 6 months\" - Can include complex expressions like “1 year 4 months 6 days”. Go Reverse go reverse, just use end_date want series end.","code":"# Selects by day automatically tk_make_timeseries(\"2011\", length_out = \"7 days\", include_endpoints = FALSE) ## [1] \"2011-01-01\" \"2011-01-02\" \"2011-01-03\" \"2011-01-04\" \"2011-01-05\" ## [6] \"2011-01-06\" \"2011-01-07\" # Guesses by second tk_make_timeseries(\"2016\", by = \"2 sec\", length_out = \"6 seconds\") ## [1] \"2016-01-01 00:00:00 UTC\" \"2016-01-01 00:00:02 UTC\" ## [3] \"2016-01-01 00:00:04 UTC\" \"2016-01-01 00:00:06 UTC\" tk_make_timeseries(\"2012-07\",                     by = \"1 month\",                    length_out = \"1 year 6 months\",                     include_endpoints = FALSE) ##  [1] \"2012-07-01\" \"2012-08-01\" \"2012-09-01\" \"2012-10-01\" \"2012-11-01\" ##  [6] \"2012-12-01\" \"2013-01-01\" \"2013-02-01\" \"2013-03-01\" \"2013-04-01\" ## [11] \"2013-05-01\" \"2013-06-01\" \"2013-07-01\" \"2013-08-01\" \"2013-09-01\" ## [16] \"2013-10-01\" \"2013-11-01\" \"2013-12-01\" tk_make_timeseries(end_date = \"2012-07-01\",                     by = \"1 month\",                    length_out = \"1 year 6 months\") ##  [1] \"2011-01-01\" \"2011-02-01\" \"2011-03-01\" \"2011-04-01\" \"2011-05-01\" ##  [6] \"2011-06-01\" \"2011-07-01\" \"2011-08-01\" \"2011-09-01\" \"2011-10-01\" ## [11] \"2011-11-01\" \"2011-12-01\" \"2012-01-01\" \"2012-02-01\" \"2012-03-01\" ## [16] \"2012-04-01\" \"2012-05-01\" \"2012-06-01\" \"2012-07-01\""},{"path":"https://business-science.github.io/timetk/articles/TK02_Time_Series_Date_Sequences.html","id":"future-time-series-sequence","dir":"Articles","previous_headings":"","what":"Future Time Series Sequence","title":"Intelligent Date & Time Sequences","text":"common operation make future time series sequence mimics existing. tk_make_future_timeseries() . Suppose existing time index. Make Future Time Series Existing can create future time sequence existing sequence using tk_make_future_timeseries().","code":"idx <- tk_make_timeseries(\"2012\", by = \"3 months\",                            length_out = \"2 years\",                            include_endpoints = FALSE) idx ## [1] \"2012-01-01\" \"2012-04-01\" \"2012-07-01\" \"2012-10-01\" \"2013-01-01\" ## [6] \"2013-04-01\" \"2013-07-01\" \"2013-10-01\" tk_make_future_timeseries(idx, length_out = \"2 years\") ## [1] \"2014-01-01\" \"2014-04-01\" \"2014-07-01\" \"2014-10-01\" \"2015-01-01\" ## [6] \"2015-04-01\" \"2015-07-01\" \"2015-10-01\""},{"path":"https://business-science.github.io/timetk/articles/TK02_Time_Series_Date_Sequences.html","id":"weekends-holidays","dir":"Articles","previous_headings":"","what":"Weekends & Holidays","title":"Intelligent Date & Time Sequences","text":"Make weekday sequence removing holidays Result 252 days. holidays removed? NYSE Trading holidays days businesses observe Make future index removing holidays","code":"idx <- tk_make_weekday_sequence(\"2012\",                                 remove_weekends = TRUE,                                  remove_holidays = TRUE, calendar = \"NYSE\")  tk_get_timeseries_summary(idx) ## # A tibble: 1 × 12 ##   n.obs start      end        units scale tzone diff.minimum diff.q1 diff.median ##   <int> <date>     <date>     <chr> <chr> <chr>        <dbl>   <dbl>       <dbl> ## 1   250 2012-01-03 2012-12-31 days  day   UTC          86400   86400       86400 ## # ℹ 3 more variables: diff.mean <dbl>, diff.q3 <dbl>, diff.maximum <dbl> tk_make_holiday_sequence(\"2012\", calendar = \"NYSE\") ##  [1] \"2012-01-02\" \"2012-01-16\" \"2012-02-20\" \"2012-04-06\" \"2012-05-28\" ##  [6] \"2012-07-04\" \"2012-09-03\" \"2012-10-29\" \"2012-10-30\" \"2012-11-22\" ## [11] \"2012-12-25\" holidays <- tk_make_holiday_sequence(     start_date = \"2013-01-01\",     end_date   = \"2013-12-31\",     calendar   = \"NYSE\")  idx_future <- idx %>%    tk_make_future_timeseries(length_out       = \"1 year\",                              inspect_weekdays = TRUE,                              skip_values      = holidays)  tk_get_timeseries_summary(idx_future) ## # A tibble: 1 × 12 ##   n.obs start      end        units scale tzone diff.minimum diff.q1 diff.median ##   <int> <date>     <date>     <chr> <chr> <chr>        <dbl>   <dbl>       <dbl> ## 1   252 2013-01-02 2013-12-31 days  day   UTC          86400   86400       86400 ## # ℹ 3 more variables: diff.mean <dbl>, diff.q3 <dbl>, diff.maximum <dbl>"},{"path":"https://business-science.github.io/timetk/articles/TK02_Time_Series_Date_Sequences.html","id":"learning-more","dir":"Articles","previous_headings":"","what":"Learning More","title":"Intelligent Date & Time Sequences","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Time Series Machine Learning","text":"time series signature collection useful features describe time series index time-based data set. contains wealth features can used forecast time series contain patterns. vignette, user learn methods implement machine learning predict future outcomes time-based data set. vignette example uses well known time series dataset, Bike Sharing Dataset, UCI Machine Learning Repository. vignette follows example ’ll use timetk build basic Machine Learning model predict future values using time series signature. objective build model predict next six months Bike Sharing daily counts.","code":""},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"prerequisites","dir":"Articles","previous_headings":"","what":"Prerequisites","title":"Time Series Machine Learning","text":"get started, load following packages.","code":"library(dplyr) library(timetk) library(recipes) library(parsnip) library(workflows) library(rsample) # Used to convert plots from interactive to static interactive = FALSE"},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Time Series Machine Learning","text":"’ll using Bike Sharing Dataset UCI Machine Learning Repository. Source: Fanaee-T, Hadi, Gama, Joao, ‘Event labeling combining ensemble detectors background knowledge’, Progress Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg Next, visualize dataset plot_time_series() function. Toggle .interactive = TRUE get plotly interactive plot. FALSE returns ggplot2 static plot.","code":"# Read data bike_transactions_tbl <- bike_sharing_daily %>%   select(date = dteday, value = cnt)  bike_transactions_tbl ## # A tibble: 731 × 2 ##    date       value ##    <date>     <dbl> ##  1 2011-01-01   985 ##  2 2011-01-02   801 ##  3 2011-01-03  1349 ##  4 2011-01-04  1562 ##  5 2011-01-05  1600 ##  6 2011-01-06  1606 ##  7 2011-01-07  1510 ##  8 2011-01-08   959 ##  9 2011-01-09   822 ## 10 2011-01-10  1321 ## # ℹ 721 more rows bike_transactions_tbl %>%   plot_time_series(date, value, .interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"train-test","dir":"Articles","previous_headings":"","what":"Train / Test","title":"Time Series Machine Learning","text":"Next, use time_series_split() make train/test set. Setting assess = \"3 months\" tells function use last 3-months data testing set. Setting cumulative = TRUE tells sampling use prior data training set. Next, visualize train/test split. tk_time_series_cv_plan(): Converts splits object data frame plot_time_series_cv_plan(): Plots time series sampling data using “date” “value” columns.","code":"splits <- bike_transactions_tbl %>%   time_series_split(assess = \"3 months\", cumulative = TRUE) splits %>%   tk_time_series_cv_plan() %>%   plot_time_series_cv_plan(date, value, .interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"modeling","dir":"Articles","previous_headings":"","what":"Modeling","title":"Time Series Machine Learning","text":"Machine learning models complex univariate models (e.g. ARIMA, Exponential Smoothing). complexity typically requires workflow (sometimes called pipeline languages). general process goes like : Create Preprocessing Recipe Create Model Specifications Use Workflow combine Model Spec Preprocessing, Fit Model","code":""},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"recipe-preprocessing-specification","dir":"Articles","previous_headings":"Modeling","what":"Recipe Preprocessing Specification","title":"Time Series Machine Learning","text":"first step add time series signature training set, used learn patterns. New timetk 0.1.3 integration recipes R package: recipes package allows us add preprocessing steps applied sequentially part data transformation pipeline. timetk step_timeseries_signature(), used add number features can help machine learning models. can see happens apply prepared recipe prep() using bake() function. Many new columns added timestamp “date” feature. features can use machine learning models. Next, apply various preprocessing steps improve modeling behavior. wish learn , Advanced Time Series course help learn techniques.","code":"library(recipes) # Add time series signature recipe_spec_timeseries <- recipe(value ~ ., data = training(splits)) %>%     step_timeseries_signature(date) bake(prep(recipe_spec_timeseries), new_data = training(splits)) ## # A tibble: 641 × 29 ##    date       value date_index.num date_year date_year.iso date_half ##    <date>     <dbl>          <dbl>     <int>         <int>     <int> ##  1 2011-01-01   985     1293840000      2011          2010         1 ##  2 2011-01-02   801     1293926400      2011          2010         1 ##  3 2011-01-03  1349     1294012800      2011          2011         1 ##  4 2011-01-04  1562     1294099200      2011          2011         1 ##  5 2011-01-05  1600     1294185600      2011          2011         1 ##  6 2011-01-06  1606     1294272000      2011          2011         1 ##  7 2011-01-07  1510     1294358400      2011          2011         1 ##  8 2011-01-08   959     1294444800      2011          2011         1 ##  9 2011-01-09   822     1294531200      2011          2011         1 ## 10 2011-01-10  1321     1294617600      2011          2011         1 ## # ℹ 631 more rows ## # ℹ 23 more variables: date_quarter <int>, date_month <int>, ## #   date_month.xts <int>, date_month.lbl <ord>, date_day <int>, ## #   date_hour <int>, date_minute <int>, date_second <int>, date_hour12 <int>, ## #   date_am.pm <int>, date_wday <int>, date_wday.xts <int>, ## #   date_wday.lbl <ord>, date_mday <int>, date_qday <int>, date_yday <int>, ## #   date_mweek <int>, date_week <int>, date_week.iso <int>, date_week2 <int>, … recipe_spec_final <- recipe_spec_timeseries %>%     step_fourier(date, period = 365, K = 5) %>%     step_rm(date) %>%     step_rm(contains(\"iso\"), contains(\"minute\"), contains(\"hour\"),             contains(\"am.pm\"), contains(\"xts\")) %>%     step_normalize(contains(\"index.num\"), date_year) %>%     step_dummy(contains(\"lbl\"), one_hot = TRUE)   juice(prep(recipe_spec_final)) ## # A tibble: 641 × 47 ##    value date_index.num date_year date_half date_quarter date_month date_day ##    <dbl>          <dbl>     <dbl>     <int>        <int>      <int>    <int> ##  1   985          -1.73    -0.869         1            1          1        1 ##  2   801          -1.72    -0.869         1            1          1        2 ##  3  1349          -1.72    -0.869         1            1          1        3 ##  4  1562          -1.71    -0.869         1            1          1        4 ##  5  1600          -1.71    -0.869         1            1          1        5 ##  6  1606          -1.70    -0.869         1            1          1        6 ##  7  1510          -1.70    -0.869         1            1          1        7 ##  8   959          -1.69    -0.869         1            1          1        8 ##  9   822          -1.68    -0.869         1            1          1        9 ## 10  1321          -1.68    -0.869         1            1          1       10 ## # ℹ 631 more rows ## # ℹ 40 more variables: date_second <int>, date_wday <int>, date_mday <int>, ## #   date_qday <int>, date_yday <int>, date_mweek <int>, date_week <int>, ## #   date_week2 <int>, date_week3 <int>, date_week4 <int>, date_mday7 <int>, ## #   date_sin365_K1 <dbl>, date_cos365_K1 <dbl>, date_sin365_K2 <dbl>, ## #   date_cos365_K2 <dbl>, date_sin365_K3 <dbl>, date_cos365_K3 <dbl>, ## #   date_sin365_K4 <dbl>, date_cos365_K4 <dbl>, date_sin365_K5 <dbl>, …"},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"model-specification","dir":"Articles","previous_headings":"Modeling","what":"Model Specification","title":"Time Series Machine Learning","text":"Next, let’s create model specification. ’ll use Elastic Net penalized regression via glmnet package.","code":"model_spec_lm <- linear_reg(     mode = \"regression\",      penalty = 0.1 ) %>%     set_engine(\"glmnet\")"},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"workflow","dir":"Articles","previous_headings":"Modeling","what":"Workflow","title":"Time Series Machine Learning","text":"can mary preprocessing recipe model using workflow().","code":"workflow_lm <- workflow() %>%     add_recipe(recipe_spec_final) %>%     add_model(model_spec_lm)  workflow_lm ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: linear_reg() ##  ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 6 Recipe Steps ##  ## • step_timeseries_signature() ## • step_fourier() ## • step_rm() ## • step_rm() ## • step_normalize() ## • step_dummy() ##  ## ── Model ─────────────────────────────────────────────────────────────────────── ## Linear Regression Model Specification (regression) ##  ## Main Arguments: ##   penalty = 0.1 ##  ## Computational engine: glmnet"},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"training","dir":"Articles","previous_headings":"Modeling","what":"Training","title":"Time Series Machine Learning","text":"workflow can trained fit() function.","code":"if (requireNamespace(\"glmnet\")) {   workflow_fit_lm <- workflow_lm %>% fit(data = training(splits)) }"},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"hyperparameter-tuning","dir":"Articles","previous_headings":"Modeling","what":"Hyperparameter Tuning","title":"Time Series Machine Learning","text":"Linear regression parameters. Therefore, step needed. complex models hyperparameters require tuning. Algorithms include: Elastic Net XGBoost Random Forest Support Vector Machine (SVM) K-Nearest Neighbors Multivariate Adaptive Regression Spines (MARS) like learn tune models time series, join waitlist advanced Time Series Analysis & Forecasting Course.","code":""},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"forecasting-with-modeltime","dir":"Articles","previous_headings":"","what":"Forecasting with Modeltime","title":"Time Series Machine Learning","text":"Modeltime Workflow designed speed model evaluation selection. Now several time series models, let’s analyze forecast future modeltime package.","code":""},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"modeltime-table","dir":"Articles","previous_headings":"Forecasting with Modeltime","what":"Modeltime Table","title":"Time Series Machine Learning","text":"Modeltime Table organizes models IDs creates generic descriptions help us keep track models. Let’s add models modeltime_table().","code":"if (rlang::is_installed(\"modeltime\")) {   model_table <- modeltime::modeltime_table(   workflow_fit_lm )   model_table } ## # Modeltime Table ## # A tibble: 1 × 3 ##   .model_id .model     .model_desc ##       <int> <list>     <chr>       ## 1         1 <workflow> GLMNET"},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"calibration","dir":"Articles","previous_headings":"Forecasting with Modeltime","what":"Calibration","title":"Time Series Machine Learning","text":"Model Calibration used quantify error estimate confidence intervals. ’ll perform model calibration --sample data (aka. Testing Set) modeltime::modeltime_calibrate() function. Two new columns generated (“.type” “.calibration_data”), important “.calibration_data”. includes actual values, fitted values, residuals testing set.","code":"calibration_table <- model_table %>%   modeltime::modeltime_calibrate(testing(splits))  calibration_table ## # Modeltime Table ## # A tibble: 1 × 5 ##   .model_id .model     .model_desc .type .calibration_data ##       <int> <list>     <chr>       <chr> <list>            ## 1         1 <workflow> GLMNET      Test  <tibble [90 × 4]>"},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"forecast-testing-set","dir":"Articles","previous_headings":"Forecasting with Modeltime > Calibration","what":"Forecast (Testing Set)","title":"Time Series Machine Learning","text":"calibrated data, can visualize testing predictions (forecast). Use modeltime::modeltime_forecast() generate forecast data testing set tibble. Use modeltime::plot_modeltime_forecast() visualize results interactive static plot formats.","code":"calibration_table %>%   modeltime::modeltime_forecast(actual_data = bike_transactions_tbl) %>%   modeltime::plot_modeltime_forecast(.interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"accuracy-testing-set","dir":"Articles","previous_headings":"Forecasting with Modeltime > Calibration","what":"Accuracy (Testing Set)","title":"Time Series Machine Learning","text":"Next, calculate testing accuracy compare models. Use modeltime::modeltime_accuracy() generate --sample accuracy metrics tibble. Use modeltime::table_modeltime_accuracy() generate interactive static","code":"calibration_table %>%   modeltime::modeltime_accuracy() %>%   modeltime::table_modeltime_accuracy(.interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"refit-and-forecast-forward","dir":"Articles","previous_headings":"Forecasting with Modeltime","what":"Refit and Forecast Forward","title":"Time Series Machine Learning","text":"Refitting best-practice forecasting future. modeltime::modeltime_refit(): re-train full data (bike_transactions_tbl) modeltime::modeltime_forecast(): models depend “date” feature, can use h (horizon) forecast forward. Setting h = \"12 months\" forecasts next 12-months data.","code":"calibration_table %>%   modeltime::modeltime_refit(bike_transactions_tbl) %>%   modeltime::modeltime_forecast(h = \"12 months\", actual_data = bike_transactions_tbl) %>%   modeltime::plot_modeltime_forecast(.interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Time Series Machine Learning","text":"Timetk part amazing Modeltime Ecosystem time series forecasting. can take long time learn: Many algorithms Ensembling Resampling Feature Engineering Machine Learning Deep Learning Scalable Modeling: 10,000+ time series probably thinking ever going learn time series forecasting. ’s solution save years struggling.","code":""},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"take-the-high-performance-forecasting-course","dir":"Articles","previous_headings":"","what":"Take the High-Performance Forecasting Course","title":"Time Series Machine Learning","text":"Become forecasting expert organization  High-Performance Time Series Course","code":""},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"time-series-is-changing","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"Time Series is Changing","title":"Time Series Machine Learning","text":"Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies improving accuracy scalability. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System).","code":""},{"path":"https://business-science.github.io/timetk/articles/TK03_Forecasting_Using_Time_Series_Signature.html","id":"how-to-learn-high-performance-time-series-forecasting","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"How to Learn High-Performance Time Series Forecasting","title":"Time Series Machine Learning","text":"teach build HPTFS System High-Performance Time Series Forecasting Course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Become Time Series Expert organization. Take High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/TK04_Plotting_Time_Series.html","id":"libraries","dir":"Articles","previous_headings":"","what":"Libraries","title":"Visualizing Time Series","text":"Run following code setup tutorial.","code":"library(dplyr) library(ggplot2) library(lubridate) library(timetk)  # Setup for the plotly charts (# FALSE returns ggplots) interactive <- FALSE"},{"path":"https://business-science.github.io/timetk/articles/TK04_Plotting_Time_Series.html","id":"plotting-time-series","dir":"Articles","previous_headings":"","what":"Plotting Time Series","title":"Visualizing Time Series","text":"Let’s start popular time series, taylor_30_min, includes energy demand megawatts sampling interval 30-minutes. single time series. plot_time_series() function generates interactive plotly chart default. Simply provide date variable (time-based column, .date_var) numeric variable (.value) changes time first 2 arguments .interactive = TRUE, .plotly_slider = TRUE adds date slider bottom chart.","code":"taylor_30_min #> # A tibble: 4,032 × 2 #>    date                value #>    <dttm>              <dbl> #>  1 2000-06-05 00:00:00 22262 #>  2 2000-06-05 00:30:00 21756 #>  3 2000-06-05 01:00:00 22247 #>  4 2000-06-05 01:30:00 22759 #>  5 2000-06-05 02:00:00 22549 #>  6 2000-06-05 02:30:00 22313 #>  7 2000-06-05 03:00:00 22128 #>  8 2000-06-05 03:30:00 21860 #>  9 2000-06-05 04:00:00 21751 #> 10 2000-06-05 04:30:00 21336 #> # ℹ 4,022 more rows taylor_30_min %>%    plot_time_series(date, value,                     .interactive = interactive,                    .plotly_slider = TRUE)"},{"path":"https://business-science.github.io/timetk/articles/TK04_Plotting_Time_Series.html","id":"plotting-groups","dir":"Articles","previous_headings":"Plotting Time Series","what":"Plotting Groups","title":"Visualizing Time Series","text":"Next, let’s move dataset time series groups, m4_daily, sample 4 time series M4 competition sampled daily frequency. Visualizing grouped data simple grouping data set group_by() prior piping plot_time_series() function. Key points: Groups can added 2 ways: group_by() using ... add groups. Groups converted facets. .facet_ncol = 2 returns 2-column faceted plot .facet_scales = \"free\" allows x y-axis plot scale independently plots","code":"m4_daily %>% group_by(id) #> # A tibble: 9,743 × 3 #> # Groups:   id [4] #>    id    date       value #>    <fct> <date>     <dbl> #>  1 D10   2014-07-03 2076. #>  2 D10   2014-07-04 2073. #>  3 D10   2014-07-05 2049. #>  4 D10   2014-07-06 2049. #>  5 D10   2014-07-07 2006. #>  6 D10   2014-07-08 2018. #>  7 D10   2014-07-09 2019. #>  8 D10   2014-07-10 2007. #>  9 D10   2014-07-11 2010  #> 10 D10   2014-07-12 2002. #> # ℹ 9,733 more rows m4_daily %>%   group_by(id) %>%   plot_time_series(date, value,                     .facet_ncol = 2, .facet_scales = \"free\",                    .interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/TK04_Plotting_Time_Series.html","id":"visualizing-transformations-sub-groups","dir":"Articles","previous_headings":"Plotting Time Series","what":"Visualizing Transformations & Sub-Groups","title":"Visualizing Time Series","text":"Let’s switch hourly dataset multiple groups. can showcase: Log transformation .value Use .color_var highlight sub-groups. intent showcase groups faceted plots, highlight weekly windows (sub-groups) within data simultaneously log() transformation value. simple : .value = log(value) Applies Log Transformation .color_var = week(date) date column transformed lubridate::week() number. color applied week numbers.","code":"m4_hourly %>% group_by(id) #> # A tibble: 3,060 × 3 #> # Groups:   id [4] #>    id    date                value #>    <fct> <dttm>              <dbl> #>  1 H10   2015-07-01 12:00:00   513 #>  2 H10   2015-07-01 13:00:00   512 #>  3 H10   2015-07-01 14:00:00   506 #>  4 H10   2015-07-01 15:00:00   500 #>  5 H10   2015-07-01 16:00:00   490 #>  6 H10   2015-07-01 17:00:00   484 #>  7 H10   2015-07-01 18:00:00   467 #>  8 H10   2015-07-01 19:00:00   446 #>  9 H10   2015-07-01 20:00:00   434 #> 10 H10   2015-07-01 21:00:00   422 #> # ℹ 3,050 more rows m4_hourly %>%   group_by(id) %>%   plot_time_series(date, log(value),             # Apply a Log Transformation                    .color_var = week(date),      # Color applied to Week transformation                    # Facet formatting                    .facet_ncol = 2,                     .facet_scales = \"free\",                     .interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/TK04_Plotting_Time_Series.html","id":"static-ggplot2-visualizations-customizations","dir":"Articles","previous_headings":"Plotting Time Series","what":"Static ggplot2 Visualizations & Customizations","title":"Visualizing Time Series","text":"visualizations can converted interactive plotly (great exploring shiny apps) static ggplot2 visualizations (great reports).","code":"taylor_30_min %>%   plot_time_series(date, value,                     .color_var = month(date, label = TRUE),                                        # Returns static ggplot                    .interactive = FALSE,                                          # Customization                    .title = \"Taylor's MegaWatt Data\",                    .x_lab = \"Date (30-min intervals)\",                    .y_lab = \"Energy Demand (MW)\",                    .color_lab = \"Month\") +   scale_y_continuous(labels = scales::label_comma())"},{"path":"https://business-science.github.io/timetk/articles/TK04_Plotting_Time_Series.html","id":"box-plots-time-series","dir":"Articles","previous_headings":"","what":"Box Plots (Time Series)","title":"Visualizing Time Series","text":"plot_time_series_boxplot() function can used make box plots. Box plots use aggregation, key parameter defined .period argument.","code":"m4_monthly %>%     group_by(id) %>%     plot_time_series_boxplot(         date, value,         .period      = \"1 year\",         .facet_ncol  = 2,         .interactive = FALSE)"},{"path":"https://business-science.github.io/timetk/articles/TK04_Plotting_Time_Series.html","id":"regression-plots-time-series","dir":"Articles","previous_headings":"","what":"Regression Plots (Time Series)","title":"Visualizing Time Series","text":"time series regression plot, plot_time_series_regression(), can useful quickly assess key features correlated time series. Internally function passes formula stats::lm() function. linear regression summary can output toggling show_summary = TRUE.","code":"m4_monthly %>%     group_by(id) %>%     plot_time_series_regression(         .date_var     = date,         .formula      = log(value) ~ as.numeric(date) + month(date, label = TRUE),         .facet_ncol   = 2,         .interactive  = FALSE,         .show_summary = FALSE     )"},{"path":"https://business-science.github.io/timetk/articles/TK04_Plotting_Time_Series.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Visualizing Time Series","text":"Timetk part amazing Modeltime Ecosystem time series forecasting. can take long time learn: Many algorithms Ensembling Resampling Machine Learning Deep Learning Scalable Modeling: 10,000+ time series probably thinking ever going learn time series forecasting. ’s solution save years struggling.","code":""},{"path":"https://business-science.github.io/timetk/articles/TK04_Plotting_Time_Series.html","id":"take-the-high-performance-forecasting-course","dir":"Articles","previous_headings":"","what":"Take the High-Performance Forecasting Course","title":"Visualizing Time Series","text":"Become forecasting expert organization  High-Performance Time Series Course","code":""},{"path":"https://business-science.github.io/timetk/articles/TK04_Plotting_Time_Series.html","id":"time-series-is-changing","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"Time Series is Changing","title":"Visualizing Time Series","text":"Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies improving accuracy scalability. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System).","code":""},{"path":"https://business-science.github.io/timetk/articles/TK04_Plotting_Time_Series.html","id":"how-to-learn-high-performance-time-series-forecasting","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"How to Learn High-Performance Time Series Forecasting","title":"Visualizing Time Series","text":"teach build HPTFS System High-Performance Time Series Forecasting Course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Become Time Series Expert organization. Take High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/TK05_Plotting_Seasonality_and_Correlation.html","id":"libraries","dir":"Articles","previous_headings":"","what":"Libraries","title":"Plotting Seasonality and Correlation","text":"Run following code set tutorial.","code":"library(dplyr) library(timetk)  # Setup for the plotly charts (# FALSE returns ggplots) interactive <- TRUE"},{"path":[]},{"path":"https://business-science.github.io/timetk/articles/TK05_Plotting_Seasonality_and_Correlation.html","id":"grouped-acf-diagnostics","dir":"Articles","previous_headings":"Correlation Plots","what":"Grouped ACF Diagnostics","title":"Plotting Seasonality and Correlation","text":"","code":"m4_hourly %>%     group_by(id) %>%     plot_acf_diagnostics(         date, value,               # ACF & PACF         .lags = \"7 days\",          # 7-Days of hourly lags         .interactive = interactive     )"},{"path":"https://business-science.github.io/timetk/articles/TK05_Plotting_Seasonality_and_Correlation.html","id":"grouped-ccf-plots","dir":"Articles","previous_headings":"Correlation Plots","what":"Grouped CCF Plots","title":"Plotting Seasonality and Correlation","text":"","code":"walmart_sales_weekly %>%     select(id, Date, Weekly_Sales, Temperature, Fuel_Price) %>%     group_by(id) %>%     plot_acf_diagnostics(         Date, Weekly_Sales,        # ACF & PACF         .ccf_vars    = c(Temperature, Fuel_Price),   # CCFs         .lags        = \"3 months\",    # 3 months of weekly lags         .interactive = interactive     )"},{"path":[]},{"path":"https://business-science.github.io/timetk/articles/TK05_Plotting_Seasonality_and_Correlation.html","id":"seasonal-visualizations","dir":"Articles","previous_headings":"Seasonality","what":"Seasonal Visualizations","title":"Plotting Seasonality and Correlation","text":"","code":"taylor_30_min %>%     plot_seasonal_diagnostics(date, value, .interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/TK05_Plotting_Seasonality_and_Correlation.html","id":"grouped-seasonal-visualizations","dir":"Articles","previous_headings":"Seasonality","what":"Grouped Seasonal Visualizations","title":"Plotting Seasonality and Correlation","text":"","code":"m4_hourly %>%     group_by(id) %>%     plot_seasonal_diagnostics(date, value, .interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/TK05_Plotting_Seasonality_and_Correlation.html","id":"stl-diagnostics","dir":"Articles","previous_headings":"","what":"STL Diagnostics","title":"Plotting Seasonality and Correlation","text":"","code":"m4_hourly %>%     group_by(id) %>%     plot_stl_diagnostics(         date, value,         .frequency = \"auto\", .trend = \"auto\",         .feature_set = c(\"observed\", \"season\", \"trend\", \"remainder\"),         .interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/TK05_Plotting_Seasonality_and_Correlation.html","id":"learning-more","dir":"Articles","previous_headings":"","what":"Learning More","title":"Plotting Seasonality and Correlation","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"prerequisites","dir":"Articles","previous_headings":"","what":"Prerequisites","title":"Frequency and Trend Selection","text":"get started, load following packages.","code":"library(dplyr) library(timetk)"},{"path":"https://business-science.github.io/timetk/articles/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Frequency and Trend Selection","text":"Daily Irregular Data daily stock prices Facebook 2013 2016. Note trading days occur “business days” (non-weekends non-business-holidays). Sub-Daily Data Taylor’s Energy Demand data 30-minute timestamp interval.","code":"data(FANG)  FB_tbl <- FANG %>% dplyr::filter(symbol == \"FB\") FB_tbl ## # A tibble: 1,008 × 8 ##    symbol date        open  high   low close    volume adjusted ##    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl> ##  1 FB     2013-01-02  27.4  28.2  27.4  28    69846400     28   ##  2 FB     2013-01-03  27.9  28.5  27.6  27.8  63140600     27.8 ##  3 FB     2013-01-04  28.0  28.9  27.8  28.8  72715400     28.8 ##  4 FB     2013-01-07  28.7  29.8  28.6  29.4  83781800     29.4 ##  5 FB     2013-01-08  29.5  29.6  28.9  29.1  45871300     29.1 ##  6 FB     2013-01-09  29.7  30.6  29.5  30.6 104787700     30.6 ##  7 FB     2013-01-10  30.6  31.5  30.3  31.3  95316400     31.3 ##  8 FB     2013-01-11  31.3  32.0  31.1  31.7  89598000     31.7 ##  9 FB     2013-01-14  32.1  32.2  30.6  31.0  98892800     31.0 ## 10 FB     2013-01-15  30.6  31.7  29.9  30.1 173242600     30.1 ## # ℹ 998 more rows taylor_30_min ## # A tibble: 4,032 × 2 ##    date                value ##    <dttm>              <dbl> ##  1 2000-06-05 00:00:00 22262 ##  2 2000-06-05 00:30:00 21756 ##  3 2000-06-05 01:00:00 22247 ##  4 2000-06-05 01:30:00 22759 ##  5 2000-06-05 02:00:00 22549 ##  6 2000-06-05 02:30:00 22313 ##  7 2000-06-05 03:00:00 22128 ##  8 2000-06-05 03:30:00 21860 ##  9 2000-06-05 04:00:00 21751 ## 10 2000-06-05 04:30:00 21336 ## # ℹ 4,022 more rows"},{"path":"https://business-science.github.io/timetk/articles/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"applications","dir":"Articles","previous_headings":"","what":"Applications","title":"Frequency and Trend Selection","text":"example automatic frequency detection occurs plot_stl_diagnostics() function.","code":"taylor_30_min %>%     plot_stl_diagnostics(date, value,                           .frequency = \"auto\", .trend = \"auto\",                          .interactive = FALSE) ## frequency = 48 observations per 1 day ## trend = 672 observations per 14 days"},{"path":[]},{"path":"https://business-science.github.io/timetk/articles/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"specifying-a-frequency-or-trend","dir":"Articles","previous_headings":"Automatic Frequency & Trend Selection","what":"Specifying a Frequency or Trend","title":"Frequency and Trend Selection","text":"period argument three basic options returning frequency. Options include: “auto”: target frequency determined using pre-defined Time Scale Template (see ). time-based duration: (e.g. “7 days” “2 quarters” per cycle) numeric number observations: (e.g. 5 5 observations per cycle)","code":""},{"path":"https://business-science.github.io/timetk/articles/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"frequency","dir":"Articles","previous_headings":"Automatic Frequency & Trend Selection","what":"Frequency","title":"Frequency and Trend Selection","text":"frequency loosely defined number observations comprise cycle data set. Using tk_get_frequency(), can pick number observations roughly define frequency series. Daily Irregular Data FB_tbl irregular (weekends holidays present), frequency selected weekly week 5-days typically. 5 selected. Sub-Daily Data works well sub-daily time series. ’ll use taylor_30_min 30-minute timestamp series. frequency selected 48 48 timestamps (observations) 1 day 30-minute cycle.","code":"FB_tbl %>% tk_index() %>% tk_get_frequency(period = \"auto\") ## frequency = 5 observations per 1 week ## [1] 5 taylor_30_min %>% tk_index() %>% tk_get_frequency(\"1 day\") ## frequency = 48 observations per 1 day ## [1] 48"},{"path":"https://business-science.github.io/timetk/articles/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"trend","dir":"Articles","previous_headings":"Automatic Frequency & Trend Selection","what":"Trend","title":"Frequency and Trend Selection","text":"trend loosely defined time span can aggregated across visualize central tendency data. Using tk_get_trend(), can pick number observations help describe trend data. Daily Irregular Data FB_tbl irregular (weekends holidays present), trend selected 3 months week 5-days typically. 64 observations selected. Sub-Daily Data 14-day (2 week) interval selected “30-minute” interval data.","code":"FB_tbl %>% tk_index() %>% tk_get_trend(period = \"auto\") ## trend = 64 observations per 3 months ## [1] 64 taylor_30_min %>% tk_index() %>% tk_get_trend(\"auto\") ## trend = 672 observations per 14 days ## [1] 672"},{"path":"https://business-science.github.io/timetk/articles/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"time-scale-template","dir":"Articles","previous_headings":"","what":"Time Scale Template","title":"Frequency and Trend Selection","text":"Time-Scale Template used get set time scale template, used tk_get_frequency() tk_get_trend() period = \"auto\". predefined template stored function tk_time_scale_template(). default used timetk. Accessing Default Template can access current template get_tk_time_scale_template(). Changing Default Template can modify current template set_tk_time_scale_template().","code":"get_tk_time_scale_template() ## # A tibble: 8 × 3 ##   time_scale frequency trend    ##   <chr>      <chr>     <chr>    ## 1 second     1 hour    12 hours ## 2 minute     1 day     14 days  ## 3 hour       1 day     1 month  ## 4 day        1 week    3 months ## 5 week       1 quarter 1 year   ## 6 month      1 year    5 years  ## 7 quarter    1 year    10 years ## 8 year       5 years   30 years"},{"path":"https://business-science.github.io/timetk/articles/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"learning-more","dir":"Articles","previous_headings":"","what":"Learning More","title":"Frequency and Trend Selection","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/TK07_Time_Series_Data_Wrangling.html","id":"libraries","dir":"Articles","previous_headings":"","what":"Libraries","title":"Time Series Data Wrangling","text":"Load following libraries.","code":"library(dplyr) library(tidyr) library(timetk)"},{"path":"https://business-science.github.io/timetk/articles/TK07_Time_Series_Data_Wrangling.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Time Series Data Wrangling","text":"tutorial use FANG dataset: Daily Irregular (missing business holidays weekends) 4 groups (FB, AMZN, NFLX, GOOG). adjusted column contains adjusted closing prices day.  volume column contains trade volume (number times stock transacted) day.","code":"FANG ## # A tibble: 4,032 × 8 ##    symbol date        open  high   low close    volume adjusted ##    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl> ##  1 FB     2013-01-02  27.4  28.2  27.4  28    69846400     28   ##  2 FB     2013-01-03  27.9  28.5  27.6  27.8  63140600     27.8 ##  3 FB     2013-01-04  28.0  28.9  27.8  28.8  72715400     28.8 ##  4 FB     2013-01-07  28.7  29.8  28.6  29.4  83781800     29.4 ##  5 FB     2013-01-08  29.5  29.6  28.9  29.1  45871300     29.1 ##  6 FB     2013-01-09  29.7  30.6  29.5  30.6 104787700     30.6 ##  7 FB     2013-01-10  30.6  31.5  30.3  31.3  95316400     31.3 ##  8 FB     2013-01-11  31.3  32.0  31.1  31.7  89598000     31.7 ##  9 FB     2013-01-14  32.1  32.2  30.6  31.0  98892800     31.0 ## 10 FB     2013-01-15  30.6  31.7  29.9  30.1 173242600     30.1 ## # ℹ 4,022 more rows FANG %>%   group_by(symbol) %>%   plot_time_series(date, adjusted, .facet_ncol = 2, .interactive = FALSE) FANG %>%   group_by(symbol) %>%   plot_time_series(date, volume, .facet_ncol = 2, .interactive = FALSE)"},{"path":"https://business-science.github.io/timetk/articles/TK07_Time_Series_Data_Wrangling.html","id":"summarize-by-time","dir":"Articles","previous_headings":"","what":"Summarize by Time","title":"Time Series Data Wrangling","text":"summarise_by_time() aggregates period. ’s great : Period Aggregation - sum() Period Smoothing - mean(), first(), last()","code":""},{"path":"https://business-science.github.io/timetk/articles/TK07_Time_Series_Data_Wrangling.html","id":"period-summarization","dir":"Articles","previous_headings":"Summarize by Time","what":"Period Summarization","title":"Time Series Data Wrangling","text":"Objective: Get total trade volume quarter Use sum() Aggregate using .= \"quarter\"","code":"FANG %>%   group_by(symbol) %>%   summarise_by_time(     date,      .by    = \"quarter\",     volume = sum(volume)   ) %>%   plot_time_series(date, volume, .facet_ncol = 2, .interactive = FALSE, .y_intercept = 0)"},{"path":"https://business-science.github.io/timetk/articles/TK07_Time_Series_Data_Wrangling.html","id":"period-smoothing","dir":"Articles","previous_headings":"Summarize by Time","what":"Period Smoothing","title":"Time Series Data Wrangling","text":"Objective: Get first value month can use first() get first value, effect reducing data (.e. smoothing). use mean() median(). Use summarization time: .= \"month\" aggregate month.","code":"FANG %>%   group_by(symbol) %>%   summarise_by_time(     date,      .by = \"month\",     adjusted = first(adjusted)   ) %>%   plot_time_series(date, adjusted, .facet_ncol = 2, .interactive = FALSE)"},{"path":"https://business-science.github.io/timetk/articles/TK07_Time_Series_Data_Wrangling.html","id":"filter-by-time","dir":"Articles","previous_headings":"","what":"Filter By Time","title":"Time Series Data Wrangling","text":"Used quickly filter continuous time range.","code":""},{"path":"https://business-science.github.io/timetk/articles/TK07_Time_Series_Data_Wrangling.html","id":"time-range-filtering","dir":"Articles","previous_headings":"Filter By Time","what":"Time Range Filtering","title":"Time Series Data Wrangling","text":"Objective: Get adjusted stock prices 3rd quarter 2013. .start_date = \"2013-09\": Converts “2013-09-01 .end_date = \"2013\": Converts “2013-12-31 advanced example filtering using %+time %-time shown “Padding Data: Low High Frequency”.","code":"FANG %>%   group_by(symbol) %>%   filter_by_time(date, \"2013-09\", \"2013\") %>%   plot_time_series(date, adjusted, .facet_ncol = 2, .interactive = FALSE)"},{"path":"https://business-science.github.io/timetk/articles/TK07_Time_Series_Data_Wrangling.html","id":"padding-data","dir":"Articles","previous_headings":"","what":"Padding Data","title":"Time Series Data Wrangling","text":"Used fill (pad) gaps go low frequency high frequency. function uses awesome padr library filling expanding timestamps.","code":""},{"path":"https://business-science.github.io/timetk/articles/TK07_Time_Series_Data_Wrangling.html","id":"fill-in-gaps","dir":"Articles","previous_headings":"Padding Data","what":"Fill in Gaps","title":"Time Series Data Wrangling","text":"Objective: Make irregular series regular. leave padded values NA. can add value using .pad_value can impute using function like ts_impute_vec() (shown next).","code":"FANG %>%   group_by(symbol) %>%   pad_by_time(date, .by = \"auto\") # Guesses .by = \"day\" ## pad applied on the interval: day ## # A tibble: 5,836 × 8 ## # Groups:   symbol [4] ##    symbol date        open  high   low close  volume adjusted ##    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>   <dbl>    <dbl> ##  1 AMZN   2013-01-02  256.  258.  253.  257. 3271000     257. ##  2 AMZN   2013-01-03  257.  261.  256.  258. 2750900     258. ##  3 AMZN   2013-01-04  258.  260.  257.  259. 1874200     259. ##  4 AMZN   2013-01-05   NA    NA    NA    NA       NA      NA  ##  5 AMZN   2013-01-06   NA    NA    NA    NA       NA      NA  ##  6 AMZN   2013-01-07  263.  270.  263.  268. 4910000     268. ##  7 AMZN   2013-01-08  267.  269.  264.  266. 3010700     266. ##  8 AMZN   2013-01-09  268.  270.  265.  266. 2265600     266. ##  9 AMZN   2013-01-10  269.  269.  262.  265. 2863400     265. ## 10 AMZN   2013-01-11  265.  268.  264.  268. 2413300     268. ## # ℹ 5,826 more rows"},{"path":"https://business-science.github.io/timetk/articles/TK07_Time_Series_Data_Wrangling.html","id":"low-to-high-frequency","dir":"Articles","previous_headings":"Padding Data","what":"Low to High Frequency","title":"Time Series Data Wrangling","text":"Objective: Go Daily Hourly timestamp intervals 1 month start date. Impute missing values. .= \"hour\" pads daily hourly Imputation hourly data accomplished ts_impute_vec(), performs linear interpolation period = 1. “start”: special keyword signals start series FIRST(date) %+time% \"1 month\": Selecting first date sequence using special infix operation, %+time%, called “add time”. case add “1 month”.","code":"FANG %>%   group_by(symbol) %>%   pad_by_time(date, .by = \"hour\") %>%   mutate_at(vars(open:adjusted), .funs = ts_impute_vec, period = 1) %>%   filter_by_time(date, \"start\", first(date) %+time% \"1 month\") %>%   plot_time_series(date, adjusted, .facet_ncol = 2, .interactive = FALSE)"},{"path":"https://business-science.github.io/timetk/articles/TK07_Time_Series_Data_Wrangling.html","id":"sliding-rolling-calculations","dir":"Articles","previous_headings":"","what":"Sliding (Rolling) Calculations","title":"Time Series Data Wrangling","text":"new function, slidify() turns function sliding (rolling) window function. takes concepts tibbletime::rollify() improves R package slider.","code":""},{"path":"https://business-science.github.io/timetk/articles/TK07_Time_Series_Data_Wrangling.html","id":"rolling-mean","dir":"Articles","previous_headings":"Sliding (Rolling) Calculations","what":"Rolling Mean","title":"Time Series Data Wrangling","text":"Objective: Calculate “centered” simple rolling average partial window rolling start end windows. slidify() turns mean() function rolling average.  simple rolling calculations (rolling average), can accomplish operation faster slidify_vec() - vectorized rolling function simple summary rolls (e.g. mean(), sd(), sum(), etc)","code":"# Make the rolling function roll_avg_30 <- slidify(.f = mean, .period = 30, .align = \"center\", .partial = TRUE)  # Apply the rolling function FANG %>%   select(symbol, date, adjusted) %>%   group_by(symbol) %>%   # Apply Sliding Function   mutate(rolling_avg_30 = roll_avg_30(adjusted)) %>%   tidyr::pivot_longer(cols = c(adjusted, rolling_avg_30)) %>%   plot_time_series(date, value, .color_var = name,                    .facet_ncol = 2, .smooth = FALSE,                     .interactive = FALSE) FANG %>%   select(symbol, date, adjusted) %>%   group_by(symbol) %>%   # Apply roll apply Function   mutate(rolling_avg_30 = slidify_vec(adjusted,  ~ mean(.),                                        .period = 30, .partial = TRUE)) ## # A tibble: 4,032 × 4 ## # Groups:   symbol [4] ##    symbol date       adjusted rolling_avg_30 ##    <chr>  <date>        <dbl>          <dbl> ##  1 FB     2013-01-02     28             30.0 ##  2 FB     2013-01-03     27.8           30.1 ##  3 FB     2013-01-04     28.8           30.2 ##  4 FB     2013-01-07     29.4           30.2 ##  5 FB     2013-01-08     29.1           30.3 ##  6 FB     2013-01-09     30.6           30.3 ##  7 FB     2013-01-10     31.3           30.3 ##  8 FB     2013-01-11     31.7           30.2 ##  9 FB     2013-01-14     31.0           30.1 ## 10 FB     2013-01-15     30.1           30.1 ## # ℹ 4,022 more rows"},{"path":"https://business-science.github.io/timetk/articles/TK07_Time_Series_Data_Wrangling.html","id":"rolling-regression","dir":"Articles","previous_headings":"Sliding (Rolling) Calculations","what":"Rolling Regression","title":"Time Series Data Wrangling","text":"Objective: Calculate rolling regression. complex sliding (rolling) calculation requires multiple columns involved. slidify() built . Use multi-variable purrr ..1, ..2, ..3, etc notation setup function","code":"# Rolling regressions are easy to implement using `.unlist = FALSE` lm_roll <- slidify(~ lm(..1 ~ ..2 + ..3), .period = 90,                     .unlist = FALSE, .align = \"right\")   FANG %>%   select(symbol, date, adjusted, volume) %>%   group_by(symbol) %>%   mutate(numeric_date = as.numeric(date)) %>%   # Apply rolling regression   mutate(rolling_lm = lm_roll(adjusted, volume, numeric_date)) %>%   filter(!is.na(rolling_lm)) ## # A tibble: 3,676 × 6 ## # Groups:   symbol [4] ##    symbol date       adjusted   volume numeric_date rolling_lm ##    <chr>  <date>        <dbl>    <dbl>        <dbl> <list>     ##  1 FB     2013-05-10     26.7 30847100        15835 <lm>       ##  2 FB     2013-05-13     26.8 29068800        15838 <lm>       ##  3 FB     2013-05-14     27.1 24930300        15839 <lm>       ##  4 FB     2013-05-15     26.6 30299800        15840 <lm>       ##  5 FB     2013-05-16     26.1 35499100        15841 <lm>       ##  6 FB     2013-05-17     26.2 29462700        15842 <lm>       ##  7 FB     2013-05-20     25.8 42402900        15845 <lm>       ##  8 FB     2013-05-21     25.7 26261300        15846 <lm>       ##  9 FB     2013-05-22     25.2 45314500        15847 <lm>       ## 10 FB     2013-05-23     25.1 37663100        15848 <lm>       ## # ℹ 3,666 more rows"},{"path":"https://business-science.github.io/timetk/articles/TK07_Time_Series_Data_Wrangling.html","id":"learning-more","dir":"Articles","previous_headings":"","what":"Learning More","title":"Time Series Data Wrangling","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/TK08_Automatic_Anomaly_Detection.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Anomaly Detection","text":"tutorial use wikipedia_traffic_daily dataset:","code":"wikipedia_traffic_daily %>% glimpse() ## Rows: 5,500 ## Columns: 3 ## $ Page  <chr> \"Death_of_Freddie_Gray_en.wikipedia.org_mobile-web_all-agents\", … ## $ date  <date> 2015-07-01, 2015-07-02, 2015-07-03, 2015-07-04, 2015-07-05, 201… ## $ value <dbl> 791, 704, 903, 732, 558, 504, 543, 1156, 1196, 701, 829, 838, 11…"},{"path":"https://business-science.github.io/timetk/articles/TK08_Automatic_Anomaly_Detection.html","id":"visualization","dir":"Articles","previous_headings":"","what":"Visualization","title":"Anomaly Detection","text":"Using plot_time_series() function, can interactively detect anomalies scale.","code":"wikipedia_traffic_daily %>%   group_by(Page) %>%   plot_time_series(date, value, .facet_ncol = 2)"},{"path":"https://business-science.github.io/timetk/articles/TK08_Automatic_Anomaly_Detection.html","id":"anomalize-breakdown-identify-and-clean-in-1-easy-step","dir":"Articles","previous_headings":"","what":"Anomalize: breakdown, identify, and clean in 1 easy step","title":"Anomaly Detection","text":"anomalize() function feature rich tool performing anomaly detection. Anomalize group-aware, can use part normal pandas groupby chain. one easy step: breakdown (decompose) time series Analyze ’s remainder (residuals) spikes (anomalies) Clean anomalies desired anomalize() function returns: original grouping datetime columns. seasonal decomposition: observed, seasonal, seasadj, trend, remainder. objective remove trend seasonality remainder stationary representative normal variation anomalous variations. Anomaly identification scoring: anomaly, anomaly_score, anomaly_direction. identify anomaly decision (Yes/), score anomaly distance centerline, label direction (-1 (), zero (anomalous), +1 ()). Recomposition: recomposed_l1 recomposed_l2. Think lower upper bands. observed data l1 l2 anomalous. Cleaned data: observed_clean. Cleaned data automatically provided, outliers replaced data within recomposed l1/l2 boundaries. said, always first seek understand data considered anomalous simply removing outliers using cleaned data. important aspect data ready visualized, inspected, modifications can made address tweaks like make.","code":"anomalize_tbl <- wikipedia_traffic_daily %>%   group_by(Page) %>%   anomalize(       .date_var      = date,        .value         = value,       .iqr_alpha     = 0.05,       .max_anomalies = 0.20,       .message       = FALSE   )  anomalize_tbl %>% glimpse() ## Rows: 5,500 ## Columns: 13 ## Groups: Page [10] ## $ Page              <chr> \"Death_of_Freddie_Gray_en.wikipedia.org_mobile-web_a… ## $ date              <date> 2015-07-01, 2015-07-02, 2015-07-03, 2015-07-04, 201… ## $ observed          <dbl> 791, 704, 903, 732, 558, 504, 543, 1156, 1196, 701, … ## $ season            <dbl> 9.2554027, 13.1448419, -20.5421912, -24.0966839, 0.4… ## $ trend             <dbl> 748.1907, 744.3101, 740.4296, 736.5490, 732.6684, 72… ## $ remainder         <dbl> 33.553930, -53.454953, 183.112637, 19.547687, -175.1… ## $ seasadj           <dbl> 781.7446, 690.8552, 923.5422, 756.0967, 557.5091, 50… ## $ anomaly           <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"… ## $ anomaly_direction <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ anomaly_score     <dbl> 83.50187, 170.51076, 66.05683, 97.50812, 292.21517, … ## $ recomposed_l1     <dbl> -328.3498, -328.3409, -365.9085, -373.3435, -352.636… ## $ recomposed_l2     <dbl> 2077.354, 2077.362, 2039.795, 2032.360, 2053.067, 20… ## $ observed_clean    <dbl> 791.000, 704.000, 903.000, 732.000, 558.000, 504.000…"},{"path":"https://business-science.github.io/timetk/articles/TK08_Automatic_Anomaly_Detection.html","id":"anomaly-visualization-1-seasonal-decomposition-plot","dir":"Articles","previous_headings":"","what":"Anomaly Visualization 1: Seasonal Decomposition Plot","title":"Anomaly Detection","text":"first step normal process analyze seasonal decomposition. want see remainders look like, make sure trend seasonality removed remainder centered around zero.","code":"anomalize_tbl %>%     group_by(Page) %>%     plot_anomalies_decomp(         .date_var = date,          .interactive = FALSE     )"},{"path":"https://business-science.github.io/timetk/articles/TK08_Automatic_Anomaly_Detection.html","id":"anomaly-visualization-2-anomaly-detection-plot","dir":"Articles","previous_headings":"","what":"Anomaly Visualization 2: Anomaly Detection Plot","title":"Anomaly Detection","text":"’m satisfied remainders, next step visualize anomalies. ’m looking see need grow shrink remainder l1 l2 bands, classify anomalies.","code":"anomalize_tbl %>%     group_by(Page) %>%     plot_anomalies(         date,         .facet_ncol = 2     )"},{"path":"https://business-science.github.io/timetk/articles/TK08_Automatic_Anomaly_Detection.html","id":"anomaly-visualization-3-anomalies-cleaned-plot","dir":"Articles","previous_headings":"","what":"Anomaly Visualization 3: Anomalies Cleaned Plot","title":"Anomaly Detection","text":"pros cons cleaning anomalies. ’ll leave discussion another time. , interested seeing data looks like cleaned (outliers removed), plot help compare .","code":"anomalize_tbl %>%     group_by(Page) %>%     plot_anomalies_cleaned(         date,         .facet_ncol = 2     )"},{"path":"https://business-science.github.io/timetk/articles/TK08_Automatic_Anomaly_Detection.html","id":"learning-more","dir":"Articles","previous_headings":"","what":"Learning More","title":"Anomaly Detection","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/TK09_Clustering.html","id":"libraries","dir":"Articles","previous_headings":"","what":"Libraries","title":"Time Series Clustering","text":"get started, load following libraries.","code":"library(dplyr) library(purrr) library(timetk)"},{"path":"https://business-science.github.io/timetk/articles/TK09_Clustering.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Time Series Clustering","text":"tutorial use walmart_sales_weekly dataset: Weekly Sales spikes various events","code":"walmart_sales_weekly ## # A tibble: 1,001 × 17 ##    id    Store  Dept Date       Weekly_Sales IsHoliday Type    Size Temperature ##    <fct> <dbl> <dbl> <date>            <dbl> <lgl>     <chr>  <dbl>       <dbl> ##  1 1_1       1     1 2010-02-05       24924. FALSE     A     151315        42.3 ##  2 1_1       1     1 2010-02-12       46039. TRUE      A     151315        38.5 ##  3 1_1       1     1 2010-02-19       41596. FALSE     A     151315        39.9 ##  4 1_1       1     1 2010-02-26       19404. FALSE     A     151315        46.6 ##  5 1_1       1     1 2010-03-05       21828. FALSE     A     151315        46.5 ##  6 1_1       1     1 2010-03-12       21043. FALSE     A     151315        57.8 ##  7 1_1       1     1 2010-03-19       22137. FALSE     A     151315        54.6 ##  8 1_1       1     1 2010-03-26       26229. FALSE     A     151315        51.4 ##  9 1_1       1     1 2010-04-02       57258. FALSE     A     151315        62.3 ## 10 1_1       1     1 2010-04-09       42961. FALSE     A     151315        65.9 ## # ℹ 991 more rows ## # ℹ 8 more variables: Fuel_Price <dbl>, MarkDown1 <dbl>, MarkDown2 <dbl>, ## #   MarkDown3 <dbl>, MarkDown4 <dbl>, MarkDown5 <dbl>, CPI <dbl>, ## #   Unemployment <dbl>"},{"path":"https://business-science.github.io/timetk/articles/TK09_Clustering.html","id":"ts-features","dir":"Articles","previous_headings":"","what":"TS Features","title":"Time Series Clustering","text":"Using tk_tsfeatures() function, can quickly get “tsfeatures” time series. important points: features parameter come tsfeatures R package. Use one function names tsfeatures R package e.g.(“lumpiness”, “stl_features”). can supply function returns aggregation (e.g. “mean” apply base::mean() function). can supply custom functions creating function providing (e.g. my_mean() defined )","code":"# Custom Function my_mean <- function(x, na.rm=TRUE) {   mean(x, na.rm = na.rm) }  tsfeature_tbl <- walmart_sales_weekly %>%     group_by(id) %>%     tk_tsfeatures(       .date_var = Date,       .value    = Weekly_Sales,       .period   = 52,       .features = c(\"frequency\", \"stl_features\", \"entropy\", \"acf_features\", \"my_mean\"),       .scale    = TRUE,       .prefix   = \"ts_\"     ) %>%     ungroup()  tsfeature_tbl ## # A tibble: 7 × 22 ##   id    ts_frequency ts_nperiods ts_seasonal_period ts_trend    ts_spike ##   <fct>        <dbl>       <dbl>              <dbl>    <dbl>       <dbl> ## 1 1_1             52           1                 52 0.000670 0.0000280   ## 2 1_3             52           1                 52 0.0614   0.00000987  ## 3 1_8             52           1                 52 0.756    0.00000195  ## 4 1_13            52           1                 52 0.354    0.00000475  ## 5 1_38            52           1                 52 0.425    0.0000179   ## 6 1_93            52           1                 52 0.791    0.000000754 ## 7 1_95            52           1                 52 0.639    0.000000567 ## # ℹ 16 more variables: ts_linearity <dbl>, ts_curvature <dbl>, ts_e_acf1 <dbl>, ## #   ts_e_acf10 <dbl>, ts_seasonal_strength <dbl>, ts_peak <dbl>, ## #   ts_trough <dbl>, ts_entropy <dbl>, ts_x_acf1 <dbl>, ts_x_acf10 <dbl>, ## #   ts_diff1_acf1 <dbl>, ts_diff1_acf10 <dbl>, ts_diff2_acf1 <dbl>, ## #   ts_diff2_acf10 <dbl>, ts_seas_acf1 <dbl>, ts_my_mean <dbl>"},{"path":"https://business-science.github.io/timetk/articles/TK09_Clustering.html","id":"clustering-with-k-means","dir":"Articles","previous_headings":"","what":"Clustering with K-Means","title":"Time Series Clustering","text":"can quickly add cluster assignments kmeans() function tidyverse data wrangling.","code":"set.seed(123)   cluster_tbl <- tibble(     cluster = tsfeature_tbl %>%          select(-id) %>%         as.matrix() %>%         kmeans(centers = 3, nstart = 100) %>%         pluck(\"cluster\") ) %>%     bind_cols(         tsfeature_tbl     )  cluster_tbl ## # A tibble: 7 × 23 ##   cluster id    ts_frequency ts_nperiods ts_seasonal_period ts_trend    ts_spike ##     <int> <fct>        <dbl>       <dbl>              <dbl>    <dbl>       <dbl> ## 1       2 1_1             52           1                 52 0.000670 0.0000280   ## 2       2 1_3             52           1                 52 0.0614   0.00000987  ## 3       2 1_8             52           1                 52 0.756    0.00000195  ## 4       1 1_13            52           1                 52 0.354    0.00000475  ## 5       3 1_38            52           1                 52 0.425    0.0000179   ## 6       3 1_93            52           1                 52 0.791    0.000000754 ## 7       1 1_95            52           1                 52 0.639    0.000000567 ## # ℹ 16 more variables: ts_linearity <dbl>, ts_curvature <dbl>, ts_e_acf1 <dbl>, ## #   ts_e_acf10 <dbl>, ts_seasonal_strength <dbl>, ts_peak <dbl>, ## #   ts_trough <dbl>, ts_entropy <dbl>, ts_x_acf1 <dbl>, ts_x_acf10 <dbl>, ## #   ts_diff1_acf1 <dbl>, ts_diff1_acf10 <dbl>, ts_diff2_acf1 <dbl>, ## #   ts_diff2_acf10 <dbl>, ts_seas_acf1 <dbl>, ts_my_mean <dbl>"},{"path":"https://business-science.github.io/timetk/articles/TK09_Clustering.html","id":"visualize-the-cluster-assignments","dir":"Articles","previous_headings":"","what":"Visualize the Cluster Assignments","title":"Time Series Clustering","text":"Finally, can visualize cluster assignments joining cluster_tbl original walmart_sales_weekly plotting plot_time_series().","code":"cluster_tbl %>%     select(cluster, id) %>%     right_join(walmart_sales_weekly, by = \"id\") %>%     group_by(id) %>%     plot_time_series(       Date, Weekly_Sales,        .color_var   = cluster,        .facet_ncol  = 2,        .interactive = FALSE     )"},{"path":"https://business-science.github.io/timetk/articles/TK09_Clustering.html","id":"learning-more","dir":"Articles","previous_headings":"","what":"Learning More","title":"Time Series Clustering","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"introduction","dir":"Articles > Temp_archive","previous_headings":"","what":"Introduction","title":"Time Series Class Conversion","text":"time series landscape R vast, deep, complex causing many inconsistencies data attributes formats ultimately making difficult coerce different data structures. zoo xts packages solved number issues dealing various classes (ts, zoo, xts, irts, msts, list goes …). However, packages deal classes data frame, issues conversion tbl time series object classes still present. timetk package provides tools solve issues conversion, maximizing attribute extensibility (required data attributes retained conversion primary time series classes). following tools available coerce retrieve key information: Conversion functions: tk_tbl, tk_ts, tk_xts, tk_zoo, tk_zooreg. functions coerce time-based tibbles tbl main time-series data types xts, zoo, zooreg, ts, maintaining time-based index. Index function: tk_index returns index. argument, timetk_idx = TRUE, time-based index (non-regularized index) forecast objects, models, ts objects returned present. Refer tk_ts() learn non-regularized index persistence conversion process. vignette includes brief case study conversion issues detailed explanation timetk function conversion time-based tbl objects several primary time series classes (xts, zoo, zooreg ts).","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"prerequisites","dir":"Articles > Temp_archive","previous_headings":"","what":"Prerequisites","title":"Time Series Class Conversion","text":"get started, load following packages.","code":"library(dplyr) library(timetk)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"data","dir":"Articles > Temp_archive","previous_headings":"","what":"Data","title":"Time Series Class Conversion","text":"’ll use “Q10” dataset - first ID sample quarterly datasets (see m4_quarterly) M4 Competition. return structure tibble, conducive many popular time series analysis packages including quantmod, TTR, forecast many others.","code":"q10_quarterly <- m4_quarterly %>% filter(id == \"Q10\") q10_quarterly ## # A tibble: 59 × 3 ##    id    date       value ##    <fct> <date>     <dbl> ##  1 Q10   2000-01-01 2329  ##  2 Q10   2000-04-01 2350. ##  3 Q10   2000-07-01 2333. ##  4 Q10   2000-10-01 2382. ##  5 Q10   2001-01-01 2383. ##  6 Q10   2001-04-01 2405  ##  7 Q10   2001-07-01 2411  ##  8 Q10   2001-10-01 2428. ##  9 Q10   2002-01-01 2392. ## 10 Q10   2002-04-01 2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"case-study-conversion-issues-with-ts","dir":"Articles > Temp_archive","previous_headings":"","what":"Case Study: Conversion issues with ts()","title":"Time Series Class Conversion","text":"ts object class roots stats package many popular packages use time series data structure including popular forecast package. said, ts data structure difficult coerce back forth default contain time-based index. Rather uses regularized index computed using start frequency arguments. Conversion ts done using ts() function stats library, results various problems.","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"problems","dir":"Articles > Temp_archive","previous_headings":"Case Study: Conversion issues with ts()","what":"Problems","title":"Time Series Class Conversion","text":"First, numeric columns get coerced. user forgets add [,\"pct\"] drop “date” column, ts() returns dates numeric format user wants. correct method call specific column desired. However, presents new issue. date index lost, different “regularized” index built using start frequency attributes. can see structure (using str() function) regularized time series present, date index retained. can get index using index() function zoo package. index retained regular sequence numeric values. many cases, regularized values coerced back original time-base date date time data contains significantly information (.e. year-month-day, hour-minute-second, timezone attributes) data may regularized interval (frequency).","code":"# date column gets coerced to numeric ts(q10_quarterly, start = c(2000, 1), freq = 4) %>%     head() ##         id  date  value ## 2000 Q1  1 10957 2329.0 ## 2000 Q2  1 11048 2349.9 ## 2000 Q3  1 11139 2332.9 ## 2000 Q4  1 11231 2381.5 ## 2001 Q1  1 11323 2382.6 ## 2001 Q2  1 11413 2405.0 q10_quarterly_ts <- ts(q10_quarterly$value, start = c(2000, 1), freq  = 4) q10_quarterly_ts ##        Qtr1   Qtr2   Qtr3   Qtr4 ## 2000 2329.0 2349.9 2332.9 2381.5 ## 2001 2382.6 2405.0 2411.0 2428.5 ## 2002 2391.6 2418.5 2406.5 2418.5 ## 2003 2420.4 2438.6 2448.7 2470.6 ## 2004 2484.5 2495.9 2492.5 2521.6 ## 2005 2538.1 2549.7 2587.2 2585.0 ## 2006 2602.6 2615.3 2654.0 2680.8 ## 2007 2665.4 2645.1 2647.5 2719.2 ## 2008 2677.0 2650.9 2667.8 2660.2 ## 2009 2554.7 2522.7 2510.0 2541.7 ## 2010 2499.1 2527.9 2519.0 2536.3 ## 2011 2493.2 2542.1 2501.6 2516.3 ## 2012 2510.5 2548.4 2548.6 2530.7 ## 2013 2497.1 2520.4 2516.9 2505.5 ## 2014 2513.9 2549.9 2555.3 # No date index attribute str(q10_quarterly_ts) ##  Time-Series [1:59] from 2000 to 2014: 2329 2350 2333 2382 2383 ... # Regularized numeric sequence zoo::index(q10_quarterly_ts) ##  [1] 2000.00 2000.25 2000.50 2000.75 2001.00 2001.25 2001.50 2001.75 2002.00 ## [10] 2002.25 2002.50 2002.75 2003.00 2003.25 2003.50 2003.75 2004.00 2004.25 ## [19] 2004.50 2004.75 2005.00 2005.25 2005.50 2005.75 2006.00 2006.25 2006.50 ## [28] 2006.75 2007.00 2007.25 2007.50 2007.75 2008.00 2008.25 2008.50 2008.75 ## [37] 2009.00 2009.25 2009.50 2009.75 2010.00 2010.25 2010.50 2010.75 2011.00 ## [46] 2011.25 2011.50 2011.75 2012.00 2012.25 2012.50 2012.75 2013.00 2013.25 ## [55] 2013.50 2013.75 2014.00 2014.25 2014.50"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"solution","dir":"Articles > Temp_archive","previous_headings":"Case Study: Conversion issues with ts()","what":"Solution","title":"Time Series Class Conversion","text":"timetk package contains new function, tk_ts(), enables maintaining original date index attribute. repeat tbl ts conversion process using new function, tk_ts(), can see differences. First, numeric columns get coerced, prevents unintended consequences due R conversion rules (e.g. dates getting unintentionally converted characters causing homogeneous data structure converting numeric values character). column dropped, user gets warning. Second, data returned additional attributes. important numeric attribute, “index”, contains original date information number. ts() function preserve index tk_ts() preserve index numeric form along time zone class.","code":"# date automatically dropped and user is warned q10_quarterly_ts_timetk <- tk_ts(q10_quarterly, start = 2000, freq  = 4) ## Warning: Non-numeric columns being dropped: id, date q10_quarterly_ts_timetk ##        Qtr1   Qtr2   Qtr3   Qtr4 ## 2000 2329.0 2349.9 2332.9 2381.5 ## 2001 2382.6 2405.0 2411.0 2428.5 ## 2002 2391.6 2418.5 2406.5 2418.5 ## 2003 2420.4 2438.6 2448.7 2470.6 ## 2004 2484.5 2495.9 2492.5 2521.6 ## 2005 2538.1 2549.7 2587.2 2585.0 ## 2006 2602.6 2615.3 2654.0 2680.8 ## 2007 2665.4 2645.1 2647.5 2719.2 ## 2008 2677.0 2650.9 2667.8 2660.2 ## 2009 2554.7 2522.7 2510.0 2541.7 ## 2010 2499.1 2527.9 2519.0 2536.3 ## 2011 2493.2 2542.1 2501.6 2516.3 ## 2012 2510.5 2548.4 2548.6 2530.7 ## 2013 2497.1 2520.4 2516.9 2505.5 ## 2014 2513.9 2549.9 2555.3 # More attributes including time index, time class, time zone str(q10_quarterly_ts_timetk) ##  Time-Series [1:59, 1] from 2000 to 2014: 2329 2350 2333 2382 2383 ... ##  - attr(*, \"dimnames\")=List of 2 ##   ..$ : NULL ##   ..$ : chr \"value\" ##  - attr(*, \"index\")= num [1:59] 9.47e+08 9.55e+08 9.62e+08 9.70e+08 9.78e+08 ... ##   ..- attr(*, \"tzone\")= chr \"UTC\" ##   ..- attr(*, \"tclass\")= chr \"Date\""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"advantages-of-conversion-with-tk_tbl","dir":"Articles > Temp_archive","previous_headings":"Case Study: Conversion issues with ts()","what":"Advantages of conversion with tk_tbl()","title":"Time Series Class Conversion","text":"Since used tk_ts() conversion, can extract original index date format using tk_index(timetk_idx = TRUE) (default timetk_idx = FALSE returns default regularized index). Next, tk_tbl() function argument timetk_idx also can used select index return. First, show conversion using default index. Notice index returned “regularized” meaning actually numeric index rather time-based index. can now get original date index using tk_tbl() argument timetk_idx = TRUE. can see case (cases) can get data frame began .","code":"# Can now retrieve the original date index timetk_index <- q10_quarterly_ts_timetk %>%     tk_index(timetk_idx = TRUE) ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent head(timetk_index) ## [1] \"2000-01-01\" \"2000-04-01\" \"2000-07-01\" \"2000-10-01\" \"2001-01-01\" ## [6] \"2001-04-01\" class(timetk_index) ## [1] \"Date\" # Conversion back to tibble using the default index (regularized) q10_quarterly_ts_timetk %>%     tk_tbl(index_rename = \"date\", timetk_idx = FALSE) ## # A tibble: 59 × 2 ##    index     value ##    <yearqtr> <dbl> ##  1 2000 Q1   2329  ##  2 2000 Q2   2350. ##  3 2000 Q3   2333. ##  4 2000 Q4   2382. ##  5 2001 Q1   2383. ##  6 2001 Q2   2405  ##  7 2001 Q3   2411  ##  8 2001 Q4   2428. ##  9 2002 Q1   2392. ## 10 2002 Q2   2418. ## # ℹ 49 more rows # Conversion back to tibble now using the timetk index (date / date-time) q10_quarterly_timetk <- q10_quarterly_ts_timetk %>%     tk_tbl(timetk_idx = TRUE) %>%     rename(date = index) ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent q10_quarterly_timetk ## # A tibble: 59 × 2 ##    date       value ##    <date>     <dbl> ##  1 2000-01-01 2329  ##  2 2000-04-01 2350. ##  3 2000-07-01 2333. ##  4 2000-10-01 2382. ##  5 2001-01-01 2383. ##  6 2001-04-01 2405  ##  7 2001-07-01 2411  ##  8 2001-10-01 2428. ##  9 2002-01-01 2392. ## 10 2002-04-01 2418. ## # ℹ 49 more rows # Comparing the coerced tibble with the original tibble identical(q10_quarterly_timetk, q10_quarterly %>% select(-id)) ## [1] TRUE"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"conversion-methods","dir":"Articles > Temp_archive","previous_headings":"","what":"Conversion Methods","title":"Time Series Class Conversion","text":"Using q10_quarterly, ’ll go various conversion methods using tk_tbl, tk_xts, tk_zoo, tk_zooreg, tk_ts.","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"from-tbl","dir":"Articles > Temp_archive","previous_headings":"Conversion Methods","what":"From tbl","title":"Time Series Class Conversion","text":"starting point q10_quarterly. coerce xts, zoo, zooreg ts classes.","code":"# Start: q10_quarterly ## # A tibble: 59 × 3 ##    id    date       value ##    <fct> <date>     <dbl> ##  1 Q10   2000-01-01 2329  ##  2 Q10   2000-04-01 2350. ##  3 Q10   2000-07-01 2333. ##  4 Q10   2000-10-01 2382. ##  5 Q10   2001-01-01 2383. ##  6 Q10   2001-04-01 2405  ##  7 Q10   2001-07-01 2411  ##  8 Q10   2001-10-01 2428. ##  9 Q10   2002-01-01 2392. ## 10 Q10   2002-04-01 2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"to-xts","dir":"Articles > Temp_archive","previous_headings":"Conversion Methods > From tbl","what":"to xts","title":"Time Series Class Conversion","text":"Use tk_xts(). default “date” used date index “date” column dropped output. numeric columns coerced avoid unintentional conversion issues. Use select argument specify columns drop. Use date_var argument specify column use date index. Notice message warning longer present. Also, alternative, can set silent = TRUE bypass warnings since default dropping “date” column desired. Notice warnings messages.","code":"# End q10_quarterly_xts <- tk_xts(q10_quarterly) ## Warning: Non-numeric columns being dropped: id, date ## Using column `date` for date_var. head(q10_quarterly_xts) ##             value ## 2000-01-01 2329.0 ## 2000-04-01 2349.9 ## 2000-07-01 2332.9 ## 2000-10-01 2381.5 ## 2001-01-01 2382.6 ## 2001-04-01 2405.0 # End - Using `select` and `date_var` args tk_xts(q10_quarterly, select = -(id:date), date_var = date) %>%     head() ##             value ## 2000-01-01 2329.0 ## 2000-04-01 2349.9 ## 2000-07-01 2332.9 ## 2000-10-01 2381.5 ## 2001-01-01 2382.6 ## 2001-04-01 2405.0 # End - Using `silent` to silence warnings tk_xts(q10_quarterly, silent = TRUE) %>%     head() ##             value ## 2000-01-01 2329.0 ## 2000-04-01 2349.9 ## 2000-07-01 2332.9 ## 2000-10-01 2381.5 ## 2001-01-01 2382.6 ## 2001-04-01 2405.0"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"to-zoo","dir":"Articles > Temp_archive","previous_headings":"Conversion Methods > From tbl","what":"to zoo","title":"Time Series Class Conversion","text":"Use tk_zoo(). coercing xts, non-numeric “date” column automatically dropped index automatically selected date column.","code":"# End q10_quarterly_zoo <- tk_zoo(q10_quarterly, silent = TRUE)  head(q10_quarterly_zoo) ##             value ## 2000-01-01 2329.0 ## 2000-04-01 2349.9 ## 2000-07-01 2332.9 ## 2000-10-01 2381.5 ## 2001-01-01 2382.6 ## 2001-04-01 2405.0"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"to-zooreg","dir":"Articles > Temp_archive","previous_headings":"Conversion Methods > From tbl","what":"to zooreg","title":"Time Series Class Conversion","text":"Use tk_zooreg(). coercing xts, non-numeric “date” column automatically dropped. regularized index built function arguments start freq. original time-based index retained can accessed using tk_index(timetk_idx = TRUE).","code":"# End q10_quarterly_zooreg <- tk_zooreg(q10_quarterly, start = 2000, freq = 4, silent = TRUE)  head(q10_quarterly_zooreg) ##          value ## 2000 Q1 2329.0 ## 2000 Q2 2349.9 ## 2000 Q3 2332.9 ## 2000 Q4 2381.5 ## 2001 Q1 2382.6 ## 2001 Q2 2405.0 # Retrieve original time-based index tk_index(q10_quarterly_zooreg, timetk_idx = TRUE) %>%     str() ##  Date[1:59], format: \"2000-01-01\" \"2000-04-01\" \"2000-07-01\" \"2000-10-01\" \"2001-01-01\" ..."},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"to-ts","dir":"Articles > Temp_archive","previous_headings":"Conversion Methods > From tbl","what":"to ts","title":"Time Series Class Conversion","text":"Use tk_ts(). non-numeric “date” column automatically dropped. regularized index built function arguments. original time-based index retained can accessed using tk_index(timetk_idx = TRUE).","code":"# End q10_quarterly_ts <- tk_ts(q10_quarterly, start = 2000, freq = 4, silent = TRUE)  q10_quarterly_ts ##        Qtr1   Qtr2   Qtr3   Qtr4 ## 2000 2329.0 2349.9 2332.9 2381.5 ## 2001 2382.6 2405.0 2411.0 2428.5 ## 2002 2391.6 2418.5 2406.5 2418.5 ## 2003 2420.4 2438.6 2448.7 2470.6 ## 2004 2484.5 2495.9 2492.5 2521.6 ## 2005 2538.1 2549.7 2587.2 2585.0 ## 2006 2602.6 2615.3 2654.0 2680.8 ## 2007 2665.4 2645.1 2647.5 2719.2 ## 2008 2677.0 2650.9 2667.8 2660.2 ## 2009 2554.7 2522.7 2510.0 2541.7 ## 2010 2499.1 2527.9 2519.0 2536.3 ## 2011 2493.2 2542.1 2501.6 2516.3 ## 2012 2510.5 2548.4 2548.6 2530.7 ## 2013 2497.1 2520.4 2516.9 2505.5 ## 2014 2513.9 2549.9 2555.3 # Retrieve original time-based index tk_index(q10_quarterly_ts, timetk_idx = TRUE) %>%     str() ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent ##  Date[1:59], format: \"2000-01-01\" \"2000-04-01\" \"2000-07-01\" \"2000-10-01\" \"2001-01-01\" ..."},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"to-tbl","dir":"Articles > Temp_archive","previous_headings":"Conversion Methods","what":"To tbl","title":"Time Series Class Conversion","text":"Going back tibble just easy using tk_tbl().","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"from-xts","dir":"Articles > Temp_archive","previous_headings":"Conversion Methods > To tbl","what":"From xts","title":"Time Series Class Conversion","text":"Notice loss data going back tbl.","code":"# Start head(q10_quarterly_xts) ##             value ## 2000-01-01 2329.0 ## 2000-04-01 2349.9 ## 2000-07-01 2332.9 ## 2000-10-01 2381.5 ## 2001-01-01 2382.6 ## 2001-04-01 2405.0 # End tk_tbl(q10_quarterly_xts) ## # A tibble: 59 × 2 ##    index      value ##    <date>     <dbl> ##  1 2000-01-01 2329  ##  2 2000-04-01 2350. ##  3 2000-07-01 2333. ##  4 2000-10-01 2382. ##  5 2001-01-01 2383. ##  6 2001-04-01 2405  ##  7 2001-07-01 2411  ##  8 2001-10-01 2428. ##  9 2002-01-01 2392. ## 10 2002-04-01 2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"from-zoo","dir":"Articles > Temp_archive","previous_headings":"Conversion Methods > To tbl","what":"From zoo","title":"Time Series Class Conversion","text":"Notice loss data going back tbl.","code":"# Start head(q10_quarterly_zoo) ##             value ## 2000-01-01 2329.0 ## 2000-04-01 2349.9 ## 2000-07-01 2332.9 ## 2000-10-01 2381.5 ## 2001-01-01 2382.6 ## 2001-04-01 2405.0 # End tk_tbl(q10_quarterly_zoo) ## # A tibble: 59 × 2 ##    index      value ##    <date>     <dbl> ##  1 2000-01-01 2329  ##  2 2000-04-01 2350. ##  3 2000-07-01 2333. ##  4 2000-10-01 2382. ##  5 2001-01-01 2383. ##  6 2001-04-01 2405  ##  7 2001-07-01 2411  ##  8 2001-10-01 2428. ##  9 2002-01-01 2392. ## 10 2002-04-01 2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"from-zooreg","dir":"Articles > Temp_archive","previous_headings":"Conversion Methods > To tbl","what":"From zooreg","title":"Time Series Class Conversion","text":"Notice index regularized numeric sequence default. timetk_idx = TRUE index original date sequence. result original tbl started !","code":"# Start head(q10_quarterly_zooreg) ##          value ## 2000 Q1 2329.0 ## 2000 Q2 2349.9 ## 2000 Q3 2332.9 ## 2000 Q4 2381.5 ## 2001 Q1 2382.6 ## 2001 Q2 2405.0 # End - with default regularized index tk_tbl(q10_quarterly_zooreg) ## # A tibble: 59 × 2 ##    index     value ##    <yearqtr> <dbl> ##  1 2000 Q1   2329  ##  2 2000 Q2   2350. ##  3 2000 Q3   2333. ##  4 2000 Q4   2382. ##  5 2001 Q1   2383. ##  6 2001 Q2   2405  ##  7 2001 Q3   2411  ##  8 2001 Q4   2428. ##  9 2002 Q1   2392. ## 10 2002 Q2   2418. ## # ℹ 49 more rows # End - with timetk index that is the same as original time-based index tk_tbl(q10_quarterly_zooreg, timetk_idx = TRUE) ## # A tibble: 59 × 2 ##    index      value ##    <date>     <dbl> ##  1 2000-01-01 2329  ##  2 2000-04-01 2350. ##  3 2000-07-01 2333. ##  4 2000-10-01 2382. ##  5 2001-01-01 2383. ##  6 2001-04-01 2405  ##  7 2001-07-01 2411  ##  8 2001-10-01 2428. ##  9 2002-01-01 2392. ## 10 2002-04-01 2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"from-ts","dir":"Articles > Temp_archive","previous_headings":"Conversion Methods > To tbl","what":"From ts","title":"Time Series Class Conversion","text":"Notice index regularized numeric sequence default. timetk_idx = TRUE index original date sequence. result original tbl started !","code":"# Start q10_quarterly_ts ##        Qtr1   Qtr2   Qtr3   Qtr4 ## 2000 2329.0 2349.9 2332.9 2381.5 ## 2001 2382.6 2405.0 2411.0 2428.5 ## 2002 2391.6 2418.5 2406.5 2418.5 ## 2003 2420.4 2438.6 2448.7 2470.6 ## 2004 2484.5 2495.9 2492.5 2521.6 ## 2005 2538.1 2549.7 2587.2 2585.0 ## 2006 2602.6 2615.3 2654.0 2680.8 ## 2007 2665.4 2645.1 2647.5 2719.2 ## 2008 2677.0 2650.9 2667.8 2660.2 ## 2009 2554.7 2522.7 2510.0 2541.7 ## 2010 2499.1 2527.9 2519.0 2536.3 ## 2011 2493.2 2542.1 2501.6 2516.3 ## 2012 2510.5 2548.4 2548.6 2530.7 ## 2013 2497.1 2520.4 2516.9 2505.5 ## 2014 2513.9 2549.9 2555.3 # End - with default regularized index tk_tbl(q10_quarterly_ts) ## # A tibble: 59 × 2 ##    index     value ##    <yearqtr> <dbl> ##  1 2000 Q1   2329  ##  2 2000 Q2   2350. ##  3 2000 Q3   2333. ##  4 2000 Q4   2382. ##  5 2001 Q1   2383. ##  6 2001 Q2   2405  ##  7 2001 Q3   2411  ##  8 2001 Q4   2428. ##  9 2002 Q1   2392. ## 10 2002 Q2   2418. ## # ℹ 49 more rows # End - with timetk index  tk_tbl(q10_quarterly_ts, timetk_idx = TRUE) ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent ## # A tibble: 59 × 2 ##    index      value ##    <date>     <dbl> ##  1 2000-01-01 2329  ##  2 2000-04-01 2350. ##  3 2000-07-01 2333. ##  4 2000-10-01 2382. ##  5 2001-01-01 2383. ##  6 2001-04-01 2405  ##  7 2001-07-01 2411  ##  8 2001-10-01 2428. ##  9 2002-01-01 2392. ## 10 2002-04-01 2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"testing-if-an-object-has-a-timetk-index","dir":"Articles > Temp_archive","previous_headings":"","what":"Testing if an object has a timetk index","title":"Time Series Class Conversion","text":"function has_timetk_idx() can used test whether toggling timetk_idx argument tk_index() tk_tbl() functions effect output. several examples using ten year treasury data used case study:","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"tk_ts","dir":"Articles > Temp_archive","previous_headings":"Testing if an object has a timetk index","what":"tk_ts()","title":"Time Series Class Conversion","text":"tk_ts() function returns object “timetk index” attribute. toggle timetk_idx = TRUE retrieving index tk_index(), get index dates rather regularized time series. toggle timetk_idx = TRUE conversion tbl using tk_tbl(), get index dates rather regularized index returned tbl.","code":"# Data coerced with tk_ts() has timetk index has_timetk_idx(q10_quarterly_ts) ## [1] TRUE tk_index(q10_quarterly_ts, timetk_idx = TRUE) ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent ##  [1] \"2000-01-01\" \"2000-04-01\" \"2000-07-01\" \"2000-10-01\" \"2001-01-01\" ##  [6] \"2001-04-01\" \"2001-07-01\" \"2001-10-01\" \"2002-01-01\" \"2002-04-01\" ## [11] \"2002-07-01\" \"2002-10-01\" \"2003-01-01\" \"2003-04-01\" \"2003-07-01\" ## [16] \"2003-10-01\" \"2004-01-01\" \"2004-04-01\" \"2004-07-01\" \"2004-10-01\" ## [21] \"2005-01-01\" \"2005-04-01\" \"2005-07-01\" \"2005-10-01\" \"2006-01-01\" ## [26] \"2006-04-01\" \"2006-07-01\" \"2006-10-01\" \"2007-01-01\" \"2007-04-01\" ## [31] \"2007-07-01\" \"2007-10-01\" \"2008-01-01\" \"2008-04-01\" \"2008-07-01\" ## [36] \"2008-10-01\" \"2009-01-01\" \"2009-04-01\" \"2009-07-01\" \"2009-10-01\" ## [41] \"2010-01-01\" \"2010-04-01\" \"2010-07-01\" \"2010-10-01\" \"2011-01-01\" ## [46] \"2011-04-01\" \"2011-07-01\" \"2011-10-01\" \"2012-01-01\" \"2012-04-01\" ## [51] \"2012-07-01\" \"2012-10-01\" \"2013-01-01\" \"2013-04-01\" \"2013-07-01\" ## [56] \"2013-10-01\" \"2014-01-01\" \"2014-04-01\" \"2014-07-01\" tk_tbl(q10_quarterly_ts, timetk_idx = TRUE) ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent ## # A tibble: 59 × 2 ##    index      value ##    <date>     <dbl> ##  1 2000-01-01 2329  ##  2 2000-04-01 2350. ##  3 2000-07-01 2333. ##  4 2000-10-01 2382. ##  5 2001-01-01 2383. ##  6 2001-04-01 2405  ##  7 2001-07-01 2411  ##  8 2001-10-01 2428. ##  9 2002-01-01 2392. ## 10 2002-04-01 2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"testing-other-data-types","dir":"Articles > Temp_archive","previous_headings":"Testing if an object has a timetk index","what":"Testing other data types","title":"Time Series Class Conversion","text":"timetk_idx argument effect objects use regularized time series. Therefore, has_timetk_idx() returns FALSE object types (e.g. tbl, xts, zoo) since toggling argument effect classes. Toggling timetk_idx argument effect output. Output timetk_idx = TRUE timetk_idx = FALSE.","code":"has_timetk_idx(q10_quarterly_xts) ## [1] FALSE tk_index(q10_quarterly_xts, timetk_idx = TRUE) ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent ##  [1] \"2000-01-01\" \"2000-04-01\" \"2000-07-01\" \"2000-10-01\" \"2001-01-01\" ##  [6] \"2001-04-01\" \"2001-07-01\" \"2001-10-01\" \"2002-01-01\" \"2002-04-01\" ## [11] \"2002-07-01\" \"2002-10-01\" \"2003-01-01\" \"2003-04-01\" \"2003-07-01\" ## [16] \"2003-10-01\" \"2004-01-01\" \"2004-04-01\" \"2004-07-01\" \"2004-10-01\" ## [21] \"2005-01-01\" \"2005-04-01\" \"2005-07-01\" \"2005-10-01\" \"2006-01-01\" ## [26] \"2006-04-01\" \"2006-07-01\" \"2006-10-01\" \"2007-01-01\" \"2007-04-01\" ## [31] \"2007-07-01\" \"2007-10-01\" \"2008-01-01\" \"2008-04-01\" \"2008-07-01\" ## [36] \"2008-10-01\" \"2009-01-01\" \"2009-04-01\" \"2009-07-01\" \"2009-10-01\" ## [41] \"2010-01-01\" \"2010-04-01\" \"2010-07-01\" \"2010-10-01\" \"2011-01-01\" ## [46] \"2011-04-01\" \"2011-07-01\" \"2011-10-01\" \"2012-01-01\" \"2012-04-01\" ## [51] \"2012-07-01\" \"2012-10-01\" \"2013-01-01\" \"2013-04-01\" \"2013-07-01\" ## [56] \"2013-10-01\" \"2014-01-01\" \"2014-04-01\" \"2014-07-01\""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"working-with-zooyearmon-and-zooyearqtr-index","dir":"Articles > Temp_archive","previous_headings":"","what":"Working with zoo::yearmon and zoo::yearqtr index","title":"Time Series Class Conversion","text":"zoo package yearmon yearqtr classes working regularized monthly quarterly data, respectively. “timetk index” tracks format conversion. ’s example yearqtr. can coerce xts yearqtr class intact. can coerce ts , although “timetk index” hidden, yearqtr class intact. Coercing ts tbl using timetk_idx = TRUE shows original index maintained conversion steps.","code":"yearqtr_tbl <- q10_quarterly %>%     mutate(date = zoo::as.yearqtr(date)) yearqtr_tbl ## # A tibble: 59 × 3 ##    id    date      value ##    <fct> <yearqtr> <dbl> ##  1 Q10   2000 Q1   2329  ##  2 Q10   2000 Q2   2350. ##  3 Q10   2000 Q3   2333. ##  4 Q10   2000 Q4   2382. ##  5 Q10   2001 Q1   2383. ##  6 Q10   2001 Q2   2405  ##  7 Q10   2001 Q3   2411  ##  8 Q10   2001 Q4   2428. ##  9 Q10   2002 Q1   2392. ## 10 Q10   2002 Q2   2418. ## # ℹ 49 more rows yearqtr_xts <- tk_xts(yearqtr_tbl) ## Warning: Non-numeric columns being dropped: id, date ## Using column `date` for date_var. yearqtr_xts %>% head() ##          value ## 2000 Q1 2329.0 ## 2000 Q2 2349.9 ## 2000 Q3 2332.9 ## 2000 Q4 2381.5 ## 2001 Q1 2382.6 ## 2001 Q2 2405.0 yearqtr_ts <- tk_ts(yearqtr_xts, start = 1997, freq = 4) yearqtr_ts %>% head() ##        Qtr1   Qtr2   Qtr3   Qtr4 ## 1997 2329.0 2349.9 2332.9 2381.5 ## 1998 2382.6 2405.0 yearqtr_ts %>% tk_tbl(timetk_idx = TRUE) ## Warning in .check_tzones(e1, e2): 'tzone' attributes are inconsistent ## # A tibble: 59 × 2 ##    index     value ##    <yearqtr> <dbl> ##  1 2000 Q1   2329  ##  2 2000 Q2   2350. ##  3 2000 Q3   2333. ##  4 2000 Q4   2382. ##  5 2001 Q1   2383. ##  6 2001 Q2   2405  ##  7 2001 Q3   2411  ##  8 2001 Q4   2428. ##  9 2002 Q1   2392. ## 10 2002 Q2   2418. ## # ℹ 49 more rows"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK00_Time_Series_Coercion.html","id":"learning-more","dir":"Articles > Temp_archive","previous_headings":"","what":"Learning More","title":"Time Series Class Conversion","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK01_Working_With_Time_Series_Index.html","id":"introduction","dir":"Articles > Temp_archive","previous_headings":"","what":"Introduction","title":"Calendar Features","text":"time series index consists collection time-based values define observation occurred, important part time series object. index gives user lot information simple timestamp. Consider datetime “2016-01-01 00:00:00”. timestamp, can decompose date time information get signature, consists year, quarter, month, day, day year, day month, hour, minute, second occurrence single observation. , difference two observations frequency can obtain even information periodicity data whether observations regular interval. information critical provides basis performance time finance, decay rates biology, growth rates economics, . vignette user exposed : Time Series Index Time Series Signature Time Series Summary","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK01_Working_With_Time_Series_Index.html","id":"prerequisites","dir":"Articles > Temp_archive","previous_headings":"","what":"Prerequisites","title":"Calendar Features","text":"get started, load following packages.","code":"library(dplyr) library(timetk)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK01_Working_With_Time_Series_Index.html","id":"data","dir":"Articles > Temp_archive","previous_headings":"","what":"Data","title":"Calendar Features","text":"’ll use Facebook stock prices FANG data set. historical stock prices (open, high, low, close, volume, adjusted) “FB” stock 2013 2016. simplify tutorial, select “date” “volume” columns. FB_vol_date data frame, can see “date” column observations daily beginning second day 2013.","code":"data(\"FANG\")  FB_tbl <- FANG %>% filter(symbol == \"FB\") FB_tbl ## # A tibble: 1,008 × 8 ##    symbol date        open  high   low close    volume adjusted ##    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl> ##  1 FB     2013-01-02  27.4  28.2  27.4  28    69846400     28   ##  2 FB     2013-01-03  27.9  28.5  27.6  27.8  63140600     27.8 ##  3 FB     2013-01-04  28.0  28.9  27.8  28.8  72715400     28.8 ##  4 FB     2013-01-07  28.7  29.8  28.6  29.4  83781800     29.4 ##  5 FB     2013-01-08  29.5  29.6  28.9  29.1  45871300     29.1 ##  6 FB     2013-01-09  29.7  30.6  29.5  30.6 104787700     30.6 ##  7 FB     2013-01-10  30.6  31.5  30.3  31.3  95316400     31.3 ##  8 FB     2013-01-11  31.3  32.0  31.1  31.7  89598000     31.7 ##  9 FB     2013-01-14  32.1  32.2  30.6  31.0  98892800     31.0 ## 10 FB     2013-01-15  30.6  31.7  29.9  30.1 173242600     30.1 ## # ℹ 998 more rows FB_vol_date <- FB_tbl %>% select(date, volume) FB_vol_date ## # A tibble: 1,008 × 2 ##    date          volume ##    <date>         <dbl> ##  1 2013-01-02  69846400 ##  2 2013-01-03  63140600 ##  3 2013-01-04  72715400 ##  4 2013-01-07  83781800 ##  5 2013-01-08  45871300 ##  6 2013-01-09 104787700 ##  7 2013-01-10  95316400 ##  8 2013-01-11  89598000 ##  9 2013-01-14  98892800 ## 10 2013-01-15 173242600 ## # ℹ 998 more rows"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK01_Working_With_Time_Series_Index.html","id":"time-series-index","dir":"Articles > Temp_archive","previous_headings":"","what":"Time Series Index","title":"Calendar Features","text":"can analyze index, need extract object. function tk_index() extracts index time series object including data frame (tbl), xts, zoo, etc. index always returned native date, datetime, yearmon, yearqtr format. Note index must one time-based classes extraction work: datetimes: Must inherit POSIXt dates: Must inherit Date yearmon: Must inherit yearmon zoo package yearqtr: Must inherit yearqtr zoo package Extract index using tk_index(). structure shown see output format, vector dates.","code":"# idx_date idx_date <- tk_index(FB_vol_date) str(idx_date) ##  Date[1:1008], format: \"2013-01-02\" \"2013-01-03\" \"2013-01-04\" \"2013-01-07\" \"2013-01-08\" ..."},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK01_Working_With_Time_Series_Index.html","id":"time-series-signature","dir":"Articles > Temp_archive","previous_headings":"","what":"Time Series Signature","title":"Calendar Features","text":"index can decomposed signature. time series signature unique set properties time series values describe time series.","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK01_Working_With_Time_Series_Index.html","id":"get-functions---turning-an-index-into-information","dir":"Articles > Temp_archive","previous_headings":"Time Series Signature","what":"Get Functions - Turning an Index into Information","title":"Calendar Features","text":"function tk_get_timeseries_signature() can used convert index tibble containing following values (columns): index: index value decomposed index.num: numeric value index seconds. base “1970-01-01 00:00:00” (Execute \"1970-01-01 00:00:00\" %>% ymd_hms() %>% .numeric() see value returned zero). Every time series value date can converted numeric value seconds. diff: difference seconds previous numeric index value. year: year component index. year.iso: ISO year number year (Monday start). half: half component index. quarter: quarter component index. month: month component index base 1. month.xts: month component index base 0, xts implements. month.lbl: month label ordered factor begining January ending December. day: day component index. hour: hour component index. minute: minute component index. second: second component index. hour12: hour component 12 hour scale. .pm: Morning () = 1, Afternoon (PM) = 2. wday: day week base 1. Sunday = 1 Saturday = 7. wday.xts: day week base 0, xts implements. Sunday = 0 Saturday = 6. wday.lbl: day week label ordered factor begining Sunday ending Saturday. mday: day month. qday: day quarter. yday: day year. mweek: week month. week: week number year (Sunday start). week.iso: ISO week number year (Monday start). week2: modulus bi-weekly frequency. week3: modulus tri-weekly frequency. week4: modulus quad-weekly frequency. mday7: integer division day month seven, returns first, second, third, … instance day appeared month. Values begin 1. example, first Saturday month mday7 = 1. second mday7 = 2.","code":"# idx_date signature tk_get_timeseries_signature(idx_date) ## # A tibble: 1,008 × 29 ##    index       index.num   diff  year year.iso  half quarter month month.xts ##    <date>          <dbl>  <dbl> <int>    <int> <int>   <int> <int>     <int> ##  1 2013-01-02 1357084800     NA  2013     2013     1       1     1         0 ##  2 2013-01-03 1357171200  86400  2013     2013     1       1     1         0 ##  3 2013-01-04 1357257600  86400  2013     2013     1       1     1         0 ##  4 2013-01-07 1357516800 259200  2013     2013     1       1     1         0 ##  5 2013-01-08 1357603200  86400  2013     2013     1       1     1         0 ##  6 2013-01-09 1357689600  86400  2013     2013     1       1     1         0 ##  7 2013-01-10 1357776000  86400  2013     2013     1       1     1         0 ##  8 2013-01-11 1357862400  86400  2013     2013     1       1     1         0 ##  9 2013-01-14 1358121600 259200  2013     2013     1       1     1         0 ## 10 2013-01-15 1358208000  86400  2013     2013     1       1     1         0 ## # ℹ 998 more rows ## # ℹ 20 more variables: month.lbl <ord>, day <int>, hour <int>, minute <int>, ## #   second <int>, hour12 <int>, am.pm <int>, wday <int>, wday.xts <int>, ## #   wday.lbl <ord>, mday <int>, qday <int>, yday <int>, mweek <int>, ## #   week <int>, week.iso <int>, week2 <int>, week3 <int>, week4 <int>, ## #   mday7 <int>"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK01_Working_With_Time_Series_Index.html","id":"augment-functions-adding-many-features-to-a-data-frame","dir":"Articles > Temp_archive","previous_headings":"Time Series Signature","what":"Augment Functions (Adding Many Features to a Data Frame)","title":"Calendar Features","text":"’s usually important keep index signature values (e.g. volume example). can use expedited approach tk_augment_timeseries_signature(), adds signature end time series object. Modeling now much easier. example, can use linear regression model using lm() function month year predictor volume.","code":"# Augmenting a data frame FB_vol_date_signature <- FB_vol_date %>% tk_augment_timeseries_signature(.date_var = date) FB_vol_date_signature ## # A tibble: 1,008 × 30 ##    date          volume  index.num   diff  year year.iso  half quarter month ##    <date>         <dbl>      <dbl>  <dbl> <int>    <int> <int>   <int> <int> ##  1 2013-01-02  69846400 1357084800     NA  2013     2013     1       1     1 ##  2 2013-01-03  63140600 1357171200  86400  2013     2013     1       1     1 ##  3 2013-01-04  72715400 1357257600  86400  2013     2013     1       1     1 ##  4 2013-01-07  83781800 1357516800 259200  2013     2013     1       1     1 ##  5 2013-01-08  45871300 1357603200  86400  2013     2013     1       1     1 ##  6 2013-01-09 104787700 1357689600  86400  2013     2013     1       1     1 ##  7 2013-01-10  95316400 1357776000  86400  2013     2013     1       1     1 ##  8 2013-01-11  89598000 1357862400  86400  2013     2013     1       1     1 ##  9 2013-01-14  98892800 1358121600 259200  2013     2013     1       1     1 ## 10 2013-01-15 173242600 1358208000  86400  2013     2013     1       1     1 ## # ℹ 998 more rows ## # ℹ 21 more variables: month.xts <int>, month.lbl <ord>, day <int>, hour <int>, ## #   minute <int>, second <int>, hour12 <int>, am.pm <int>, wday <int>, ## #   wday.xts <int>, wday.lbl <ord>, mday <int>, qday <int>, yday <int>, ## #   mweek <int>, week <int>, week.iso <int>, week2 <int>, week3 <int>, ## #   week4 <int>, mday7 <int> # Example Benefit 2: Modeling is easier fit <- lm(volume ~ year + month.lbl, data = FB_vol_date_signature) summary(fit) ##  ## Call: ## lm(formula = volume ~ year + month.lbl, data = FB_vol_date_signature) ##  ## Residuals: ##       Min        1Q    Median        3Q       Max  ## -51042223 -13528407  -4588594   8296073 304011277  ##  ## Coefficients: ##                Estimate Std. Error t value Pr(>|t|)     ## (Intercept)   2.494e+10  1.414e+09  17.633  < 2e-16 *** ## year         -1.236e+07  7.021e+05 -17.604  < 2e-16 *** ## month.lbl.L  -9.589e+06  2.740e+06  -3.499 0.000488 *** ## month.lbl.Q   7.348e+06  2.725e+06   2.697 0.007122 **  ## month.lbl.C  -9.773e+06  2.711e+06  -3.605 0.000328 *** ## month.lbl^4  -2.885e+06  2.720e+06  -1.060 0.289176     ## month.lbl^5  -2.994e+06  2.749e+06  -1.089 0.276428     ## month.lbl^6   3.169e+06  2.753e+06   1.151 0.249851     ## month.lbl^7   6.000e+05  2.721e+06   0.221 0.825514     ## month.lbl^8   8.281e+03  2.702e+06   0.003 0.997555     ## month.lbl^9   9.504e+06  2.704e+06   3.515 0.000459 *** ## month.lbl^10 -5.911e+06  2.701e+06  -2.188 0.028888 *   ## month.lbl^11 -4.738e+06  2.696e+06  -1.757 0.079181 .   ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ##  ## Residual standard error: 24910000 on 995 degrees of freedom ## Multiple R-squared:  0.2714, Adjusted R-squared:  0.2626  ## F-statistic: 30.89 on 12 and 995 DF,  p-value: < 2.2e-16"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK01_Working_With_Time_Series_Index.html","id":"time-series-summary","dir":"Articles > Temp_archive","previous_headings":"","what":"Time Series Summary","title":"Calendar Features","text":"next index analysis tool summary metrics, can retrieved using tk_get_timeseries_summary() function. summary reports following attributes single-row tibble. General Summary: first six columns general summary information. n.obs: total number observations start: start appropriate time class end: end appropriate time class units: label describes unit index value independent frequency (.e. date class always “days” whereas datetime class always “seconds”). Values can days, hours, mins, secs. scale: label describes median difference (frequency) observations. Values can quarter, month, day, hour, minute, second. tzone: timezone index. Differences Summary: next group values differences summary (.e. summary frequency). values seconds: diff.minimum: minimum difference index values. diff.q1: first quartile index differences. diff.median: median difference index values (.e. common frequency). diff.mean: average difference index values. diff.q3: third quartile index differences. diff.maximum: maximum difference index values. differences provide information regularity frequency. Generally speaking difference values equal, index regular. However, scales beyond “day” never theoretically regular since differences seconds equivalent. However, conceptually monthly, quarterly yearly data can thought regular index contains consecutive months, quarters, years, respectively. Therefore, difference attributes meaningful daily lower time scales difference summary always indicates level regularity. second group (differences summary), immediately recognize mean different median therefore index irregular (meaning certain days missing). can see maximum difference 345,600 seconds, indicating maximum difference 4 days (345,600 seconds / 86400 seconds/day).","code":"# idx_date: First six columns, general summary tk_get_timeseries_summary(idx_date)[,1:6] ## # A tibble: 1 × 6 ##   n.obs start      end        units scale tzone ##   <int> <date>     <date>     <chr> <chr> <chr> ## 1  1008 2013-01-02 2016-12-30 days  day   UTC # idx_date: Last six columns, difference summary tk_get_timeseries_summary(idx_date)[,7:12] ## # A tibble: 1 × 6 ##   diff.minimum diff.q1 diff.median diff.mean diff.q3 diff.maximum ##          <dbl>   <dbl>       <dbl>     <dbl>   <dbl>        <dbl> ## 1        86400   86400       86400   125096.   86400       345600"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK01_Working_With_Time_Series_Index.html","id":"learning-more","dir":"Articles > Temp_archive","previous_headings":"","what":"Learning More","title":"Calendar Features","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK02_Time_Series_Date_Sequences.html","id":"prerequisites","dir":"Articles > Temp_archive","previous_headings":"","what":"Prerequisites","title":"Intelligent Date & Time Sequences","text":"get started, load following packages.","code":"library(dplyr) library(timetk)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK02_Time_Series_Date_Sequences.html","id":"making-a-time-series-sequence","dir":"Articles > Temp_archive","previous_headings":"","what":"Making a Time Series Sequence","title":"Intelligent Date & Time Sequences","text":"tk_make_timeseries() improves seq.Date() seq.POSIXt() functions simplifying 1 function. Intelligently handles character dates logical assumptions based user inputs. Day Can use = \"day\" leave blank. include_endpoints = FALSE removes last value series 7 observations. 2 Seconds Can use = \"2 sec\" adjust interval width. include_endpoints = TRUE keeps last value series ends 6th second. Length = 1 year 6 months length_out = \"1 year 6 months\" - Can include complex expressions like “1 year 4 months 6 days”. Go Reverse go reverse, just use end_date want series end.","code":"# Selects by day automatically tk_make_timeseries(\"2011\", length_out = \"7 days\", include_endpoints = FALSE) ## [1] \"2011-01-01\" \"2011-01-02\" \"2011-01-03\" \"2011-01-04\" \"2011-01-05\" ## [6] \"2011-01-06\" \"2011-01-07\" # Guesses by second tk_make_timeseries(\"2016\", by = \"2 sec\", length_out = \"6 seconds\") ## [1] \"2016-01-01 00:00:00 UTC\" \"2016-01-01 00:00:02 UTC\" ## [3] \"2016-01-01 00:00:04 UTC\" \"2016-01-01 00:00:06 UTC\" tk_make_timeseries(\"2012-07\",                     by = \"1 month\",                    length_out = \"1 year 6 months\",                     include_endpoints = FALSE) ##  [1] \"2012-07-01\" \"2012-08-01\" \"2012-09-01\" \"2012-10-01\" \"2012-11-01\" ##  [6] \"2012-12-01\" \"2013-01-01\" \"2013-02-01\" \"2013-03-01\" \"2013-04-01\" ## [11] \"2013-05-01\" \"2013-06-01\" \"2013-07-01\" \"2013-08-01\" \"2013-09-01\" ## [16] \"2013-10-01\" \"2013-11-01\" \"2013-12-01\" tk_make_timeseries(end_date = \"2012-07-01\",                     by = \"1 month\",                    length_out = \"1 year 6 months\") ##  [1] \"2011-01-01\" \"2011-02-01\" \"2011-03-01\" \"2011-04-01\" \"2011-05-01\" ##  [6] \"2011-06-01\" \"2011-07-01\" \"2011-08-01\" \"2011-09-01\" \"2011-10-01\" ## [11] \"2011-11-01\" \"2011-12-01\" \"2012-01-01\" \"2012-02-01\" \"2012-03-01\" ## [16] \"2012-04-01\" \"2012-05-01\" \"2012-06-01\" \"2012-07-01\""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK02_Time_Series_Date_Sequences.html","id":"future-time-series-sequence","dir":"Articles > Temp_archive","previous_headings":"","what":"Future Time Series Sequence","title":"Intelligent Date & Time Sequences","text":"common operation make future time series sequence mimics existing. tk_make_future_timeseries() . Suppose existing time index. Make Future Time Series Existing can create future time sequence existing sequence using tk_make_future_timeseries().","code":"idx <- tk_make_timeseries(\"2012\", by = \"3 months\",                            length_out = \"2 years\",                            include_endpoints = FALSE) idx ## [1] \"2012-01-01\" \"2012-04-01\" \"2012-07-01\" \"2012-10-01\" \"2013-01-01\" ## [6] \"2013-04-01\" \"2013-07-01\" \"2013-10-01\" tk_make_future_timeseries(idx, length_out = \"2 years\") ## [1] \"2014-01-01\" \"2014-04-01\" \"2014-07-01\" \"2014-10-01\" \"2015-01-01\" ## [6] \"2015-04-01\" \"2015-07-01\" \"2015-10-01\""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK02_Time_Series_Date_Sequences.html","id":"weekends-holidays","dir":"Articles > Temp_archive","previous_headings":"","what":"Weekends & Holidays","title":"Intelligent Date & Time Sequences","text":"Make weekday sequence removing holidays Result 252 days. holidays removed? NYSE Trading holidays days businesses observe Make future index removing holidays","code":"idx <- tk_make_weekday_sequence(\"2012\",                                 remove_weekends = TRUE,                                  remove_holidays = TRUE, calendar = \"NYSE\")  tk_get_timeseries_summary(idx) ## # A tibble: 1 × 12 ##   n.obs start      end        units scale tzone diff.minimum diff.q1 diff.median ##   <int> <date>     <date>     <chr> <chr> <chr>        <dbl>   <dbl>       <dbl> ## 1   250 2012-01-03 2012-12-31 days  day   UTC          86400   86400       86400 ## # ℹ 3 more variables: diff.mean <dbl>, diff.q3 <dbl>, diff.maximum <dbl> tk_make_holiday_sequence(\"2012\", calendar = \"NYSE\") ##  [1] \"2012-01-02\" \"2012-01-16\" \"2012-02-20\" \"2012-04-06\" \"2012-05-28\" ##  [6] \"2012-07-04\" \"2012-09-03\" \"2012-10-29\" \"2012-10-30\" \"2012-11-22\" ## [11] \"2012-12-25\" holidays <- tk_make_holiday_sequence(     start_date = \"2013-01-01\",     end_date   = \"2013-12-31\",     calendar   = \"NYSE\")  idx_future <- idx %>%    tk_make_future_timeseries(length_out       = \"1 year\",                              inspect_weekdays = TRUE,                              skip_values      = holidays)  tk_get_timeseries_summary(idx_future) ## # A tibble: 1 × 12 ##   n.obs start      end        units scale tzone diff.minimum diff.q1 diff.median ##   <int> <date>     <date>     <chr> <chr> <chr>        <dbl>   <dbl>       <dbl> ## 1   252 2013-01-02 2013-12-31 days  day   UTC          86400   86400       86400 ## # ℹ 3 more variables: diff.mean <dbl>, diff.q3 <dbl>, diff.maximum <dbl>"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK02_Time_Series_Date_Sequences.html","id":"learning-more","dir":"Articles > Temp_archive","previous_headings":"","what":"Learning More","title":"Intelligent Date & Time Sequences","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"introduction","dir":"Articles > Temp_archive","previous_headings":"","what":"Introduction","title":"Time Series Machine Learning","text":"time series signature collection useful features describe time series index time-based data set. contains wealth features can used forecast time series contain patterns. vignette, user learn methods implement machine learning predict future outcomes time-based data set. vignette example uses well known time series dataset, Bike Sharing Dataset, UCI Machine Learning Repository. vignette follows example ’ll use timetk build basic Machine Learning model predict future values using time series signature. objective build model predict next six months Bike Sharing daily counts.","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"prerequisites","dir":"Articles > Temp_archive","previous_headings":"","what":"Prerequisites","title":"Time Series Machine Learning","text":"get started, load following packages.","code":"library(tidymodels) library(modeltime) library(dplyr) library(timetk)  # Used to convert plots from interactive to static interactive = TRUE"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"data","dir":"Articles > Temp_archive","previous_headings":"","what":"Data","title":"Time Series Machine Learning","text":"’ll using Bike Sharing Dataset UCI Machine Learning Repository. Source: Fanaee-T, Hadi, Gama, Joao, ‘Event labeling combining ensemble detectors background knowledge’, Progress Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg Next, visualize dataset plot_time_series() function. Toggle .interactive = TRUE get plotly interactive plot. FALSE returns ggplot2 static plot.","code":"# Read data bike_transactions_tbl <- bike_sharing_daily %>%   select(dteday, cnt) %>%   set_names(c(\"date\", \"value\"))   bike_transactions_tbl ## # A tibble: 731 × 2 ##    date       value ##    <date>     <dbl> ##  1 2011-01-01   985 ##  2 2011-01-02   801 ##  3 2011-01-03  1349 ##  4 2011-01-04  1562 ##  5 2011-01-05  1600 ##  6 2011-01-06  1606 ##  7 2011-01-07  1510 ##  8 2011-01-08   959 ##  9 2011-01-09   822 ## 10 2011-01-10  1321 ## # ℹ 721 more rows bike_transactions_tbl %>%   plot_time_series(date, value, .interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"train-test","dir":"Articles > Temp_archive","previous_headings":"","what":"Train / Test","title":"Time Series Machine Learning","text":"Next, use time_series_split() make train/test set. Setting assess = \"3 months\" tells function use last 3-months data testing set. Setting cumulative = TRUE tells sampling use prior data training set. Next, visualize train/test split. tk_time_series_cv_plan(): Converts splits object data frame plot_time_series_cv_plan(): Plots time series sampling data using “date” “value” columns.","code":"splits <- bike_transactions_tbl %>%   time_series_split(assess = \"3 months\", cumulative = TRUE) splits %>%   tk_time_series_cv_plan() %>%   plot_time_series_cv_plan(date, value, .interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"modeling","dir":"Articles > Temp_archive","previous_headings":"","what":"Modeling","title":"Time Series Machine Learning","text":"Machine learning models complex univariate models (e.g. ARIMA, Exponential Smoothing). complexity typically requires workflow (sometimes called pipeline languages). general process goes like : Create Preprocessing Recipe Create Model Specifications Use Workflow combine Model Spec Preprocessing, Fit Model","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"recipe-preprocessing-specification","dir":"Articles > Temp_archive","previous_headings":"Modeling","what":"Recipe Preprocessing Specification","title":"Time Series Machine Learning","text":"first step add time series signature training set, used learn patterns. New timetk 0.1.3 integration recipes R package: recipes package allows us add preprocessing steps applied sequentially part data transformation pipeline. timetk step_timeseries_signature(), used add number features can help machine learning models. can see happens apply prepared recipe prep() using bake() function. Many new columns added timestamp “date” feature. features can use machine learning models. Next, apply various preprocessing steps improve modeling behavior. wish learn , Advanced Time Series course help learn techniques.","code":"# Add time series signature recipe_spec_timeseries <- recipe(value ~ ., data = training(splits)) %>%     step_timeseries_signature(date) bake(prep(recipe_spec_timeseries), new_data = training(splits)) ## # A tibble: 641 × 29 ##    date       value date_index.num date_year date_year.iso date_half ##    <date>     <dbl>          <dbl>     <int>         <int>     <int> ##  1 2011-01-01   985     1293840000      2011          2010         1 ##  2 2011-01-02   801     1293926400      2011          2010         1 ##  3 2011-01-03  1349     1294012800      2011          2011         1 ##  4 2011-01-04  1562     1294099200      2011          2011         1 ##  5 2011-01-05  1600     1294185600      2011          2011         1 ##  6 2011-01-06  1606     1294272000      2011          2011         1 ##  7 2011-01-07  1510     1294358400      2011          2011         1 ##  8 2011-01-08   959     1294444800      2011          2011         1 ##  9 2011-01-09   822     1294531200      2011          2011         1 ## 10 2011-01-10  1321     1294617600      2011          2011         1 ## # ℹ 631 more rows ## # ℹ 23 more variables: date_quarter <int>, date_month <int>, ## #   date_month.xts <int>, date_month.lbl <ord>, date_day <int>, ## #   date_hour <int>, date_minute <int>, date_second <int>, date_hour12 <int>, ## #   date_am.pm <int>, date_wday <int>, date_wday.xts <int>, ## #   date_wday.lbl <ord>, date_mday <int>, date_qday <int>, date_yday <int>, ## #   date_mweek <int>, date_week <int>, date_week.iso <int>, date_week2 <int>, … recipe_spec_final <- recipe_spec_timeseries %>%     step_fourier(date, period = 365, K = 5) %>%     step_rm(date) %>%     step_rm(contains(\"iso\"), contains(\"minute\"), contains(\"hour\"),             contains(\"am.pm\"), contains(\"xts\")) %>%     step_normalize(contains(\"index.num\"), date_year) %>%     step_dummy(contains(\"lbl\"), one_hot = TRUE)   juice(prep(recipe_spec_final)) ## # A tibble: 641 × 47 ##    value date_index.num date_year date_half date_quarter date_month date_day ##    <dbl>          <dbl>     <dbl>     <int>        <int>      <int>    <int> ##  1   985          -1.73    -0.869         1            1          1        1 ##  2   801          -1.72    -0.869         1            1          1        2 ##  3  1349          -1.72    -0.869         1            1          1        3 ##  4  1562          -1.71    -0.869         1            1          1        4 ##  5  1600          -1.71    -0.869         1            1          1        5 ##  6  1606          -1.70    -0.869         1            1          1        6 ##  7  1510          -1.70    -0.869         1            1          1        7 ##  8   959          -1.69    -0.869         1            1          1        8 ##  9   822          -1.68    -0.869         1            1          1        9 ## 10  1321          -1.68    -0.869         1            1          1       10 ## # ℹ 631 more rows ## # ℹ 40 more variables: date_second <int>, date_wday <int>, date_mday <int>, ## #   date_qday <int>, date_yday <int>, date_mweek <int>, date_week <int>, ## #   date_week2 <int>, date_week3 <int>, date_week4 <int>, date_mday7 <int>, ## #   date_sin365_K1 <dbl>, date_cos365_K1 <dbl>, date_sin365_K2 <dbl>, ## #   date_cos365_K2 <dbl>, date_sin365_K3 <dbl>, date_cos365_K3 <dbl>, ## #   date_sin365_K4 <dbl>, date_cos365_K4 <dbl>, date_sin365_K5 <dbl>, …"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"model-specification","dir":"Articles > Temp_archive","previous_headings":"Modeling","what":"Model Specification","title":"Time Series Machine Learning","text":"Next, let’s create model specification. ’ll use lm.","code":"model_spec_lm <- linear_reg(mode = \"regression\") %>%     set_engine(\"lm\")"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"workflow","dir":"Articles > Temp_archive","previous_headings":"Modeling","what":"Workflow","title":"Time Series Machine Learning","text":"can mary preprocessing recipe model using workflow().","code":"workflow_lm <- workflow() %>%     add_recipe(recipe_spec_final) %>%     add_model(model_spec_lm)  workflow_lm ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: linear_reg() ##  ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 6 Recipe Steps ##  ## • step_timeseries_signature() ## • step_fourier() ## • step_rm() ## • step_rm() ## • step_normalize() ## • step_dummy() ##  ## ── Model ─────────────────────────────────────────────────────────────────────── ## Linear Regression Model Specification (regression) ##  ## Computational engine: lm"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"training","dir":"Articles > Temp_archive","previous_headings":"Modeling","what":"Training","title":"Time Series Machine Learning","text":"workflow can trained fit() function.","code":"workflow_fit_lm <- workflow_lm %>% fit(data = training(splits))"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"hyperparameter-tuning","dir":"Articles > Temp_archive","previous_headings":"Modeling","what":"Hyperparameter Tuning","title":"Time Series Machine Learning","text":"Linear regression parameters. Therefore, step needed. complex models hyperparameters require tuning. Algorithms include: Elastic Net XGBoost Random Forest Support Vector Machine (SVM) K-Nearest Neighbors Multivariate Adaptive Regression Spines (MARS) like learn tune models time series, join waitlist advanced Time Series Analysis & Forecasting Course.","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"forecasting-with-modeltime","dir":"Articles > Temp_archive","previous_headings":"","what":"Forecasting with Modeltime","title":"Time Series Machine Learning","text":"Modeltime Workflow designed speed model evaluation selection. Now several time series models, let’s analyze forecast future modeltime package.","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"modeltime-table","dir":"Articles > Temp_archive","previous_headings":"Forecasting with Modeltime","what":"Modeltime Table","title":"Time Series Machine Learning","text":"Modeltime Table organizes models IDs creates generic descriptions help us keep track models. Let’s add models modeltime_table().","code":"model_table <- modeltime_table(   workflow_fit_lm )   model_table ## # Modeltime Table ## # A tibble: 1 × 3 ##   .model_id .model     .model_desc ##       <int> <list>     <chr>       ## 1         1 <workflow> LM"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"calibration","dir":"Articles > Temp_archive","previous_headings":"Forecasting with Modeltime","what":"Calibration","title":"Time Series Machine Learning","text":"Model Calibration used quantify error estimate confidence intervals. ’ll perform model calibration --sample data (aka. Testing Set) modeltime_calibrate() function. Two new columns generated (“.type” “.calibration_data”), important “.calibration_data”. includes actual values, fitted values, residuals testing set.","code":"calibration_table <- model_table %>%   modeltime_calibrate(testing(splits))  calibration_table ## # Modeltime Table ## # A tibble: 1 × 5 ##   .model_id .model     .model_desc .type .calibration_data ##       <int> <list>     <chr>       <chr> <list>            ## 1         1 <workflow> LM          Test  <tibble [90 × 4]>"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"forecast-testing-set","dir":"Articles > Temp_archive","previous_headings":"Forecasting with Modeltime > Calibration","what":"Forecast (Testing Set)","title":"Time Series Machine Learning","text":"calibrated data, can visualize testing predictions (forecast). Use modeltime_forecast() generate forecast data testing set tibble. Use plot_modeltime_forecast() visualize results interactive static plot formats.","code":"calibration_table %>%   modeltime_forecast(actual_data = bike_transactions_tbl) %>%   plot_modeltime_forecast(.interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"accuracy-testing-set","dir":"Articles > Temp_archive","previous_headings":"Forecasting with Modeltime > Calibration","what":"Accuracy (Testing Set)","title":"Time Series Machine Learning","text":"Next, calculate testing accuracy compare models. Use modeltime_accuracy() generate --sample accuracy metrics tibble. Use table_modeltime_accuracy() generate interactive static","code":"calibration_table %>%   modeltime_accuracy() %>%   table_modeltime_accuracy(.interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"refit-and-forecast-forward","dir":"Articles > Temp_archive","previous_headings":"Forecasting with Modeltime","what":"Refit and Forecast Forward","title":"Time Series Machine Learning","text":"Refitting best-practice forecasting future. modeltime_refit(): re-train full data (bike_transactions_tbl) modeltime_forecast(): models depend “date” feature, can use h (horizon) forecast forward. Setting h = \"12 months\" forecasts next 12-months data.","code":"calibration_table %>%   modeltime_refit(bike_transactions_tbl) %>%   modeltime_forecast(h = \"12 months\", actual_data = bike_transactions_tbl) %>%   plot_modeltime_forecast(.interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"summary","dir":"Articles > Temp_archive","previous_headings":"","what":"Summary","title":"Time Series Machine Learning","text":"Timetk part amazing Modeltime Ecosystem time series forecasting. can take long time learn: Many algorithms Ensembling Resampling Feature Engineering Machine Learning Deep Learning Scalable Modeling: 10,000+ time series probably thinking ever going learn time series forecasting. ’s solution save years struggling.","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"take-the-high-performance-forecasting-course","dir":"Articles > Temp_archive","previous_headings":"","what":"Take the High-Performance Forecasting Course","title":"Time Series Machine Learning","text":"Become forecasting expert organization  High-Performance Time Series Course","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"time-series-is-changing","dir":"Articles > Temp_archive","previous_headings":"Take the High-Performance Forecasting Course","what":"Time Series is Changing","title":"Time Series Machine Learning","text":"Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies improving accuracy scalability. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System).","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK03_Forecasting_Using_Time_Series_Signature.html","id":"how-to-learn-high-performance-time-series-forecasting","dir":"Articles > Temp_archive","previous_headings":"Take the High-Performance Forecasting Course","what":"How to Learn High-Performance Time Series Forecasting","title":"Time Series Machine Learning","text":"teach build HPTFS System High-Performance Time Series Forecasting Course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Become Time Series Expert organization. Take High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK04_Plotting_Time_Series.html","id":"libraries","dir":"Articles > Temp_archive","previous_headings":"","what":"Libraries","title":"Visualizing Time Series","text":"Run following code setup tutorial.","code":"library(dplyr) library(ggplot2) library(lubridate) library(timetk)  # Setup for the plotly charts (# FALSE returns ggplots) interactive <- FALSE"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK04_Plotting_Time_Series.html","id":"plotting-time-series","dir":"Articles > Temp_archive","previous_headings":"","what":"Plotting Time Series","title":"Visualizing Time Series","text":"Let’s start popular time series, taylor_30_min, includes energy demand megawatts sampling interval 30-minutes. single time series. plot_time_series() function generates interactive plotly chart default. Simply provide date variable (time-based column, .date_var) numeric variable (.value) changes time first 2 arguments .interactive = TRUE, .plotly_slider = TRUE adds date slider bottom chart.","code":"taylor_30_min #> # A tibble: 4,032 × 2 #>    date                value #>    <dttm>              <dbl> #>  1 2000-06-05 00:00:00 22262 #>  2 2000-06-05 00:30:00 21756 #>  3 2000-06-05 01:00:00 22247 #>  4 2000-06-05 01:30:00 22759 #>  5 2000-06-05 02:00:00 22549 #>  6 2000-06-05 02:30:00 22313 #>  7 2000-06-05 03:00:00 22128 #>  8 2000-06-05 03:30:00 21860 #>  9 2000-06-05 04:00:00 21751 #> 10 2000-06-05 04:30:00 21336 #> # ℹ 4,022 more rows taylor_30_min %>%    plot_time_series(date, value,                     .interactive = interactive,                    .plotly_slider = TRUE)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK04_Plotting_Time_Series.html","id":"plotting-groups","dir":"Articles > Temp_archive","previous_headings":"Plotting Time Series","what":"Plotting Groups","title":"Visualizing Time Series","text":"Next, let’s move dataset time series groups, m4_daily, sample 4 time series M4 competition sampled daily frequency. Visualizing grouped data simple grouping data set group_by() prior piping plot_time_series() function. Key points: Groups can added 2 ways: group_by() using ... add groups. Groups converted facets. .facet_ncol = 2 returns 2-column faceted plot .facet_scales = \"free\" allows x y-axis plot scale independently plots","code":"m4_daily %>% group_by(id) #> # A tibble: 9,743 × 3 #> # Groups:   id [4] #>    id    date       value #>    <fct> <date>     <dbl> #>  1 D10   2014-07-03 2076. #>  2 D10   2014-07-04 2073. #>  3 D10   2014-07-05 2049. #>  4 D10   2014-07-06 2049. #>  5 D10   2014-07-07 2006. #>  6 D10   2014-07-08 2018. #>  7 D10   2014-07-09 2019. #>  8 D10   2014-07-10 2007. #>  9 D10   2014-07-11 2010  #> 10 D10   2014-07-12 2002. #> # ℹ 9,733 more rows m4_daily %>%   group_by(id) %>%   plot_time_series(date, value,                     .facet_ncol = 2, .facet_scales = \"free\",                    .interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK04_Plotting_Time_Series.html","id":"visualizing-transformations-sub-groups","dir":"Articles > Temp_archive","previous_headings":"Plotting Time Series","what":"Visualizing Transformations & Sub-Groups","title":"Visualizing Time Series","text":"Let’s switch hourly dataset multiple groups. can showcase: Log transformation .value Use .color_var highlight sub-groups. intent showcase groups faceted plots, highlight weekly windows (sub-groups) within data simultaneously log() transformation value. simple : .value = log(value) Applies Log Transformation .color_var = week(date) date column transformed lubridate::week() number. color applied week numbers.","code":"m4_hourly %>% group_by(id) #> # A tibble: 3,060 × 3 #> # Groups:   id [4] #>    id    date                value #>    <fct> <dttm>              <dbl> #>  1 H10   2015-07-01 12:00:00   513 #>  2 H10   2015-07-01 13:00:00   512 #>  3 H10   2015-07-01 14:00:00   506 #>  4 H10   2015-07-01 15:00:00   500 #>  5 H10   2015-07-01 16:00:00   490 #>  6 H10   2015-07-01 17:00:00   484 #>  7 H10   2015-07-01 18:00:00   467 #>  8 H10   2015-07-01 19:00:00   446 #>  9 H10   2015-07-01 20:00:00   434 #> 10 H10   2015-07-01 21:00:00   422 #> # ℹ 3,050 more rows m4_hourly %>%   group_by(id) %>%   plot_time_series(date, log(value),             # Apply a Log Transformation                    .color_var = week(date),      # Color applied to Week transformation                    # Facet formatting                    .facet_ncol = 2,                     .facet_scales = \"free\",                     .interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK04_Plotting_Time_Series.html","id":"static-ggplot2-visualizations-customizations","dir":"Articles > Temp_archive","previous_headings":"Plotting Time Series","what":"Static ggplot2 Visualizations & Customizations","title":"Visualizing Time Series","text":"visualizations can converted interactive plotly (great exploring shiny apps) static ggplot2 visualizations (great reports).","code":"taylor_30_min %>%   plot_time_series(date, value,                     .color_var = month(date, label = TRUE),                                        # Returns static ggplot                    .interactive = FALSE,                                          # Customization                    .title = \"Taylor's MegaWatt Data\",                    .x_lab = \"Date (30-min intervals)\",                    .y_lab = \"Energy Demand (MW)\",                    .color_lab = \"Month\") +   scale_y_continuous(labels = scales::comma_format())"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK04_Plotting_Time_Series.html","id":"box-plots-time-series","dir":"Articles > Temp_archive","previous_headings":"","what":"Box Plots (Time Series)","title":"Visualizing Time Series","text":"plot_time_series_boxplot() function can used make box plots. Box plots use aggregation, key parameter defined .period argument.","code":"m4_monthly %>%     group_by(id) %>%     plot_time_series_boxplot(         date, value,         .period      = \"1 year\",         .facet_ncol  = 2,         .interactive = FALSE)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK04_Plotting_Time_Series.html","id":"regression-plots-time-series","dir":"Articles > Temp_archive","previous_headings":"","what":"Regression Plots (Time Series)","title":"Visualizing Time Series","text":"time series regression plot, plot_time_series_regression(), can useful quickly assess key features correlated time series. Internally function passes formula stats::lm() function. linear regression summary can output toggling show_summary = TRUE.","code":"m4_monthly %>%     group_by(id) %>%     plot_time_series_regression(         .date_var     = date,         .formula      = log(value) ~ as.numeric(date) + month(date, label = TRUE),         .facet_ncol   = 2,         .interactive  = FALSE,         .show_summary = FALSE     )"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK04_Plotting_Time_Series.html","id":"summary","dir":"Articles > Temp_archive","previous_headings":"","what":"Summary","title":"Visualizing Time Series","text":"Timetk part amazing Modeltime Ecosystem time series forecasting. can take long time learn: Many algorithms Ensembling Resampling Machine Learning Deep Learning Scalable Modeling: 10,000+ time series probably thinking ever going learn time series forecasting. ’s solution save years struggling.","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK04_Plotting_Time_Series.html","id":"take-the-high-performance-forecasting-course","dir":"Articles > Temp_archive","previous_headings":"","what":"Take the High-Performance Forecasting Course","title":"Visualizing Time Series","text":"Become forecasting expert organization  High-Performance Time Series Course","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK04_Plotting_Time_Series.html","id":"time-series-is-changing","dir":"Articles > Temp_archive","previous_headings":"Take the High-Performance Forecasting Course","what":"Time Series is Changing","title":"Visualizing Time Series","text":"Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies improving accuracy scalability. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System).","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK04_Plotting_Time_Series.html","id":"how-to-learn-high-performance-time-series-forecasting","dir":"Articles > Temp_archive","previous_headings":"Take the High-Performance Forecasting Course","what":"How to Learn High-Performance Time Series Forecasting","title":"Visualizing Time Series","text":"teach build HPTFS System High-Performance Time Series Forecasting Course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Become Time Series Expert organization. Take High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK05_Plotting_Seasonality_and_Correlation.html","id":"libraries","dir":"Articles > Temp_archive","previous_headings":"","what":"Libraries","title":"Plotting Seasonality and Correlation","text":"Run following code set tutorial.","code":"library(dplyr) library(timetk)  # Setup for the plotly charts (# FALSE returns ggplots) interactive <- TRUE"},{"path":[]},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK05_Plotting_Seasonality_and_Correlation.html","id":"grouped-acf-diagnostics","dir":"Articles > Temp_archive","previous_headings":"Correlation Plots","what":"Grouped ACF Diagnostics","title":"Plotting Seasonality and Correlation","text":"","code":"m4_hourly %>%     group_by(id) %>%     plot_acf_diagnostics(         date, value,               # ACF & PACF         .lags = \"7 days\",          # 7-Days of hourly lags         .interactive = interactive     )"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK05_Plotting_Seasonality_and_Correlation.html","id":"grouped-ccf-plots","dir":"Articles > Temp_archive","previous_headings":"Correlation Plots","what":"Grouped CCF Plots","title":"Plotting Seasonality and Correlation","text":"","code":"walmart_sales_weekly %>%     select(id, Date, Weekly_Sales, Temperature, Fuel_Price) %>%     group_by(id) %>%     plot_acf_diagnostics(         Date, Weekly_Sales,        # ACF & PACF         .ccf_vars    = c(Temperature, Fuel_Price),   # CCFs         .lags        = \"3 months\",    # 3 months of weekly lags         .interactive = interactive     )"},{"path":[]},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK05_Plotting_Seasonality_and_Correlation.html","id":"seasonal-visualizations","dir":"Articles > Temp_archive","previous_headings":"Seasonality","what":"Seasonal Visualizations","title":"Plotting Seasonality and Correlation","text":"","code":"taylor_30_min %>%     plot_seasonal_diagnostics(date, value, .interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK05_Plotting_Seasonality_and_Correlation.html","id":"grouped-seasonal-visualizations","dir":"Articles > Temp_archive","previous_headings":"Seasonality","what":"Grouped Seasonal Visualizations","title":"Plotting Seasonality and Correlation","text":"","code":"m4_hourly %>%     group_by(id) %>%     plot_seasonal_diagnostics(date, value, .interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK05_Plotting_Seasonality_and_Correlation.html","id":"stl-diagnostics","dir":"Articles > Temp_archive","previous_headings":"","what":"STL Diagnostics","title":"Plotting Seasonality and Correlation","text":"","code":"m4_hourly %>%     group_by(id) %>%     plot_stl_diagnostics(         date, value,         .frequency = \"auto\", .trend = \"auto\",         .feature_set = c(\"observed\", \"season\", \"trend\", \"remainder\"),         .interactive = interactive)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK05_Plotting_Seasonality_and_Correlation.html","id":"learning-more","dir":"Articles > Temp_archive","previous_headings":"","what":"Learning More","title":"Plotting Seasonality and Correlation","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"prerequisites","dir":"Articles > Temp_archive","previous_headings":"","what":"Prerequisites","title":"Frequency and Trend Selection","text":"get started, load following packages.","code":"library(dplyr) library(timetk)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"data","dir":"Articles > Temp_archive","previous_headings":"","what":"Data","title":"Frequency and Trend Selection","text":"Daily Irregular Data daily stock prices Facebook 2013 2016. Note trading days occur “business days” (non-weekends non-business-holidays). Sub-Daily Data Taylor’s Energy Demand data 30-minute timestamp interval.","code":"data(FANG)  FB_tbl <- FANG %>% filter(symbol == \"FB\") FB_tbl ## # A tibble: 1,008 × 8 ##    symbol date        open  high   low close    volume adjusted ##    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl> ##  1 FB     2013-01-02  27.4  28.2  27.4  28    69846400     28   ##  2 FB     2013-01-03  27.9  28.5  27.6  27.8  63140600     27.8 ##  3 FB     2013-01-04  28.0  28.9  27.8  28.8  72715400     28.8 ##  4 FB     2013-01-07  28.7  29.8  28.6  29.4  83781800     29.4 ##  5 FB     2013-01-08  29.5  29.6  28.9  29.1  45871300     29.1 ##  6 FB     2013-01-09  29.7  30.6  29.5  30.6 104787700     30.6 ##  7 FB     2013-01-10  30.6  31.5  30.3  31.3  95316400     31.3 ##  8 FB     2013-01-11  31.3  32.0  31.1  31.7  89598000     31.7 ##  9 FB     2013-01-14  32.1  32.2  30.6  31.0  98892800     31.0 ## 10 FB     2013-01-15  30.6  31.7  29.9  30.1 173242600     30.1 ## # ℹ 998 more rows taylor_30_min ## # A tibble: 4,032 × 2 ##    date                value ##    <dttm>              <dbl> ##  1 2000-06-05 00:00:00 22262 ##  2 2000-06-05 00:30:00 21756 ##  3 2000-06-05 01:00:00 22247 ##  4 2000-06-05 01:30:00 22759 ##  5 2000-06-05 02:00:00 22549 ##  6 2000-06-05 02:30:00 22313 ##  7 2000-06-05 03:00:00 22128 ##  8 2000-06-05 03:30:00 21860 ##  9 2000-06-05 04:00:00 21751 ## 10 2000-06-05 04:30:00 21336 ## # ℹ 4,022 more rows"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"applications","dir":"Articles > Temp_archive","previous_headings":"","what":"Applications","title":"Frequency and Trend Selection","text":"example automatic frequency detection occurs plot_stl_diagnostics() function.","code":"taylor_30_min %>%     plot_stl_diagnostics(date, value,                           .frequency = \"auto\", .trend = \"auto\",                          .interactive = FALSE) ## frequency = 48 observations per 1 day ## trend = 672 observations per 14 days"},{"path":[]},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"specifying-a-frequency-or-trend","dir":"Articles > Temp_archive","previous_headings":"Automatic Frequency & Trend Selection","what":"Specifying a Frequency or Trend","title":"Frequency and Trend Selection","text":"period argument three basic options returning frequency. Options include: “auto”: target frequency determined using pre-defined Time Scale Template (see ). time-based duration: (e.g. “7 days” “2 quarters” per cycle) numeric number observations: (e.g. 5 5 observations per cycle)","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"frequency","dir":"Articles > Temp_archive","previous_headings":"Automatic Frequency & Trend Selection","what":"Frequency","title":"Frequency and Trend Selection","text":"frequency loosely defined number observations comprise cycle data set. Using tk_get_frequency(), can pick number observations roughly define frequency series. Daily Irregular Data FB_tbl irregular (weekends holidays present), frequency selected weekly week 5-days typically. 5 selected. Sub-Daily Data works well sub-daily time series. ’ll use taylor_30_min 30-minute timestamp series. frequency selected 48 48 timestamps (observations) 1 day 30-minute cycle.","code":"FB_tbl %>% tk_index() %>% tk_get_frequency(period = \"auto\") ## frequency = 5 observations per 1 week ## [1] 5 taylor_30_min %>% tk_index() %>% tk_get_frequency(\"1 day\") ## frequency = 48 observations per 1 day ## [1] 48"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"trend","dir":"Articles > Temp_archive","previous_headings":"Automatic Frequency & Trend Selection","what":"Trend","title":"Frequency and Trend Selection","text":"trend loosely defined time span can aggregated across visualize central tendency data. Using tk_get_trend(), can pick number observations help describe trend data. Daily Irregular Data FB_tbl irregular (weekends holidays present), trend selected 3 months week 5-days typically. 64 observations selected. Sub-Daily Data 14-day (2 week) interval selected “30-minute” interval data.","code":"FB_tbl %>% tk_index() %>% tk_get_trend(period = \"auto\") ## trend = 64 observations per 3 months ## [1] 64 taylor_30_min %>% tk_index() %>% tk_get_trend(\"auto\") ## trend = 672 observations per 14 days ## [1] 672"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"time-scale-template","dir":"Articles > Temp_archive","previous_headings":"","what":"Time Scale Template","title":"Frequency and Trend Selection","text":"Time-Scale Template used get set time scale template, used tk_get_frequency() tk_get_trend() period = \"auto\". predefined template stored function tk_time_scale_template(). default used timetk. Accessing Default Template can access current template get_tk_time_scale_template(). Changing Default Template can modify current template set_tk_time_scale_template().","code":"get_tk_time_scale_template() ## # A tibble: 8 × 3 ##   time_scale frequency trend    ##   <chr>      <chr>     <chr>    ## 1 second     1 hour    12 hours ## 2 minute     1 day     14 days  ## 3 hour       1 day     1 month  ## 4 day        1 week    3 months ## 5 week       1 quarter 1 year   ## 6 month      1 year    5 years  ## 7 quarter    1 year    10 years ## 8 year       5 years   30 years"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK06_Automatic_Frequency_And_Trend_Selection.html","id":"learning-more","dir":"Articles > Temp_archive","previous_headings":"","what":"Learning More","title":"Frequency and Trend Selection","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK07_Time_Series_Data_Wrangling.html","id":"libraries","dir":"Articles > Temp_archive","previous_headings":"","what":"Libraries","title":"Time Series Data Wrangling","text":"Load following libraries.","code":"library(dplyr) library(tidyr) library(timetk)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK07_Time_Series_Data_Wrangling.html","id":"data","dir":"Articles > Temp_archive","previous_headings":"","what":"Data","title":"Time Series Data Wrangling","text":"tutorial use FANG dataset: Daily Irregular (missing business holidays weekends) 4 groups (FB, AMZN, NFLX, GOOG). adjusted column contains adjusted closing prices day.  volume column contains trade volume (number times stock transacted) day.","code":"FANG ## # A tibble: 4,032 × 8 ##    symbol date        open  high   low close    volume adjusted ##    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl> ##  1 FB     2013-01-02  27.4  28.2  27.4  28    69846400     28   ##  2 FB     2013-01-03  27.9  28.5  27.6  27.8  63140600     27.8 ##  3 FB     2013-01-04  28.0  28.9  27.8  28.8  72715400     28.8 ##  4 FB     2013-01-07  28.7  29.8  28.6  29.4  83781800     29.4 ##  5 FB     2013-01-08  29.5  29.6  28.9  29.1  45871300     29.1 ##  6 FB     2013-01-09  29.7  30.6  29.5  30.6 104787700     30.6 ##  7 FB     2013-01-10  30.6  31.5  30.3  31.3  95316400     31.3 ##  8 FB     2013-01-11  31.3  32.0  31.1  31.7  89598000     31.7 ##  9 FB     2013-01-14  32.1  32.2  30.6  31.0  98892800     31.0 ## 10 FB     2013-01-15  30.6  31.7  29.9  30.1 173242600     30.1 ## # ℹ 4,022 more rows FANG %>%   group_by(symbol) %>%   plot_time_series(date, adjusted, .facet_ncol = 2, .interactive = FALSE) FANG %>%   group_by(symbol) %>%   plot_time_series(date, volume, .facet_ncol = 2, .interactive = FALSE)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK07_Time_Series_Data_Wrangling.html","id":"summarize-by-time","dir":"Articles > Temp_archive","previous_headings":"","what":"Summarize by Time","title":"Time Series Data Wrangling","text":"summarise_by_time() aggregates period. ’s great : Period Aggregation - sum() Period Smoothing - mean(), first(), last()","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK07_Time_Series_Data_Wrangling.html","id":"period-summarization","dir":"Articles > Temp_archive","previous_headings":"Summarize by Time","what":"Period Summarization","title":"Time Series Data Wrangling","text":"Objective: Get total trade volume quarter Use sum() Aggregate using .= \"quarter\"","code":"FANG %>%   group_by(symbol) %>%   summarise_by_time(     date,      .by    = \"quarter\",     volume = sum(volume)   ) %>%   plot_time_series(date, volume, .facet_ncol = 2, .interactive = FALSE, .y_intercept = 0)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK07_Time_Series_Data_Wrangling.html","id":"period-smoothing","dir":"Articles > Temp_archive","previous_headings":"Summarize by Time","what":"Period Smoothing","title":"Time Series Data Wrangling","text":"Objective: Get first value month can use first() get first value, effect reducing data (.e. smoothing). use mean() median(). Use summarization time: .= \"month\" aggregate month.","code":"FANG %>%   group_by(symbol) %>%   summarise_by_time(     date,      .by = \"month\",     adjusted = first(adjusted)   ) %>%   plot_time_series(date, adjusted, .facet_ncol = 2, .interactive = FALSE)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK07_Time_Series_Data_Wrangling.html","id":"filter-by-time","dir":"Articles > Temp_archive","previous_headings":"","what":"Filter By Time","title":"Time Series Data Wrangling","text":"Used quickly filter continuous time range.","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK07_Time_Series_Data_Wrangling.html","id":"time-range-filtering","dir":"Articles > Temp_archive","previous_headings":"Filter By Time","what":"Time Range Filtering","title":"Time Series Data Wrangling","text":"Objective: Get adjusted stock prices 3rd quarter 2013. .start_date = \"2013-09\": Converts “2013-09-01 .end_date = \"2013\": Converts “2013-12-31 advanced example filtering using %+time %-time shown “Padding Data: Low High Frequency”.","code":"FANG %>%   group_by(symbol) %>%   filter_by_time(date, \"2013-09\", \"2013\") %>%   plot_time_series(date, adjusted, .facet_ncol = 2, .interactive = FALSE)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK07_Time_Series_Data_Wrangling.html","id":"padding-data","dir":"Articles > Temp_archive","previous_headings":"","what":"Padding Data","title":"Time Series Data Wrangling","text":"Used fill (pad) gaps go low frequency high frequency. function uses awesome padr library filling expanding timestamps.","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK07_Time_Series_Data_Wrangling.html","id":"fill-in-gaps","dir":"Articles > Temp_archive","previous_headings":"Padding Data","what":"Fill in Gaps","title":"Time Series Data Wrangling","text":"Objective: Make irregular series regular. leave padded values NA. can add value using .pad_value can impute using function like ts_impute_vec() (shown next).","code":"FANG %>%   group_by(symbol) %>%   pad_by_time(date, .by = \"auto\") # Guesses .by = \"day\" ## pad applied on the interval: day ## # A tibble: 5,836 × 8 ## # Groups:   symbol [4] ##    symbol date        open  high   low close  volume adjusted ##    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>   <dbl>    <dbl> ##  1 AMZN   2013-01-02  256.  258.  253.  257. 3271000     257. ##  2 AMZN   2013-01-03  257.  261.  256.  258. 2750900     258. ##  3 AMZN   2013-01-04  258.  260.  257.  259. 1874200     259. ##  4 AMZN   2013-01-05   NA    NA    NA    NA       NA      NA  ##  5 AMZN   2013-01-06   NA    NA    NA    NA       NA      NA  ##  6 AMZN   2013-01-07  263.  270.  263.  268. 4910000     268. ##  7 AMZN   2013-01-08  267.  269.  264.  266. 3010700     266. ##  8 AMZN   2013-01-09  268.  270.  265.  266. 2265600     266. ##  9 AMZN   2013-01-10  269.  269.  262.  265. 2863400     265. ## 10 AMZN   2013-01-11  265.  268.  264.  268. 2413300     268. ## # ℹ 5,826 more rows"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK07_Time_Series_Data_Wrangling.html","id":"low-to-high-frequency","dir":"Articles > Temp_archive","previous_headings":"Padding Data","what":"Low to High Frequency","title":"Time Series Data Wrangling","text":"Objective: Go Daily Hourly timestamp intervals 1 month start date. Impute missing values. .= \"hour\" pads daily hourly Imputation hourly data accomplished ts_impute_vec(), performs linear interpolation period = 1. “start”: special keyword signals start series FIRST(date) %+time% \"1 month\": Selecting first date sequence using special infix operation, %+time%, called “add time”. case add “1 month”.","code":"FANG %>%   group_by(symbol) %>%   pad_by_time(date, .by = \"hour\") %>%   mutate_at(vars(open:adjusted), .funs = ts_impute_vec, period = 1) %>%   filter_by_time(date, \"start\", first(date) %+time% \"1 month\") %>%   plot_time_series(date, adjusted, .facet_ncol = 2, .interactive = FALSE)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK07_Time_Series_Data_Wrangling.html","id":"sliding-rolling-calculations","dir":"Articles > Temp_archive","previous_headings":"","what":"Sliding (Rolling) Calculations","title":"Time Series Data Wrangling","text":"new function, slidify() turns function sliding (rolling) window function. takes concepts tibbletime::rollify() improves R package slider.","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK07_Time_Series_Data_Wrangling.html","id":"rolling-mean","dir":"Articles > Temp_archive","previous_headings":"Sliding (Rolling) Calculations","what":"Rolling Mean","title":"Time Series Data Wrangling","text":"Objective: Calculate “centered” simple rolling average partial window rolling start end windows. slidify() turns mean() function rolling average.  simple rolling calculations (rolling average), can accomplish operation faster slidify_vec() - vectorized rolling function simple summary rolls (e.g. mean(), sd(), sum(), etc)","code":"# Make the rolling function roll_avg_30 <- slidify(.f = mean, .period = 30, .align = \"center\", .partial = TRUE)  # Apply the rolling function FANG %>%   select(symbol, date, adjusted) %>%   group_by(symbol) %>%   # Apply Sliding Function   mutate(rolling_avg_30 = roll_avg_30(adjusted)) %>%   pivot_longer(cols = c(adjusted, rolling_avg_30)) %>%   plot_time_series(date, value, .color_var = name,                    .facet_ncol = 2, .smooth = FALSE,                     .interactive = FALSE) FANG %>%   select(symbol, date, adjusted) %>%   group_by(symbol) %>%   # Apply roll apply Function   mutate(rolling_avg_30 = slidify_vec(adjusted,  ~ mean(.),                                        .period = 30, .partial = TRUE)) ## # A tibble: 4,032 × 4 ## # Groups:   symbol [4] ##    symbol date       adjusted rolling_avg_30 ##    <chr>  <date>        <dbl>          <dbl> ##  1 FB     2013-01-02     28             30.0 ##  2 FB     2013-01-03     27.8           30.1 ##  3 FB     2013-01-04     28.8           30.2 ##  4 FB     2013-01-07     29.4           30.2 ##  5 FB     2013-01-08     29.1           30.3 ##  6 FB     2013-01-09     30.6           30.3 ##  7 FB     2013-01-10     31.3           30.3 ##  8 FB     2013-01-11     31.7           30.2 ##  9 FB     2013-01-14     31.0           30.1 ## 10 FB     2013-01-15     30.1           30.1 ## # ℹ 4,022 more rows"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK07_Time_Series_Data_Wrangling.html","id":"rolling-regression","dir":"Articles > Temp_archive","previous_headings":"Sliding (Rolling) Calculations","what":"Rolling Regression","title":"Time Series Data Wrangling","text":"Objective: Calculate rolling regression. complex sliding (rolling) calculation requires multiple columns involved. slidify() built . Use multi-variable purrr ..1, ..2, ..3, etc notation setup function","code":"# Rolling regressions are easy to implement using `.unlist = FALSE` lm_roll <- slidify(~ lm(..1 ~ ..2 + ..3), .period = 90,                     .unlist = FALSE, .align = \"right\")   FANG %>%   select(symbol, date, adjusted, volume) %>%   group_by(symbol) %>%   mutate(numeric_date = as.numeric(date)) %>%   # Apply rolling regression   mutate(rolling_lm = lm_roll(adjusted, volume, numeric_date)) %>%   filter(!is.na(rolling_lm)) ## # A tibble: 3,676 × 6 ## # Groups:   symbol [4] ##    symbol date       adjusted   volume numeric_date rolling_lm ##    <chr>  <date>        <dbl>    <dbl>        <dbl> <list>     ##  1 FB     2013-05-10     26.7 30847100        15835 <lm>       ##  2 FB     2013-05-13     26.8 29068800        15838 <lm>       ##  3 FB     2013-05-14     27.1 24930300        15839 <lm>       ##  4 FB     2013-05-15     26.6 30299800        15840 <lm>       ##  5 FB     2013-05-16     26.1 35499100        15841 <lm>       ##  6 FB     2013-05-17     26.2 29462700        15842 <lm>       ##  7 FB     2013-05-20     25.8 42402900        15845 <lm>       ##  8 FB     2013-05-21     25.7 26261300        15846 <lm>       ##  9 FB     2013-05-22     25.2 45314500        15847 <lm>       ## 10 FB     2013-05-23     25.1 37663100        15848 <lm>       ## # ℹ 3,666 more rows"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK07_Time_Series_Data_Wrangling.html","id":"learning-more","dir":"Articles > Temp_archive","previous_headings":"","what":"Learning More","title":"Time Series Data Wrangling","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK08_Automatic_Anomaly_Detection.html","id":"data","dir":"Articles > Temp_archive","previous_headings":"","what":"Data","title":"Anomaly Detection","text":"tutorial use walmart_sales_weekly dataset: Weekly Sales spikes various events","code":"walmart_sales_weekly ## # A tibble: 1,001 × 17 ##    id    Store  Dept Date       Weekly_Sales IsHoliday Type    Size Temperature ##    <fct> <dbl> <dbl> <date>            <dbl> <lgl>     <chr>  <dbl>       <dbl> ##  1 1_1       1     1 2010-02-05       24924. FALSE     A     151315        42.3 ##  2 1_1       1     1 2010-02-12       46039. TRUE      A     151315        38.5 ##  3 1_1       1     1 2010-02-19       41596. FALSE     A     151315        39.9 ##  4 1_1       1     1 2010-02-26       19404. FALSE     A     151315        46.6 ##  5 1_1       1     1 2010-03-05       21828. FALSE     A     151315        46.5 ##  6 1_1       1     1 2010-03-12       21043. FALSE     A     151315        57.8 ##  7 1_1       1     1 2010-03-19       22137. FALSE     A     151315        54.6 ##  8 1_1       1     1 2010-03-26       26229. FALSE     A     151315        51.4 ##  9 1_1       1     1 2010-04-02       57258. FALSE     A     151315        62.3 ## 10 1_1       1     1 2010-04-09       42961. FALSE     A     151315        65.9 ## # ℹ 991 more rows ## # ℹ 8 more variables: Fuel_Price <dbl>, MarkDown1 <dbl>, MarkDown2 <dbl>, ## #   MarkDown3 <dbl>, MarkDown4 <dbl>, MarkDown5 <dbl>, CPI <dbl>, ## #   Unemployment <dbl>"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK08_Automatic_Anomaly_Detection.html","id":"anomaly-visualization","dir":"Articles > Temp_archive","previous_headings":"","what":"Anomaly Visualization","title":"Anomaly Detection","text":"Using plot_anomaly_diagnostics() function, can interactively detect anomalies scale.","code":"walmart_sales_weekly %>%   group_by(Store, Dept) %>%   plot_anomaly_diagnostics(Date, Weekly_Sales, .facet_ncol = 2)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK08_Automatic_Anomaly_Detection.html","id":"automatic-anomaly-detection","dir":"Articles > Temp_archive","previous_headings":"","what":"Automatic Anomaly Detection","title":"Anomaly Detection","text":"get data anomalies, use tk_anomaly_diagnostics(), preprocessing function.","code":"walmart_sales_weekly %>%   group_by(Store, Dept) %>%   tk_anomaly_diagnostics(Date, Weekly_Sales) ## # A tibble: 1,001 × 13 ## # Groups:   Store, Dept [7] ##    Store  Dept Date       observed season  trend remainder seasadj remainder_l1 ##    <dbl> <dbl> <date>        <dbl>  <dbl>  <dbl>     <dbl>   <dbl>        <dbl> ##  1     1     1 2010-02-05   24924.   874. 19967.     4083.  24050.      -15981. ##  2     1     1 2010-02-12   46039.  -698. 19835.    26902.  46737.      -15981. ##  3     1     1 2010-02-19   41596. -1216. 19703.    23108.  42812.      -15981. ##  4     1     1 2010-02-26   19404.  -821. 19571.      653.  20224.      -15981. ##  5     1     1 2010-03-05   21828.   324. 19439.     2064.  21504.      -15981. ##  6     1     1 2010-03-12   21043.   471. 19307.     1265.  20572.      -15981. ##  7     1     1 2010-03-19   22137.   920. 19175.     2041.  21217.      -15981. ##  8     1     1 2010-03-26   26229.   752. 19069.     6409.  25478.      -15981. ##  9     1     1 2010-04-02   57258.   503. 18962.    37794.  56755.      -15981. ## 10     1     1 2010-04-09   42961.  1132. 18855.    22974.  41829.      -15981. ## # ℹ 991 more rows ## # ℹ 4 more variables: remainder_l2 <dbl>, anomaly <chr>, recomposed_l1 <dbl>, ## #   recomposed_l2 <dbl>"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK08_Automatic_Anomaly_Detection.html","id":"learning-more","dir":"Articles > Temp_archive","previous_headings":"","what":"Learning More","title":"Anomaly Detection","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK09_Clustering.html","id":"libraries","dir":"Articles > Temp_archive","previous_headings":"","what":"Libraries","title":"Time Series Clustering","text":"get started, load following libraries.","code":"library(dplyr) library(purrr) library(timetk)"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK09_Clustering.html","id":"data","dir":"Articles > Temp_archive","previous_headings":"","what":"Data","title":"Time Series Clustering","text":"tutorial use walmart_sales_weekly dataset: Weekly Sales spikes various events","code":"walmart_sales_weekly ## # A tibble: 1,001 × 17 ##    id    Store  Dept Date       Weekly_Sales IsHoliday Type    Size Temperature ##    <fct> <dbl> <dbl> <date>            <dbl> <lgl>     <chr>  <dbl>       <dbl> ##  1 1_1       1     1 2010-02-05       24924. FALSE     A     151315        42.3 ##  2 1_1       1     1 2010-02-12       46039. TRUE      A     151315        38.5 ##  3 1_1       1     1 2010-02-19       41596. FALSE     A     151315        39.9 ##  4 1_1       1     1 2010-02-26       19404. FALSE     A     151315        46.6 ##  5 1_1       1     1 2010-03-05       21828. FALSE     A     151315        46.5 ##  6 1_1       1     1 2010-03-12       21043. FALSE     A     151315        57.8 ##  7 1_1       1     1 2010-03-19       22137. FALSE     A     151315        54.6 ##  8 1_1       1     1 2010-03-26       26229. FALSE     A     151315        51.4 ##  9 1_1       1     1 2010-04-02       57258. FALSE     A     151315        62.3 ## 10 1_1       1     1 2010-04-09       42961. FALSE     A     151315        65.9 ## # ℹ 991 more rows ## # ℹ 8 more variables: Fuel_Price <dbl>, MarkDown1 <dbl>, MarkDown2 <dbl>, ## #   MarkDown3 <dbl>, MarkDown4 <dbl>, MarkDown5 <dbl>, CPI <dbl>, ## #   Unemployment <dbl>"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK09_Clustering.html","id":"ts-features","dir":"Articles > Temp_archive","previous_headings":"","what":"TS Features","title":"Time Series Clustering","text":"Using tk_tsfeatures() function, can quickly get “tsfeatures” time series. important points: features parameter come tsfeatures R package. Use one function names tsfeatures R package e.g.(“lumpiness”, “stl_features”). can supply function returns aggregation (e.g. “mean” apply base::mean() function). can supply custom functions creating function providing (e.g. my_mean() defined )","code":"# Custom Function my_mean <- function(x, na.rm=TRUE) {   mean(x, na.rm = na.rm) }  tsfeature_tbl <- walmart_sales_weekly %>%     group_by(id) %>%     tk_tsfeatures(       .date_var = Date,       .value    = Weekly_Sales,       .period   = 52,       .features = c(\"frequency\", \"stl_features\", \"entropy\", \"acf_features\", \"my_mean\"),       .scale    = TRUE,       .prefix   = \"ts_\"     ) %>%     ungroup()  tsfeature_tbl ## # A tibble: 7 × 22 ##   id    ts_frequency ts_nperiods ts_seasonal_period ts_trend    ts_spike ##   <fct>        <dbl>       <dbl>              <dbl>    <dbl>       <dbl> ## 1 1_1             52           1                 52 0.000670 0.0000280   ## 2 1_3             52           1                 52 0.0614   0.00000987  ## 3 1_8             52           1                 52 0.756    0.00000195  ## 4 1_13            52           1                 52 0.354    0.00000475  ## 5 1_38            52           1                 52 0.425    0.0000179   ## 6 1_93            52           1                 52 0.791    0.000000754 ## 7 1_95            52           1                 52 0.639    0.000000567 ## # ℹ 16 more variables: ts_linearity <dbl>, ts_curvature <dbl>, ts_e_acf1 <dbl>, ## #   ts_e_acf10 <dbl>, ts_seasonal_strength <dbl>, ts_peak <dbl>, ## #   ts_trough <dbl>, ts_entropy <dbl>, ts_x_acf1 <dbl>, ts_x_acf10 <dbl>, ## #   ts_diff1_acf1 <dbl>, ts_diff1_acf10 <dbl>, ts_diff2_acf1 <dbl>, ## #   ts_diff2_acf10 <dbl>, ts_seas_acf1 <dbl>, ts_my_mean <dbl>"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK09_Clustering.html","id":"clustering-with-k-means","dir":"Articles > Temp_archive","previous_headings":"","what":"Clustering with K-Means","title":"Time Series Clustering","text":"can quickly add cluster assignments kmeans() function tidyverse data wrangling.","code":"set.seed(123)   cluster_tbl <- tibble(     cluster = tsfeature_tbl %>%          select(-id) %>%         as.matrix() %>%         kmeans(centers = 3, nstart = 100) %>%         pluck(\"cluster\") ) %>%     bind_cols(         tsfeature_tbl     )  cluster_tbl ## # A tibble: 7 × 23 ##   cluster id    ts_frequency ts_nperiods ts_seasonal_period ts_trend    ts_spike ##     <int> <fct>        <dbl>       <dbl>              <dbl>    <dbl>       <dbl> ## 1       2 1_1             52           1                 52 0.000670 0.0000280   ## 2       2 1_3             52           1                 52 0.0614   0.00000987  ## 3       2 1_8             52           1                 52 0.756    0.00000195  ## 4       1 1_13            52           1                 52 0.354    0.00000475  ## 5       3 1_38            52           1                 52 0.425    0.0000179   ## 6       3 1_93            52           1                 52 0.791    0.000000754 ## 7       1 1_95            52           1                 52 0.639    0.000000567 ## # ℹ 16 more variables: ts_linearity <dbl>, ts_curvature <dbl>, ts_e_acf1 <dbl>, ## #   ts_e_acf10 <dbl>, ts_seasonal_strength <dbl>, ts_peak <dbl>, ## #   ts_trough <dbl>, ts_entropy <dbl>, ts_x_acf1 <dbl>, ts_x_acf10 <dbl>, ## #   ts_diff1_acf1 <dbl>, ts_diff1_acf10 <dbl>, ts_diff2_acf1 <dbl>, ## #   ts_diff2_acf10 <dbl>, ts_seas_acf1 <dbl>, ts_my_mean <dbl>"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK09_Clustering.html","id":"visualize-the-cluster-assignments","dir":"Articles > Temp_archive","previous_headings":"","what":"Visualize the Cluster Assignments","title":"Time Series Clustering","text":"Finally, can visualize cluster assignments joining cluster_tbl original walmart_sales_weekly plotting plot_time_series().","code":"cluster_tbl %>%     select(cluster, id) %>%     right_join(walmart_sales_weekly, by = \"id\") %>%     group_by(id) %>%     plot_time_series(       Date, Weekly_Sales,        .color_var   = cluster,        .facet_ncol  = 2,        .interactive = FALSE     )"},{"path":"https://business-science.github.io/timetk/articles/temp_archive/TK09_Clustering.html","id":"learning-more","dir":"Articles > Temp_archive","previous_headings":"","what":"Learning More","title":"Time Series Clustering","text":"Talk High-Performance Time Series Forecasting Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies MILLIONS dollars. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System). teach build HPTFS System High-Performance Time Series Forecasting Course. interested learning Scalable High-Performance Forecasting Strategies take course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) NEW - Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Unlock High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Matt Dancho. Author, maintainer. Davis Vaughan. Author.","code":""},{"path":"https://business-science.github.io/timetk/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Dancho M, Vaughan D (2025). timetk: Tool Kit Working Time Series. R package version 2.9.0.9000, https://github.com/business-science/timetk.","code":"@Manual{,   title = {timetk: A Tool Kit for Working with Time Series},   author = {Matt Dancho and Davis Vaughan},   year = {2025},   note = {R package version 2.9.0.9000},   url = {https://github.com/business-science/timetk}, }"},{"path":"https://business-science.github.io/timetk/index.html","id":"timetk-for-r","dir":"","previous_headings":"","what":"A Tool Kit for Working with Time Series","title":"A Tool Kit for Working with Time Series","text":"Making time series analysis R easier. Mission: make time series analysis R easier, faster, enjoyable.","code":""},{"path":"https://business-science.github.io/timetk/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"A Tool Kit for Working with Time Series","text":"Download development version latest features: , download CRAN approved version:","code":"remotes::install_github(\"business-science/timetk\") install.packages(\"timetk\")"},{"path":"https://business-science.github.io/timetk/index.html","id":"package-functionality","dir":"","previous_headings":"","what":"Package Functionality","title":"A Tool Kit for Working with Time Series","text":"many R packages working Time Series data. ’s timetk compares “tidy” time series R packages data visualization, wrangling, feature engineeering (leverage data frames tibbles).","code":""},{"path":"https://business-science.github.io/timetk/index.html","id":"getting-started","dir":"","previous_headings":"","what":"Getting Started","title":"A Tool Kit for Working with Time Series","text":"Visualizing Time Series Wrangling Time Series Full Time Series Machine Learning Feature Engineering Tutorial API Documentation articles complete list function references.","code":""},{"path":"https://business-science.github.io/timetk/index.html","id":"summary","dir":"","previous_headings":"","what":"Summary","title":"A Tool Kit for Working with Time Series","text":"Timetk amazing package part modeltime ecosystem time series analysis forecasting. forecasting system extensive, can take long time learn: Many algorithms Ensembling Resampling Machine Learning Deep Learning Scalable Modeling: 10,000+ time series probably thinking ever going learn time series forecasting. ’s solution save years struggling.","code":""},{"path":"https://business-science.github.io/timetk/index.html","id":"take-the-high-performance-forecasting-course","dir":"","previous_headings":"","what":"Take the High-Performance Forecasting Course","title":"A Tool Kit for Working with Time Series","text":"Become forecasting expert organization  High-Performance Time Series Course","code":""},{"path":"https://business-science.github.io/timetk/index.html","id":"time-series-is-changing","dir":"","previous_headings":"Take the High-Performance Forecasting Course","what":"Time Series is Changing","title":"A Tool Kit for Working with Time Series","text":"Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies improving accuracy scalability. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System).","code":""},{"path":"https://business-science.github.io/timetk/index.html","id":"how-to-learn-high-performance-time-series-forecasting","dir":"","previous_headings":"Take the High-Performance Forecasting Course","what":"How to Learn High-Performance Time Series Forecasting","title":"A Tool Kit for Working with Time Series","text":"teach build HPTFS System High-Performance Time Series Forecasting Course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Become Time Series Expert organization. Take High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/timetk/index.html","id":"acknowledgements","dir":"","previous_headings":"","what":"Acknowledgements","title":"A Tool Kit for Working with Time Series","text":"timetk package wouldn’t possible without amazing time series packages. plot_acf_diagnostics(): Leverages stats::acf(), stats::pacf() & stats::ccf() plot_stl_diagnostics(): Leverages stats::stl() Add Subtract Time (%+time% & %-time%): \"2012-01-01\" %+time% \"1 month 4 days\" uses lubridate intelligently offset day xts: Used calculate periodicity fast lag automation. ts_impute_vec() function low-level vectorized imputation using STL + Linear Interpolation uses na.interp() hood. ts_clean_vec() function low-level vectorized imputation using STL + Linear Interpolation uses tsclean() hood. Box Cox transformation auto_lambda() uses BoxCox.Lambda(). tk_make_timeseries() - Extends seq.Date() seq.POSIXt() using simple phase like “2012-02” populate entire time series start finish February 2012. filter_by_time(), between_time() - Uses innovative endpoint detection phrases like “2012” slidify() basically rollify() using slider (see ). slidify() uses slider::pslide hood. slidify_vec() uses slider::slide_vec() simple vectorized rolls (slides). pad_by_time() function wrapper padr::pad(). See step_ts_pad() apply padding preprocessing recipe! TSstudio: best interactive time series visualization tool . leverages ts system, system forecast R package uses. ton inspiration visuals came using TSstudio.","code":""},{"path":"https://business-science.github.io/timetk/reference/FANG.html","id":null,"dir":"Reference","previous_headings":"","what":"Stock prices for the ","title":"Stock prices for the ","text":"dataset containing daily historical stock prices \"FANG\" tech stocks, \"FB\", \"AMZN\", \"NFLX\", \"GOOG\", spanning beginning 2013 end 2016.","code":""},{"path":"https://business-science.github.io/timetk/reference/FANG.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stock prices for the ","text":"","code":"FANG"},{"path":"https://business-science.github.io/timetk/reference/FANG.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Stock prices for the ","text":"\"tibble\" (\"tidy\" data frame) 4,032 rows 8 variables: symbol stock ticker symbol date trade date open stock price open trading, USD high stock price highest point trading, USD low stock price lowest point trading, USD close stock price close trading, USD volume number shares traded adjusted stock price close trading adjusted stock splits, USD","code":""},{"path":"https://business-science.github.io/timetk/reference/anomalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Automatic group-wise Anomaly Detection — anomalize","title":"Automatic group-wise Anomaly Detection — anomalize","text":"anomalize() used detect anomalies time series data, either single time series multiple time series grouped specific column.","code":""},{"path":"https://business-science.github.io/timetk/reference/anomalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Automatic group-wise Anomaly Detection — anomalize","text":"","code":"anomalize(   .data,   .date_var,   .value,   .frequency = \"auto\",   .trend = \"auto\",   .method = \"stl\",   .iqr_alpha = 0.05,   .clean_alpha = 0.75,   .max_anomalies = 0.2,   .message = TRUE )"},{"path":"https://business-science.github.io/timetk/reference/anomalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Automatic group-wise Anomaly Detection — anomalize","text":".data tibble data.frame time-based column .date_var column containing either date date-time values .value column containing numeric values .frequency Controls seasonal adjustment (removal seasonality). Input can either \"auto\", time-based definition (e.g. \"2 weeks\"), numeric number observations per frequency (e.g. 10). Refer tk_get_frequency(). .trend Controls trend component. STL, trend controls sensitivity LOESS smoother, used remove remainder. Refer tk_get_trend(). .method outlier detection method. Default: \"stl\". Currently \"stl\" method. \"twitter\" planned. .iqr_alpha Controls width \"normal\" range. Lower values conservative higher values less prone incorrectly classifying \"normal\" observations. .clean_alpha Controls threshold cleaning outliers. default 0.75, means anomalies cleaned using 0.75 * lower upper bound recomposed time series, depending direction anomaly. .max_anomalies maximum percent anomalies permitted identified. .message boolean. TRUE, output information related automatic frequency trend selection (applicable).","code":""},{"path":"https://business-science.github.io/timetk/reference/anomalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Automatic group-wise Anomaly Detection — anomalize","text":"tibble data.frame following columns: observed: original data seasonal: seasonal component seasadaj: seasonal adjusted trend: trend component remainder: residual component anomaly: Yes/flag outlier detection anomaly score: distance centerline anomaly direction: -1, 0, 1 inidicator direction anomaly recomposed_l1: lower level bound recomposed time series recomposed_l2: upper level bound recomposed time series observed_clean: original data anomalies interpolated","code":""},{"path":"https://business-science.github.io/timetk/reference/anomalize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Automatic group-wise Anomaly Detection — anomalize","text":"anomalize() method anomaly detection implements 2-step process detect outliers time series. Step 1: Detrend & Remove Seasonality using STL Decomposition decomposition separates \"season\" \"trend\" components \"observed\" values leaving \"remainder\" anomaly detection. user can control two parameters: frequency trend. .frequency: Adjusts \"season\" component removed \"observed\" values. .trend: Adjusts trend window (t.window parameter stats::stl() used. user may supply .frequency .trend time-based durations (e.g. \"6 weeks\") numeric values (e.g. 180) \"auto\", predetermines frequency /trend based scale time series using tk_time_scale_template(). Step 2: Anomaly Detection \"trend\" \"season\" (seasonality) removed, anomaly detection performed \"remainder\". Anomalies identified, boundaries (recomposed_l1 recomposed_l2) determined. Anomaly Detection Method uses inner quartile range (IQR) +/-25 median. IQR Adjustment, alpha parameter default alpha = 0.05, limits established expanding 25/75 baseline IQR Factor 3 (3X). IQR Factor = 0.15 / alpha (hence 3X alpha = 0.05): increase IQR Factor controlling limits, decrease alpha, makes difficult outlier. Increase alpha make easier outlier. IQR outlier detection method used forecast::tsoutliers(). similar outlier detection method used Twitter's AnomalyDetection package. Twitter Forecast tsoutliers methods implemented Business Science's anomalize package.","code":""},{"path":"https://business-science.github.io/timetk/reference/anomalize.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Automatic group-wise Anomaly Detection — anomalize","text":"CLEVELAND, R. B., CLEVELAND, W. S., MCRAE, J. E., TERPENNING, . STL: Seasonal-Trend Decomposition Procedure Based Loess. Journal Official Statistics, Vol. 6, . 1 (1990), pp. 3-73. Owen S. Vallis, Jordan Hochenbaum Arun Kejariwal (2014). Novel Technique Long-Term Anomaly Detection Cloud. Twitter Inc.","code":""},{"path":"https://business-science.github.io/timetk/reference/anomalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Automatic group-wise Anomaly Detection — anomalize","text":"","code":"library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union  walmart_sales_weekly %>%     filter(id %in% c(\"1_1\", \"1_3\")) %>%     group_by(id) %>%     anomalize(Date, Weekly_Sales) #> frequency = 13 observations per 1 quarter #> trend = 52 observations per 1 year #> frequency = 13 observations per 1 quarter #> trend = 52 observations per 1 year #> # A tibble: 286 × 13 #> # Groups:   id [2] #>    id    Date       observed season  trend remainder seasadj anomaly #>    <fct> <date>        <dbl>  <dbl>  <dbl>     <dbl>   <dbl> <chr>   #>  1 1_1   2010-02-05   24924.   874. 19967.     4083.  24050. No      #>  2 1_1   2010-02-12   46039.  -698. 19835.    26902.  46737. Yes     #>  3 1_1   2010-02-19   41596. -1216. 19703.    23108.  42812. Yes     #>  4 1_1   2010-02-26   19404.  -821. 19571.      653.  20224. No      #>  5 1_1   2010-03-05   21828.   324. 19439.     2064.  21504. No      #>  6 1_1   2010-03-12   21043.   471. 19307.     1265.  20572. No      #>  7 1_1   2010-03-19   22137.   920. 19175.     2041.  21217. No      #>  8 1_1   2010-03-26   26229.   752. 19069.     6409.  25478. No      #>  9 1_1   2010-04-02   57258.   503. 18962.    37794.  56755. Yes     #> 10 1_1   2010-04-09   42961.  1132. 18855.    22974.  41829. Yes     #> # ℹ 276 more rows #> # ℹ 5 more variables: anomaly_direction <dbl>, anomaly_score <dbl>, #> #   recomposed_l1 <dbl>, recomposed_l2 <dbl>, observed_clean <dbl>"},{"path":"https://business-science.github.io/timetk/reference/between_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Between (For Time Series): Range detection for date or date-time sequences — between_time","title":"Between (For Time Series): Range detection for date or date-time sequences — between_time","text":"easiest way filter time series date date-time vectors. Returns logical vector indicating date date-time values within range. See filter_by_time() data.frame (tibble) implementation.","code":""},{"path":"https://business-science.github.io/timetk/reference/between_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Between (For Time Series): Range detection for date or date-time sequences — between_time","text":"","code":"between_time(index, start_date = \"start\", end_date = \"end\")"},{"path":"https://business-science.github.io/timetk/reference/between_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Between (For Time Series): Range detection for date or date-time sequences — between_time","text":"index date date-time vector. start_date starting date end_date ending date","code":""},{"path":"https://business-science.github.io/timetk/reference/between_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Between (For Time Series): Range detection for date or date-time sequences — between_time","text":"logical vector length index indicating whether timestamp value within start_date end_date range.","code":""},{"path":"https://business-science.github.io/timetk/reference/between_time.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Between (For Time Series): Range detection for date or date-time sequences — between_time","text":"Pure Time Series Filtering Flexibilty start_date  end_date parameters designed flexibility mind. side time_formula specified character 'YYYY-MM-DD HH:MM:SS', powerful shorthand available. examples : Year: start_date = '2013', end_date = '2015' Month: start_date = '2013-01', end_date = '2016-06' Day: start_date = '2013-01-05', end_date = '2016-06-04' Second: start_date = '2013-01-05 10:22:15', end_date = '2018-06-03 12:14:22' Variations: start_date = '2013', end_date = '2016-06' Key Words: \"start\" \"end\" Use keywords \"start\" \"end\" shorthand, instead specifying actual start end values. examples: Start series end 2015: start_date = 'start', end_date = '2015' Start 2014 end series: start_date = '2014', end_date = 'end' Internal Calculations shorthand dates expanded: start_date expanded first date period end_date side expanded last date period means following examples equivalent (assuming index POSIXct): start_date = '2015' equivalent start_date = '2015-01-01 + 00:00:00' end_date = '2016' equivalent 2016-12-31 + 23:59:59'","code":""},{"path":"https://business-science.github.io/timetk/reference/between_time.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Between (For Time Series): Range detection for date or date-time sequences — between_time","text":"function based tibbletime::filter_time() function developed Davis Vaughan.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/between_time.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Between (For Time Series): Range detection for date or date-time sequences — between_time","text":"","code":"library(dplyr)  index_daily <- tk_make_timeseries(\"2016-01-01\", \"2017-01-01\", by = \"day\") index_min   <- tk_make_timeseries(\"2016-01-01\", \"2017-01-01\", by = \"min\")  # How it works # - Returns TRUE/FALSE length of index # - Use sum() to tally the number of TRUE values index_daily %>% between_time(\"start\", \"2016-01\") %>% sum() #> [1] 31  # ---- INDEX SLICING ----  # Daily Series: Month of January 2016 index_daily[index_daily %>% between_time(\"start\", \"2016-01\")] #>  [1] \"2016-01-01\" \"2016-01-02\" \"2016-01-03\" \"2016-01-04\" \"2016-01-05\" #>  [6] \"2016-01-06\" \"2016-01-07\" \"2016-01-08\" \"2016-01-09\" \"2016-01-10\" #> [11] \"2016-01-11\" \"2016-01-12\" \"2016-01-13\" \"2016-01-14\" \"2016-01-15\" #> [16] \"2016-01-16\" \"2016-01-17\" \"2016-01-18\" \"2016-01-19\" \"2016-01-20\" #> [21] \"2016-01-21\" \"2016-01-22\" \"2016-01-23\" \"2016-01-24\" \"2016-01-25\" #> [26] \"2016-01-26\" \"2016-01-27\" \"2016-01-28\" \"2016-01-29\" \"2016-01-30\" #> [31] \"2016-01-31\"  # Daily Series: March 1st - June 15th, 2016 index_daily[index_daily %>% between_time(\"2016-03\", \"2016-06-15\")] #>   [1] \"2016-03-01\" \"2016-03-02\" \"2016-03-03\" \"2016-03-04\" \"2016-03-05\" #>   [6] \"2016-03-06\" \"2016-03-07\" \"2016-03-08\" \"2016-03-09\" \"2016-03-10\" #>  [11] \"2016-03-11\" \"2016-03-12\" \"2016-03-13\" \"2016-03-14\" \"2016-03-15\" #>  [16] \"2016-03-16\" \"2016-03-17\" \"2016-03-18\" \"2016-03-19\" \"2016-03-20\" #>  [21] \"2016-03-21\" \"2016-03-22\" \"2016-03-23\" \"2016-03-24\" \"2016-03-25\" #>  [26] \"2016-03-26\" \"2016-03-27\" \"2016-03-28\" \"2016-03-29\" \"2016-03-30\" #>  [31] \"2016-03-31\" \"2016-04-01\" \"2016-04-02\" \"2016-04-03\" \"2016-04-04\" #>  [36] \"2016-04-05\" \"2016-04-06\" \"2016-04-07\" \"2016-04-08\" \"2016-04-09\" #>  [41] \"2016-04-10\" \"2016-04-11\" \"2016-04-12\" \"2016-04-13\" \"2016-04-14\" #>  [46] \"2016-04-15\" \"2016-04-16\" \"2016-04-17\" \"2016-04-18\" \"2016-04-19\" #>  [51] \"2016-04-20\" \"2016-04-21\" \"2016-04-22\" \"2016-04-23\" \"2016-04-24\" #>  [56] \"2016-04-25\" \"2016-04-26\" \"2016-04-27\" \"2016-04-28\" \"2016-04-29\" #>  [61] \"2016-04-30\" \"2016-05-01\" \"2016-05-02\" \"2016-05-03\" \"2016-05-04\" #>  [66] \"2016-05-05\" \"2016-05-06\" \"2016-05-07\" \"2016-05-08\" \"2016-05-09\" #>  [71] \"2016-05-10\" \"2016-05-11\" \"2016-05-12\" \"2016-05-13\" \"2016-05-14\" #>  [76] \"2016-05-15\" \"2016-05-16\" \"2016-05-17\" \"2016-05-18\" \"2016-05-19\" #>  [81] \"2016-05-20\" \"2016-05-21\" \"2016-05-22\" \"2016-05-23\" \"2016-05-24\" #>  [86] \"2016-05-25\" \"2016-05-26\" \"2016-05-27\" \"2016-05-28\" \"2016-05-29\" #>  [91] \"2016-05-30\" \"2016-05-31\" \"2016-06-01\" \"2016-06-02\" \"2016-06-03\" #>  [96] \"2016-06-04\" \"2016-06-05\" \"2016-06-06\" \"2016-06-07\" \"2016-06-08\" #> [101] \"2016-06-09\" \"2016-06-10\" \"2016-06-11\" \"2016-06-12\" \"2016-06-13\" #> [106] \"2016-06-14\" \"2016-06-15\"  # Minute Series: index_min[index_min %>% between_time(\"2016-02-01 12:00\", \"2016-02-01 13:00\")] #>  [1] \"2016-02-01 12:00:00 UTC\" \"2016-02-01 12:01:00 UTC\" #>  [3] \"2016-02-01 12:02:00 UTC\" \"2016-02-01 12:03:00 UTC\" #>  [5] \"2016-02-01 12:04:00 UTC\" \"2016-02-01 12:05:00 UTC\" #>  [7] \"2016-02-01 12:06:00 UTC\" \"2016-02-01 12:07:00 UTC\" #>  [9] \"2016-02-01 12:08:00 UTC\" \"2016-02-01 12:09:00 UTC\" #> [11] \"2016-02-01 12:10:00 UTC\" \"2016-02-01 12:11:00 UTC\" #> [13] \"2016-02-01 12:12:00 UTC\" \"2016-02-01 12:13:00 UTC\" #> [15] \"2016-02-01 12:14:00 UTC\" \"2016-02-01 12:15:00 UTC\" #> [17] \"2016-02-01 12:16:00 UTC\" \"2016-02-01 12:17:00 UTC\" #> [19] \"2016-02-01 12:18:00 UTC\" \"2016-02-01 12:19:00 UTC\" #> [21] \"2016-02-01 12:20:00 UTC\" \"2016-02-01 12:21:00 UTC\" #> [23] \"2016-02-01 12:22:00 UTC\" \"2016-02-01 12:23:00 UTC\" #> [25] \"2016-02-01 12:24:00 UTC\" \"2016-02-01 12:25:00 UTC\" #> [27] \"2016-02-01 12:26:00 UTC\" \"2016-02-01 12:27:00 UTC\" #> [29] \"2016-02-01 12:28:00 UTC\" \"2016-02-01 12:29:00 UTC\" #> [31] \"2016-02-01 12:30:00 UTC\" \"2016-02-01 12:31:00 UTC\" #> [33] \"2016-02-01 12:32:00 UTC\" \"2016-02-01 12:33:00 UTC\" #> [35] \"2016-02-01 12:34:00 UTC\" \"2016-02-01 12:35:00 UTC\" #> [37] \"2016-02-01 12:36:00 UTC\" \"2016-02-01 12:37:00 UTC\" #> [39] \"2016-02-01 12:38:00 UTC\" \"2016-02-01 12:39:00 UTC\" #> [41] \"2016-02-01 12:40:00 UTC\" \"2016-02-01 12:41:00 UTC\" #> [43] \"2016-02-01 12:42:00 UTC\" \"2016-02-01 12:43:00 UTC\" #> [45] \"2016-02-01 12:44:00 UTC\" \"2016-02-01 12:45:00 UTC\" #> [47] \"2016-02-01 12:46:00 UTC\" \"2016-02-01 12:47:00 UTC\" #> [49] \"2016-02-01 12:48:00 UTC\" \"2016-02-01 12:49:00 UTC\" #> [51] \"2016-02-01 12:50:00 UTC\" \"2016-02-01 12:51:00 UTC\" #> [53] \"2016-02-01 12:52:00 UTC\" \"2016-02-01 12:53:00 UTC\" #> [55] \"2016-02-01 12:54:00 UTC\" \"2016-02-01 12:55:00 UTC\" #> [57] \"2016-02-01 12:56:00 UTC\" \"2016-02-01 12:57:00 UTC\" #> [59] \"2016-02-01 12:58:00 UTC\" \"2016-02-01 12:59:00 UTC\" #> [61] \"2016-02-01 13:00:00 UTC\"  # ---- FILTERING WITH DPLYR ---- FANG %>%     group_by(symbol) %>%     filter(date %>% between_time(\"2016-01\", \"2016-01\")) #> # A tibble: 76 × 8 #> # Groups:   symbol [4] #>    symbol date        open  high   low close   volume adjusted #>    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl> #>  1 FB     2016-01-04 102.  102.   99.8 102.  37912400    102.  #>  2 FB     2016-01-05 103.  104.  102.  103.  23258200    103.  #>  3 FB     2016-01-06 101.  104.  101.  103.  25096200    103.  #>  4 FB     2016-01-07 100.  101.   97.3  97.9 45172900     97.9 #>  5 FB     2016-01-08  99.9 100.   97.0  97.3 35402300     97.3 #>  6 FB     2016-01-11  97.9  98.6  95.4  97.5 29932400     97.5 #>  7 FB     2016-01-12  99   100.0  97.6  99.4 28395400     99.4 #>  8 FB     2016-01-13 101.  101.   95.2  95.4 33410600     95.4 #>  9 FB     2016-01-14  95.8  98.9  92.4  98.4 48658600     98.4 #> 10 FB     2016-01-15  94.0  96.4  93.5  95.0 45935600     95.0 #> # ℹ 66 more rows"},{"path":"https://business-science.github.io/timetk/reference/bike_sharing_daily.html","id":null,"dir":"Reference","previous_headings":"","what":"Daily Bike Sharing Data — bike_sharing_daily","title":"Daily Bike Sharing Data — bike_sharing_daily","text":"dataset contains daily count rental bike transactions years 2011 2012 Capital bikeshare system corresponding weather seasonal information.","code":""},{"path":"https://business-science.github.io/timetk/reference/bike_sharing_daily.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Daily Bike Sharing Data — bike_sharing_daily","text":"","code":"bike_sharing_daily"},{"path":"https://business-science.github.io/timetk/reference/bike_sharing_daily.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Daily Bike Sharing Data — bike_sharing_daily","text":"tibble: 731 x 16 instant: record index dteday : date season : season (1:winter, 2:spring, 3:summer, 4:fall) yr : year (0: 2011, 1:2012) mnth : month ( 1 12) hr : hour (0 23) holiday : weather day holiday weekday : day week workingday : day neither weekend holiday 1, otherwise 0. weathersit : 1: Clear, clouds, Partly cloudy, Partly cloudy 2: Mist + Cloudy, Mist + Broken clouds, Mist + clouds, Mist 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog temp : Normalized temperature Celsius. values derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (hourly scale) atemp: Normalized feeling temperature Celsius. values derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (hourly scale) hum: Normalized humidity. values divided 100 (max) windspeed: Normalized wind speed. values divided 67 (max) casual: count casual users registered: count registered users cnt: count total rental bikes including casual registered","code":""},{"path":"https://business-science.github.io/timetk/reference/bike_sharing_daily.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Daily Bike Sharing Data — bike_sharing_daily","text":"Fanaee-T, Hadi, Gama, Joao, 'Event labeling combining ensemble detectors background knowledge', Progress Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg.","code":""},{"path":"https://business-science.github.io/timetk/reference/bike_sharing_daily.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Daily Bike Sharing Data — bike_sharing_daily","text":"","code":"bike_sharing_daily #> # A tibble: 731 × 16 #>    instant dteday     season    yr  mnth holiday weekday workingday weathersit #>      <dbl> <date>      <dbl> <dbl> <dbl>   <dbl>   <dbl>      <dbl>      <dbl> #>  1       1 2011-01-01      1     0     1       0       6          0          2 #>  2       2 2011-01-02      1     0     1       0       0          0          2 #>  3       3 2011-01-03      1     0     1       0       1          1          1 #>  4       4 2011-01-04      1     0     1       0       2          1          1 #>  5       5 2011-01-05      1     0     1       0       3          1          1 #>  6       6 2011-01-06      1     0     1       0       4          1          1 #>  7       7 2011-01-07      1     0     1       0       5          1          2 #>  8       8 2011-01-08      1     0     1       0       6          0          2 #>  9       9 2011-01-09      1     0     1       0       0          0          1 #> 10      10 2011-01-10      1     0     1       0       1          1          1 #> # ℹ 721 more rows #> # ℹ 7 more variables: temp <dbl>, atemp <dbl>, hum <dbl>, windspeed <dbl>, #> #   casual <dbl>, registered <dbl>, cnt <dbl>"},{"path":"https://business-science.github.io/timetk/reference/box_cox_vec.html","id":null,"dir":"Reference","previous_headings":"","what":"Box Cox Transformation — box_cox_vec","title":"Box Cox Transformation — box_cox_vec","text":"mainly wrapper BoxCox transformation forecast R package. box_cox_vec() function performs transformation. box_cox_inv_vec() inverts transformation. auto_lambda() helps selecting optimal lambda value.","code":""},{"path":"https://business-science.github.io/timetk/reference/box_cox_vec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Box Cox Transformation — box_cox_vec","text":"","code":"box_cox_vec(x, lambda = \"auto\", silent = FALSE)  box_cox_inv_vec(x, lambda)  auto_lambda(   x,   method = c(\"guerrero\", \"loglik\"),   lambda_lower = -1,   lambda_upper = 2 )"},{"path":"https://business-science.github.io/timetk/reference/box_cox_vec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Box Cox Transformation — box_cox_vec","text":"x numeric vector. lambda box cox transformation parameter. set \"auto\", performs automated lambda selection using auto_lambda(). silent Whether report automated lambda selection message. method method used automatic lambda selection. Either \"guerrero\" \"loglik\". lambda_lower lower limit automatic lambda selection lambda_upper upper limit automatic lambda selection","code":""},{"path":"https://business-science.github.io/timetk/reference/box_cox_vec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Box Cox Transformation — box_cox_vec","text":"Returns numeric vector transformed.","code":""},{"path":"https://business-science.github.io/timetk/reference/box_cox_vec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Box Cox Transformation — box_cox_vec","text":"Box Cox transformation power transformation commonly used reduce variance time series. Automatic Lambda Selection desired, lambda argument can selected using auto_lambda(), wrapper Forecast R Package's forecast::BoxCox.lambda() function. Use either 2 methods: \"guerrero\" - Minimizes non-seasonal variance \"loglik\" - Maximizes log-likelihood linear model fit x","code":""},{"path":"https://business-science.github.io/timetk/reference/box_cox_vec.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Box Cox Transformation — box_cox_vec","text":"Forecast R Package Forecasting: Principles & Practices: Transformations & Adjustments Guerrero, V.M. (1993) Time-series analysis supported power transformations. Journal Forecasting, 12,  37–48.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/box_cox_vec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Box Cox Transformation — box_cox_vec","text":"","code":"library(dplyr) d10_daily <- m4_daily %>% dplyr::filter(id == \"D10\")  # --- VECTOR ----  value_bc <- box_cox_vec(d10_daily$value) #> box_cox_vec(): Using value for lambda: 1.25119350454964 value    <- box_cox_inv_vec(value_bc, lambda = 1.25119350454964)  # --- MUTATE ----  m4_daily %>%     dplyr::group_by(id) %>%     dplyr::mutate(value_bc = box_cox_vec(value)) #> box_cox_vec(): Using value for lambda: 1.25119350454964 #> box_cox_vec(): Using value for lambda: 0.0882021886505848 #> box_cox_vec(): Using value for lambda: 1.99992424816297 #> box_cox_vec(): Using value for lambda: 0.401716085353735 #> # A tibble: 9,743 × 4 #> # Groups:   id [4] #>    id    date       value value_bc #>    <fct> <date>     <dbl>    <dbl> #>  1 D10   2014-07-03 2076.   11303. #>  2 D10   2014-07-04 2073.   11284. #>  3 D10   2014-07-05 2049.   11116. #>  4 D10   2014-07-06 2049.   11117. #>  5 D10   2014-07-07 2006.   10829. #>  6 D10   2014-07-08 2018.   10905. #>  7 D10   2014-07-09 2019.   10915. #>  8 D10   2014-07-10 2007.   10836. #>  9 D10   2014-07-11 2010    10854. #> 10 D10   2014-07-12 2002.   10796. #> # ℹ 9,733 more rows"},{"path":"https://business-science.github.io/timetk/reference/condense_period.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert the Period to a Lower Periodicity (e.g. Go from Daily to Monthly) — condense_period","title":"Convert the Period to a Lower Periodicity (e.g. Go from Daily to Monthly) — condense_period","text":"Convert data.frame object daily monthly, minute data hourly, . allows user easily aggregate data less granular level taking value either beginning end period.","code":""},{"path":"https://business-science.github.io/timetk/reference/condense_period.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert the Period to a Lower Periodicity (e.g. Go from Daily to Monthly) — condense_period","text":"","code":"condense_period(.data, .date_var, .period = \"1 day\", .side = c(\"start\", \"end\"))"},{"path":"https://business-science.github.io/timetk/reference/condense_period.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert the Period to a Lower Periodicity (e.g. Go from Daily to Monthly) — condense_period","text":".data tbl object data.frame .date_var column containing date date-time values. missing, attempts auto-detect date column. .period period condense time series . Time units condensed using lubridate::floor_date() lubridate::ceiling_date(). value can : second minute hour day week month bimonth quarter season halfyear year Arbitrary unique English abbreviations lubridate::period() constructor allowed: \"1 year\" \"2 months\" \"30 seconds\" .side One \"start\" \"end\". Determines first observation period returned last.","code":""},{"path":"https://business-science.github.io/timetk/reference/condense_period.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert the Period to a Lower Periodicity (e.g. Go from Daily to Monthly) — condense_period","text":"tibble data.frame","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/condense_period.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert the Period to a Lower Periodicity (e.g. Go from Daily to Monthly) — condense_period","text":"","code":"# Libraries library(dplyr)  # First value in each month m4_daily %>%     group_by(id) %>%     condense_period(.period = \"1 month\") #> .date_var is missing. Using: date #> # A tibble: 323 × 3 #> # Groups:   id [4] #>    id    date       value #>    <fct> <date>     <dbl> #>  1 D10   2014-07-03 2076. #>  2 D10   2014-08-01 1923. #>  3 D10   2014-09-01 1908. #>  4 D10   2014-10-01 2049. #>  5 D10   2014-11-01 2133. #>  6 D10   2014-12-01 2244. #>  7 D10   2015-01-01 2351  #>  8 D10   2015-02-01 2286. #>  9 D10   2015-03-01 2291. #> 10 D10   2015-04-01 2396. #> # ℹ 313 more rows  # Last value in each month m4_daily %>%     group_by(id) %>%     condense_period(.period = \"1 month\", .side = \"end\") #> .date_var is missing. Using: date #> # A tibble: 323 × 3 #> # Groups:   id [4] #>    id    date       value #>    <fct> <date>     <dbl> #>  1 D10   2014-07-31 1917. #>  2 D10   2014-08-31 1921. #>  3 D10   2014-09-30 2024. #>  4 D10   2014-10-31 2130  #>  5 D10   2014-11-30 2217. #>  6 D10   2014-12-31 2328. #>  7 D10   2015-01-31 2210. #>  8 D10   2015-02-28 2293. #>  9 D10   2015-03-31 2392. #> 10 D10   2015-04-30 2368. #> # ℹ 313 more rows"},{"path":"https://business-science.github.io/timetk/reference/diff_vec.html","id":null,"dir":"Reference","previous_headings":"","what":"Differencing Transformation — diff_vec","title":"Differencing Transformation — diff_vec","text":"diff_vec() applies Differencing Transformation. diff_inv_vec() inverts differencing transformation.","code":""},{"path":"https://business-science.github.io/timetk/reference/diff_vec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Differencing Transformation — diff_vec","text":"","code":"diff_vec(   x,   lag = 1,   difference = 1,   log = FALSE,   initial_values = NULL,   silent = FALSE )  diff_inv_vec(x, lag = 1, difference = 1, log = FALSE, initial_values = NULL)"},{"path":"https://business-science.github.io/timetk/reference/diff_vec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Differencing Transformation — diff_vec","text":"x numeric vector differenced inverted. lag lag (far back) included differencing calculation. difference number differences perform. 1 Difference equivalent measuring period change. 2 Differences equivalent measuring period acceleration. log log differences calculated. Note difference inversion log-difference approximate. initial_values used diff_vec_inv() operation. numeric vector initial values, used invert differences. vector original values length NA missing differences. silent Whether report initial values used invert difference message.","code":""},{"path":"https://business-science.github.io/timetk/reference/diff_vec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Differencing Transformation — diff_vec","text":"numeric vector","code":""},{"path":"https://business-science.github.io/timetk/reference/diff_vec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Differencing Transformation — diff_vec","text":"Benefits: function NA padded default works well dplyr::mutate() operations. Difference Calculation Single differencing, diff_vec(x_t) equivalent : x_t - x_t1, subscript _t1 indicates first lag. transformation can interpereted change. Double Differencing Calculation Double differencing, diff_vec(x_t, difference = 2) equivalent : (x_t - x_t1) - (x_t - x_t1)_t1, subscript _t1 indicates first lag. transformation can interpereted acceleration. Log Difference Calculation Log differencing, diff_vec(x_t, log = TRUE) equivalent : log(x_t) - log(x_t1) = log(x_t / x_t1), x_t series x_t1 first lag. 1st difference diff_vec(difference = 1, log = TRUE) interesting property diff_vec(difference = 1, log = TRUE) %>% exp() approximately 1 + rate change.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/diff_vec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Differencing Transformation — diff_vec","text":"","code":"library(dplyr)  # --- USAGE ----  diff_vec(1:10, lag = 2, difference = 2) %>%     diff_inv_vec(lag = 2, difference = 2, initial_values = 1:4) #> diff_vec(): Initial values: 1, 2, 3, 4 #>  [1]  1  2  3  4  5  6  7  8  9 10  # --- VECTOR ----  # Get Change 1:10 %>% diff_vec() #> diff_vec(): Initial values: 1 #>  [1] NA  1  1  1  1  1  1  1  1  1  # Get Acceleration 1:10 %>% diff_vec(difference = 2) #> diff_vec(): Initial values: 1, 2 #>  [1] NA NA  0  0  0  0  0  0  0  0  # Get approximate rate of change 1:10 %>% diff_vec(log = TRUE) %>% exp() - 1 #> diff_vec(): Initial values: 1 #>  [1]        NA 1.0000000 0.5000000 0.3333333 0.2500000 0.2000000 0.1666667 #>  [8] 0.1428571 0.1250000 0.1111111   # --- MUTATE ----  m4_daily %>%     group_by(id) %>%     mutate(difference = diff_vec(value, lag = 1)) %>%     mutate(         difference_inv = diff_inv_vec(             difference,             lag = 1,             # Add initial value to calculate the inverse difference             initial_values = value[1]         )     ) #> diff_vec(): Initial values: 2076.2 #> diff_vec(): Initial values: 1821.9 #> diff_vec(): Initial values: 9109.38 #> diff_vec(): Initial values: 5647.3 #> # A tibble: 9,743 × 5 #> # Groups:   id [4] #>    id    date       value difference difference_inv #>    <fct> <date>     <dbl>      <dbl>          <dbl> #>  1 D10   2014-07-03 2076.     NA              2076. #>  2 D10   2014-07-04 2073.     -2.80           2073. #>  3 D10   2014-07-05 2049.    -24.7            2049. #>  4 D10   2014-07-06 2049.      0.200          2049. #>  5 D10   2014-07-07 2006.    -42.5            2006. #>  6 D10   2014-07-08 2018.     11.2            2018. #>  7 D10   2014-07-09 2019.      1.5            2019. #>  8 D10   2014-07-10 2007.    -11.7            2007. #>  9 D10   2014-07-11 2010       2.60           2010  #> 10 D10   2014-07-12 2002.     -8.5            2002. #> # ℹ 9,733 more rows"},{"path":"https://business-science.github.io/timetk/reference/filter_by_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter (for Time-Series Data) — filter_by_time","title":"Filter (for Time-Series Data) — filter_by_time","text":"easiest way filter time-based start/end ranges using shorthand timeseries notation. See filter_period() applying filter expression period (windows).","code":""},{"path":"https://business-science.github.io/timetk/reference/filter_by_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter (for Time-Series Data) — filter_by_time","text":"","code":"filter_by_time(.data, .date_var, .start_date = \"start\", .end_date = \"end\")"},{"path":"https://business-science.github.io/timetk/reference/filter_by_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter (for Time-Series Data) — filter_by_time","text":".data tibble time-based column. .date_var column containing date date-time values filter. missing, attempts auto-detect date column. .start_date starting date filter sequence .end_date ending date filter sequence","code":""},{"path":"https://business-science.github.io/timetk/reference/filter_by_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter (for Time-Series Data) — filter_by_time","text":"Returns tibble data.frame filtered.","code":""},{"path":"https://business-science.github.io/timetk/reference/filter_by_time.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Filter (for Time-Series Data) — filter_by_time","text":"Pure Time Series Filtering Flexibilty .start_date  .end_date parameters designed flexibility mind. side time_formula specified character 'YYYY-MM-DD HH:MM:SS', powerful shorthand available. examples : Year: .start_date = '2013', .end_date = '2015' Month: .start_date = '2013-01', .end_date = '2016-06' Day: .start_date = '2013-01-05', .end_date = '2016-06-04' Second: .start_date = '2013-01-05 10:22:15', .end_date = '2018-06-03 12:14:22' Variations: .start_date = '2013', .end_date = '2016-06' Key Words: \"start\" \"end\" Use keywords \"start\" \"end\" shorthand, instead specifying actual start end values. examples: Start series end 2015: .start_date = 'start', .end_date = '2015' Start 2014 end series: .start_date = '2014', .end_date = 'end' Internal Calculations shorthand dates expanded: .start_date expanded first date period .end_date side expanded last date period means following examples equivalent (assuming index POSIXct): .start_date = '2015' equivalent .start_date = '2015-01-01 + 00:00:00' .end_date = '2016' equivalent 2016-12-31 + 23:59:59'","code":""},{"path":"https://business-science.github.io/timetk/reference/filter_by_time.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Filter (for Time-Series Data) — filter_by_time","text":"function based tibbletime::filter_time() function developed Davis Vaughan.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/filter_by_time.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter (for Time-Series Data) — filter_by_time","text":"","code":"library(dplyr)  # Filter values in January 1st through end of February, 2013 FANG %>%     group_by(symbol) %>%     filter_by_time(.start_date = \"start\", .end_date = \"2013-02\") %>%     plot_time_series(date, adjusted, .facet_ncol = 2, .interactive = FALSE) #> .date_var is missing. Using: date"},{"path":"https://business-science.github.io/timetk/reference/filter_period.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply filtering expressions inside periods (windows) — filter_period","title":"Apply filtering expressions inside periods (windows) — filter_period","text":"Applies dplyr filtering expression inside time-based period (window). See filter_by_time() filtering continuous ranges defined start/end dates. filter_period() enables filtering expressions like: Filtering maximum value month. Filtering first date month. Filtering rows value greater monthly average","code":""},{"path":"https://business-science.github.io/timetk/reference/filter_period.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply filtering expressions inside periods (windows) — filter_period","text":"","code":"filter_period(.data, ..., .date_var, .period = \"1 day\")"},{"path":"https://business-science.github.io/timetk/reference/filter_period.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply filtering expressions inside periods (windows) — filter_period","text":".data tbl object data.frame ... Filtering expression. Expressions return logical value, defined terms variables .data. multiple expressions included, combined & operator. rows conditions evaluate TRUE kept. .date_var column containing date date-time values. missing, attempts auto-detect date column. .period period filter within. Time units grouped using lubridate::floor_date() lubridate::ceiling_date(). value can : second minute hour day week month bimonth quarter season halfyear year Arbitrary unique English abbreviations lubridate::period() constructor allowed: \"1 year\" \"2 months\" \"30 seconds\"","code":""},{"path":"https://business-science.github.io/timetk/reference/filter_period.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply filtering expressions inside periods (windows) — filter_period","text":"tibble data.frame","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/filter_period.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply filtering expressions inside periods (windows) — filter_period","text":"","code":"# Libraries library(dplyr)  # Max value in each month m4_daily %>%     group_by(id) %>%     filter_period(.period = \"1 month\", value == max(value)) #> .date_var is missing. Using: date #> # A tibble: 350 × 3 #> # Groups:   id [4] #>    id    date       value #>    <fct> <date>     <dbl> #>  1 D10   2014-07-03 2076. #>  2 D10   2014-08-08 2028. #>  3 D10   2014-09-30 2024. #>  4 D10   2014-10-12 2155. #>  5 D10   2014-11-13 2245. #>  6 D10   2014-12-30 2345. #>  7 D10   2015-01-09 2369. #>  8 D10   2015-02-09 2341. #>  9 D10   2015-03-31 2392. #> 10 D10   2015-04-13 2500. #> # ℹ 340 more rows  # First date each month m4_daily %>%     group_by(id) %>%     filter_period(.period = \"1 month\", date == first(date)) #> .date_var is missing. Using: date #> # A tibble: 323 × 3 #> # Groups:   id [4] #>    id    date       value #>    <fct> <date>     <dbl> #>  1 D10   2014-07-03 2076. #>  2 D10   2014-08-01 1923. #>  3 D10   2014-09-01 1908. #>  4 D10   2014-10-01 2049. #>  5 D10   2014-11-01 2133. #>  6 D10   2014-12-01 2244. #>  7 D10   2015-01-01 2351  #>  8 D10   2015-02-01 2286. #>  9 D10   2015-03-01 2291. #> 10 D10   2015-04-01 2396. #> # ℹ 313 more rows  # All observations that are greater than a monthly average m4_daily %>%     group_by(id) %>%     filter_period(.period = \"1 month\", value > mean(value)) #> .date_var is missing. Using: date #> # A tibble: 4,880 × 3 #> # Groups:   id [4] #>    id    date       value #>    <fct> <date>     <dbl> #>  1 D10   2014-07-03 2076. #>  2 D10   2014-07-04 2073. #>  3 D10   2014-07-05 2049. #>  4 D10   2014-07-06 2049. #>  5 D10   2014-07-07 2006. #>  6 D10   2014-07-08 2018. #>  7 D10   2014-07-09 2019. #>  8 D10   2014-07-10 2007. #>  9 D10   2014-07-11 2010  #> 10 D10   2014-07-12 2002. #> # ℹ 4,870 more rows"},{"path":"https://business-science.github.io/timetk/reference/fourier_vec.html","id":null,"dir":"Reference","previous_headings":"","what":"Fourier Series — fourier_vec","title":"Fourier Series — fourier_vec","text":"fourier_vec() calculates Fourier Series date date-time index.","code":""},{"path":"https://business-science.github.io/timetk/reference/fourier_vec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fourier Series — fourier_vec","text":"","code":"fourier_vec(x, period, K = 1, type = c(\"sin\", \"cos\"), scale_factor = NULL)"},{"path":"https://business-science.github.io/timetk/reference/fourier_vec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fourier Series — fourier_vec","text":"x date, POSIXct, yearmon, yearqtr, numeric sequence (scaled difference 1 period alignment) converted fourier series. period number observations complete one cycle. K fourier term order. type Either \"sin\" \"cos\" appropriate type fourier term. scale_factor Scale factor calculated value scales date sequences numeric sequences. user can provide different value scale factor override date scaling. Default: NULL (auto-scale).","code":""},{"path":"https://business-science.github.io/timetk/reference/fourier_vec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fourier Series — fourier_vec","text":"numeric vector","code":""},{"path":"https://business-science.github.io/timetk/reference/fourier_vec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fourier Series — fourier_vec","text":"Benefits: function NA padded default works well dplyr::mutate() operations. Fourier Series Calculation internal calculation relatively straightforward: fourier(x) = sin(2 * pi * term * x) cos(2 * pi * term * x), term = K / period. Period Alignment, period period alignment sequence essential part fourier series calculation. Date, Date-Time, Zoo (yearqtr yearmon) Sequences - scaled unit difference 1. happens internally, nothing need worry . Future time series scaled appropriately. Numeric Sequences - scaled, means transform unit difference 1 x sequence increases 1. Otherwise period fourier order incorrectly calculated. solution just take sequence divide median difference values. Fourier Order, K fourier order parameter increases frequency. K = 2 doubles frequency. common time series analysis add multiple fourier orders (e.g. 1 5) account seasonalities occur faster primary seasonality. Type (Sin/Cos) type fourier series can either sin cos. common time series analysis add sin cos series.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/fourier_vec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fourier Series — fourier_vec","text":"","code":"library(dplyr)  # Set max.print to 50 options_old <- options()$max.print options(max.print = 50)  date_sequence <- tk_make_timeseries(\"2016-01-01\", \"2016-01-31\", by = \"hour\")  # --- VECTOR ---  fourier_vec(date_sequence, period = 7 * 24, K = 1, type = \"sin\") #>  [1] 0.7818315 0.8045978 0.8262388 0.8467242 0.8660254 0.8841154 0.9009689 #>  [8] 0.9165623 0.9308737 0.9438833 0.9555728 0.9659258 0.9749279 0.9825665 #> [15] 0.9888308 0.9937122 0.9972038 0.9993007 1.0000000 0.9993007 0.9972038 #> [22] 0.9937122 0.9888308 0.9825665 0.9749279 0.9659258 0.9555728 0.9438833 #> [29] 0.9308737 0.9165623 0.9009689 0.8841154 0.8660254 0.8467242 0.8262388 #> [36] 0.8045978 0.7818315 0.7579717 0.7330519 0.7071068 0.6801727 0.6522874 #> [43] 0.6234898 0.5938202 0.5633201 0.5320321 0.5000000 0.4672686 0.4338837 #> [50] 0.3998920 #>  [ reached 'max' / getOption(\"max.print\") -- omitted 671 entries ]  # --- MUTATE ---  tibble(date = date_sequence) %>%     # Add cosine series that oscilates at a 7-day period     mutate(         C1_7 = fourier_vec(date, period = 7*24, K = 1, type = \"cos\"),         C2_7 = fourier_vec(date, period = 7*24, K = 2, type = \"cos\")     ) %>%     # Visualize     tidyr::pivot_longer(cols = contains(\"_\"), names_to = \"name\", values_to = \"value\") %>%     plot_time_series(         date, value, .color_var = name,         .smooth = FALSE,         .interactive = FALSE,         .title = \"7-Day Fourier Terms\"     )   options(max.print = options_old)"},{"path":"https://business-science.github.io/timetk/reference/future_frame.html","id":null,"dir":"Reference","previous_headings":"","what":"Make future time series from existing — future_frame","title":"Make future time series from existing — future_frame","text":"Make future time series existing","code":""},{"path":"https://business-science.github.io/timetk/reference/future_frame.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make future time series from existing — future_frame","text":"","code":"future_frame(   .data,   .date_var,   .length_out,   .inspect_weekdays = FALSE,   .inspect_months = FALSE,   .skip_values = NULL,   .insert_values = NULL,   .bind_data = FALSE )"},{"path":"https://business-science.github.io/timetk/reference/future_frame.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make future time series from existing — future_frame","text":".data data.frame tibble .date_var date date-time variable. .length_out Number future observations. Can numeric number phrase like \"1 year\". .inspect_weekdays Uses logistic regression algorithm inspect whether certain weekdays (e.g. weekends) excluded future dates. Default FALSE. .inspect_months Uses logistic regression algorithm inspect whether certain days months (e.g. last two weeks year seasonal days) excluded future dates. Default FALSE. .skip_values vector class idx timeseries values skip. .insert_values vector class idx timeseries values insert. .bind_data Whether perform row-wise bind .data future data. Default: FALSE","code":""},{"path":"https://business-science.github.io/timetk/reference/future_frame.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make future time series from existing — future_frame","text":"tibble extended future date, date-time timestamps.","code":""},{"path":"https://business-science.github.io/timetk/reference/future_frame.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make future time series from existing — future_frame","text":"wrapper tk_make_future_timeseries() works data.frames. respects dplyr groups. Specifying Length Future Observations argument .length_out determines many future index observations compute. can specified : numeric value - number future observations return. number observations returned always equal value user inputs. end date can vary based number timestamps chosen. time-based phrase - duration future include (e.g. \"6 months\" \"30 minutes\"). duration defines end date observations. end date change timestamps fall within end date returned (e.g. quarterly time series return 4 quarters .length_out = \"1 year\"). number observations vary fit within end date. Weekday Month Inspection .inspect_weekdays .inspect_months arguments apply \"daily\" (scale = \"day\") data (refer tk_get_timeseries_summary() get index scale). .inspect_weekdays argument useful determining missing days week occur weekly frequency every week, every week, . recommended least 60 days use option. .inspect_months argument useful determining missing days month, quarter year; however, algorithm can inadvertently select incorrect dates pattern erratic. Skipping / Inserting Values .skip_values .insert_values arguments can used remove add values series future times. values must format idx class. .skip_values argument useful passing holidays special index values excluded future time series. .insert_values argument useful adding values back algorithm may excluded. Binding Data Rowwise binding original common added argument .bind_data perform row-wise bind future data incoming data. replaces need :   Now can just :","code":"df %>%    future_frame(.length_out = \"6 months\") %>%    bind_rows(df, .) df %>%     future_frame(.length_out = \"6 months\", .bind_data = TRUE)"},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/future_frame.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make future time series from existing — future_frame","text":"","code":"# \\donttest{ library(dplyr)  # 30-min interval data taylor_30_min %>%     future_frame(date, .length_out = \"1 week\") #> # A tibble: 336 × 1 #>    date                #>    <dttm>              #>  1 2000-08-28 00:00:00 #>  2 2000-08-28 00:30:00 #>  3 2000-08-28 01:00:00 #>  4 2000-08-28 01:30:00 #>  5 2000-08-28 02:00:00 #>  6 2000-08-28 02:30:00 #>  7 2000-08-28 03:00:00 #>  8 2000-08-28 03:30:00 #>  9 2000-08-28 04:00:00 #> 10 2000-08-28 04:30:00 #> # ℹ 326 more rows  # Daily Data (Grouped) m4_daily %>%     group_by(id) %>%     future_frame(date, .length_out = \"6 weeks\") #> # A tibble: 168 × 2 #> # Groups:   id [4] #>    id    date       #>    <fct> <date>     #>  1 D10   2016-05-07 #>  2 D10   2016-05-08 #>  3 D10   2016-05-09 #>  4 D10   2016-05-10 #>  5 D10   2016-05-11 #>  6 D10   2016-05-12 #>  7 D10   2016-05-13 #>  8 D10   2016-05-14 #>  9 D10   2016-05-15 #> 10 D10   2016-05-16 #> # ℹ 158 more rows  # Specify how many observations to project into the future m4_daily %>%     group_by(id) %>%     future_frame(date, .length_out = 100) #> # A tibble: 400 × 2 #> # Groups:   id [4] #>    id    date       #>    <fct> <date>     #>  1 D10   2016-05-07 #>  2 D10   2016-05-08 #>  3 D10   2016-05-09 #>  4 D10   2016-05-10 #>  5 D10   2016-05-11 #>  6 D10   2016-05-12 #>  7 D10   2016-05-13 #>  8 D10   2016-05-14 #>  9 D10   2016-05-15 #> 10 D10   2016-05-16 #> # ℹ 390 more rows  # Bind with Original Data m4_daily %>%     group_by(id) %>%     future_frame(date, .length_out = 100, .bind_data = TRUE) #> # A tibble: 10,143 × 3 #> # Groups:   id [4] #>    id    date       value #>    <fct> <date>     <dbl> #>  1 D10   2014-07-03 2076. #>  2 D10   2014-07-04 2073. #>  3 D10   2014-07-05 2049. #>  4 D10   2014-07-06 2049. #>  5 D10   2014-07-07 2006. #>  6 D10   2014-07-08 2018. #>  7 D10   2014-07-09 2019. #>  8 D10   2014-07-10 2007. #>  9 D10   2014-07-11 2010  #> 10 D10   2014-07-12 2002. #> # ℹ 10,133 more rows  holidays <- tk_make_holiday_sequence(     start_date = \"2017-01-01\",     end_date   = \"2017-12-31\",     calendar   = \"NYSE\")  weekends <- tk_make_weekend_sequence(     start_date = \"2017-01-01\",     end_date   = \"2017-12-31\" )  FANG %>%     group_by(symbol) %>%     future_frame(         .length_out       = \"1 year\",         .skip_values      = c(holidays, weekends)     ) #> .date_var is missing. Using: date #> # A tibble: 1,008 × 2 #> # Groups:   symbol [4] #>    symbol date       #>    <chr>  <date>     #>  1 FB     2016-12-31 #>  2 FB     2017-01-03 #>  3 FB     2017-01-04 #>  4 FB     2017-01-05 #>  5 FB     2017-01-06 #>  6 FB     2017-01-09 #>  7 FB     2017-01-10 #>  8 FB     2017-01-11 #>  9 FB     2017-01-12 #> 10 FB     2017-01-13 #> # ℹ 998 more rows # }"},{"path":"https://business-science.github.io/timetk/reference/is_date_class.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if an object is a date class — is_date_class","title":"Check if an object is a date class — is_date_class","text":"Check object date class","code":""},{"path":"https://business-science.github.io/timetk/reference/is_date_class.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if an object is a date class — is_date_class","text":"","code":"is_date_class(x)"},{"path":"https://business-science.github.io/timetk/reference/is_date_class.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if an object is a date class — is_date_class","text":"x vector check","code":""},{"path":"https://business-science.github.io/timetk/reference/is_date_class.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if an object is a date class — is_date_class","text":"Logical (TRUE/FALSE)","code":""},{"path":"https://business-science.github.io/timetk/reference/is_date_class.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if an object is a date class — is_date_class","text":"","code":"library(dplyr)  tk_make_timeseries(\"2011\") %>% is_date_class() #> Using by: day #> [1] TRUE  letters %>% is_date_class() #> [1] FALSE"},{"path":"https://business-science.github.io/timetk/reference/lag_vec.html","id":null,"dir":"Reference","previous_headings":"","what":"Lag Transformation — lag_vec","title":"Lag Transformation — lag_vec","text":"lag_vec() applies Lag Transformation.","code":""},{"path":"https://business-science.github.io/timetk/reference/lag_vec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Lag Transformation — lag_vec","text":"","code":"lag_vec(x, lag = 1)  lead_vec(x, lag = -1)"},{"path":"https://business-science.github.io/timetk/reference/lag_vec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Lag Transformation — lag_vec","text":"x vector lagged. lag lag (far back) included differencing calculation. Negative lags leads.","code":""},{"path":"https://business-science.github.io/timetk/reference/lag_vec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Lag Transformation — lag_vec","text":"numeric vector","code":""},{"path":"https://business-science.github.io/timetk/reference/lag_vec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Lag Transformation — lag_vec","text":"Benefits: function NA padded default works well dplyr::mutate() operations. function allows lags leads (negative lags). Lag Calculation lag offset lag periods. NA values returned number lag periods. Lead Calculation negative lag considered lead. difference lead_vec() lag_vec() lead_vec() function contains starting negative value.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/lag_vec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Lag Transformation — lag_vec","text":"","code":"library(dplyr)  # --- VECTOR ----  # Lag 1:10 %>% lag_vec(lag = 1) #>  [1] NA  1  2  3  4  5  6  7  8  9  # Lead 1:10 %>% lag_vec(lag = -1) #>  [1]  2  3  4  5  6  7  8  9 10 NA   # --- MUTATE ----  m4_daily %>%     group_by(id) %>%     mutate(lag_1 = lag_vec(value, lag = 1)) #> # A tibble: 9,743 × 4 #> # Groups:   id [4] #>    id    date       value lag_1 #>    <fct> <date>     <dbl> <dbl> #>  1 D10   2014-07-03 2076.   NA  #>  2 D10   2014-07-04 2073. 2076. #>  3 D10   2014-07-05 2049. 2073. #>  4 D10   2014-07-06 2049. 2049. #>  5 D10   2014-07-07 2006. 2049. #>  6 D10   2014-07-08 2018. 2006. #>  7 D10   2014-07-09 2019. 2018. #>  8 D10   2014-07-10 2007. 2019. #>  9 D10   2014-07-11 2010  2007. #> 10 D10   2014-07-12 2002. 2010  #> # ℹ 9,733 more rows"},{"path":"https://business-science.github.io/timetk/reference/log_interval_vec.html","id":null,"dir":"Reference","previous_headings":"","what":"Log-Interval Transformation for Constrained Interval Forecasting — log_interval_vec","title":"Log-Interval Transformation for Constrained Interval Forecasting — log_interval_vec","text":"log_interval_vec() transformation constrains forecast interval specified upper_limit lower_limit. transformation provides similar benefits log() transformation, ensuring inverted transformation stays within upper lower limit.","code":""},{"path":"https://business-science.github.io/timetk/reference/log_interval_vec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log-Interval Transformation for Constrained Interval Forecasting — log_interval_vec","text":"","code":"log_interval_vec(   x,   limit_lower = \"auto\",   limit_upper = \"auto\",   offset = 0,   silent = FALSE )  log_interval_inv_vec(x, limit_lower, limit_upper, offset = 0)"},{"path":"https://business-science.github.io/timetk/reference/log_interval_vec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log-Interval Transformation for Constrained Interval Forecasting — log_interval_vec","text":"x positive numeric vector. limit_lower lower limit. Must less minimum value. set \"auto\", selects zero. limit_upper upper limit. Must greater maximum value. set \"auto\",  selects value 10% greater maximum value. offset offset include log transformation. Useful data contains values less equal zero. silent Whether report parameter selections message.","code":""},{"path":"https://business-science.github.io/timetk/reference/log_interval_vec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Log-Interval Transformation for Constrained Interval Forecasting — log_interval_vec","text":"numeric vector transformed series.","code":""},{"path":"https://business-science.github.io/timetk/reference/log_interval_vec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Log-Interval Transformation for Constrained Interval Forecasting — log_interval_vec","text":"Log Interval Transformation Log Interval Transformation constrains values specified upper lower limits. transformation maps limits function: log(((x + offset) - )/(b - (x + offset))) lower limit b upper limit Inverse Transformation inverse transformation: (b-)*(exp(x)) / (1 + exp(x)) + - offset","code":""},{"path":"https://business-science.github.io/timetk/reference/log_interval_vec.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Log-Interval Transformation for Constrained Interval Forecasting — log_interval_vec","text":"Forecasting: Principles & Practices: Forecasts constrained interval","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/log_interval_vec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Log-Interval Transformation for Constrained Interval Forecasting — log_interval_vec","text":"","code":"library(dplyr)  values_trans <- log_interval_vec(1:10, limit_lower = 0, limit_upper = 11) #> log_interval_vec():  #>  Using limit_lower: 0 #>  Using limit_upper: 11 #>  Using offset: 0 values_trans #>  [1] -2.3025851 -1.5040774 -0.9808293 -0.5596158 -0.1823216  0.1823216 #>  [7]  0.5596158  0.9808293  1.5040774  2.3025851  values_trans_forecast <- c(values_trans, 3.4, 4.4, 5.4)  values_trans_forecast %>%     log_interval_inv_vec(limit_lower = 0, limit_upper = 11) %>%     plot()"},{"path":"https://business-science.github.io/timetk/reference/m4_daily.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample of 4 Daily Time Series Datasets from the M4 Competition — m4_daily","title":"Sample of 4 Daily Time Series Datasets from the M4 Competition — m4_daily","text":"fourth M Competition. M4, started 1 January 2018 ended 31 May 2018. competition included 100,000 time series datasets. dataset includes sample 4 daily time series competition.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_daily.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample of 4 Daily Time Series Datasets from the M4 Competition — m4_daily","text":"","code":"m4_daily"},{"path":"https://business-science.github.io/timetk/reference/m4_daily.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sample of 4 Daily Time Series Datasets from the M4 Competition — m4_daily","text":"tibble: 9,743 x 3 id Factor. Unique series identifier (4 total) date Date. Timestamp information. Daily format. value Numeric. Value corresponding timestamp.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_daily.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Sample of 4 Daily Time Series Datasets from the M4 Competition — m4_daily","text":"M4 Competition Website","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_daily.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sample of 4 Daily Time Series Datasets from the M4 Competition — m4_daily","text":"sample 4 daily data sets M4 competition.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_daily.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample of 4 Daily Time Series Datasets from the M4 Competition — m4_daily","text":"","code":"m4_daily #> # A tibble: 9,743 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 D10   2014-07-03 2076. #>  2 D10   2014-07-04 2073. #>  3 D10   2014-07-05 2049. #>  4 D10   2014-07-06 2049. #>  5 D10   2014-07-07 2006. #>  6 D10   2014-07-08 2018. #>  7 D10   2014-07-09 2019. #>  8 D10   2014-07-10 2007. #>  9 D10   2014-07-11 2010  #> 10 D10   2014-07-12 2002. #> # ℹ 9,733 more rows"},{"path":"https://business-science.github.io/timetk/reference/m4_hourly.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample of 4 Hourly Time Series Datasets from the M4 Competition — m4_hourly","title":"Sample of 4 Hourly Time Series Datasets from the M4 Competition — m4_hourly","text":"fourth M Competition. M4, started 1 January 2018 ended 31 May 2018. competition included 100,000 time series datasets. dataset includes sample 4 hourly time series competition.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_hourly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample of 4 Hourly Time Series Datasets from the M4 Competition — m4_hourly","text":"","code":"m4_hourly"},{"path":"https://business-science.github.io/timetk/reference/m4_hourly.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sample of 4 Hourly Time Series Datasets from the M4 Competition — m4_hourly","text":"tibble: 3,060 x 3 id Factor. Unique series identifier (4 total) date Date-time. Timestamp information. Hourly format. value Numeric. Value corresponding timestamp.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_hourly.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Sample of 4 Hourly Time Series Datasets from the M4 Competition — m4_hourly","text":"M4 Competition Website","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_hourly.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sample of 4 Hourly Time Series Datasets from the M4 Competition — m4_hourly","text":"sample 4 hourly data sets M4 competition.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_hourly.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample of 4 Hourly Time Series Datasets from the M4 Competition — m4_hourly","text":"","code":"m4_hourly #> # A tibble: 3,060 × 3 #>    id    date                value #>    <fct> <dttm>              <dbl> #>  1 H10   2015-07-01 12:00:00   513 #>  2 H10   2015-07-01 13:00:00   512 #>  3 H10   2015-07-01 14:00:00   506 #>  4 H10   2015-07-01 15:00:00   500 #>  5 H10   2015-07-01 16:00:00   490 #>  6 H10   2015-07-01 17:00:00   484 #>  7 H10   2015-07-01 18:00:00   467 #>  8 H10   2015-07-01 19:00:00   446 #>  9 H10   2015-07-01 20:00:00   434 #> 10 H10   2015-07-01 21:00:00   422 #> # ℹ 3,050 more rows"},{"path":"https://business-science.github.io/timetk/reference/m4_monthly.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample of 4 Monthly Time Series Datasets from the M4 Competition — m4_monthly","title":"Sample of 4 Monthly Time Series Datasets from the M4 Competition — m4_monthly","text":"fourth M Competition. M4, started 1 January 2018 ended 31 May 2018. competition included 100,000 time series datasets. dataset includes sample 4 monthly time series competition.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_monthly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample of 4 Monthly Time Series Datasets from the M4 Competition — m4_monthly","text":"","code":"m4_monthly"},{"path":"https://business-science.github.io/timetk/reference/m4_monthly.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sample of 4 Monthly Time Series Datasets from the M4 Competition — m4_monthly","text":"tibble: 9,743 x 3 id Factor. Unique series identifier (4 total) date Date. Timestamp information. Monthly format. value Numeric. Value corresponding timestamp.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_monthly.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Sample of 4 Monthly Time Series Datasets from the M4 Competition — m4_monthly","text":"M4 Competition Website","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_monthly.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sample of 4 Monthly Time Series Datasets from the M4 Competition — m4_monthly","text":"sample 4 Monthly data sets M4 competition.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_monthly.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample of 4 Monthly Time Series Datasets from the M4 Competition — m4_monthly","text":"","code":"m4_monthly #> # A tibble: 1,574 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M1    1976-06-01  8000 #>  2 M1    1976-07-01  8350 #>  3 M1    1976-08-01  8570 #>  4 M1    1976-09-01  7700 #>  5 M1    1976-10-01  7080 #>  6 M1    1976-11-01  6520 #>  7 M1    1976-12-01  6070 #>  8 M1    1977-01-01  6650 #>  9 M1    1977-02-01  6830 #> 10 M1    1977-03-01  5710 #> # ℹ 1,564 more rows"},{"path":"https://business-science.github.io/timetk/reference/m4_quarterly.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample of 4 Quarterly Time Series Datasets from the M4 Competition — m4_quarterly","title":"Sample of 4 Quarterly Time Series Datasets from the M4 Competition — m4_quarterly","text":"fourth M Competition. M4, started 1 January 2018 ended 31 May 2018. competition included 100,000 time series datasets. dataset includes sample 4 quarterly time series competition.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_quarterly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample of 4 Quarterly Time Series Datasets from the M4 Competition — m4_quarterly","text":"","code":"m4_quarterly"},{"path":"https://business-science.github.io/timetk/reference/m4_quarterly.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sample of 4 Quarterly Time Series Datasets from the M4 Competition — m4_quarterly","text":"tibble: 9,743 x 3 id Factor. Unique series identifier (4 total) date Date. Timestamp information. Quarterly format. value Numeric. Value corresponding timestamp.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_quarterly.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Sample of 4 Quarterly Time Series Datasets from the M4 Competition — m4_quarterly","text":"M4 Competition Website","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_quarterly.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sample of 4 Quarterly Time Series Datasets from the M4 Competition — m4_quarterly","text":"sample 4 Quarterly data sets M4 competition.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_quarterly.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample of 4 Quarterly Time Series Datasets from the M4 Competition — m4_quarterly","text":"","code":"m4_quarterly #> # A tibble: 196 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 Q10   2000-01-01 2329  #>  2 Q10   2000-04-01 2350. #>  3 Q10   2000-07-01 2333. #>  4 Q10   2000-10-01 2382. #>  5 Q10   2001-01-01 2383. #>  6 Q10   2001-04-01 2405  #>  7 Q10   2001-07-01 2411  #>  8 Q10   2001-10-01 2428. #>  9 Q10   2002-01-01 2392. #> 10 Q10   2002-04-01 2418. #> # ℹ 186 more rows"},{"path":"https://business-science.github.io/timetk/reference/m4_weekly.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample of 4 Weekly Time Series Datasets from the M4 Competition — m4_weekly","title":"Sample of 4 Weekly Time Series Datasets from the M4 Competition — m4_weekly","text":"fourth M Competition. M4, started 1 January 2018 ended 31 May 2018. competition included 100,000 time series datasets. dataset includes sample 4 weekly time series competition.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_weekly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample of 4 Weekly Time Series Datasets from the M4 Competition — m4_weekly","text":"","code":"m4_weekly"},{"path":"https://business-science.github.io/timetk/reference/m4_weekly.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sample of 4 Weekly Time Series Datasets from the M4 Competition — m4_weekly","text":"tibble: 9,743 x 3 id Factor. Unique series identifier (4 total) date Date. Timestamp information. Weekly format. value Numeric. Value corresponding timestamp.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_weekly.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Sample of 4 Weekly Time Series Datasets from the M4 Competition — m4_weekly","text":"M4 Competition Website","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_weekly.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sample of 4 Weekly Time Series Datasets from the M4 Competition — m4_weekly","text":"sample 4 Weekly data sets M4 competition.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_weekly.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample of 4 Weekly Time Series Datasets from the M4 Competition — m4_weekly","text":"","code":"m4_weekly #> # A tibble: 2,295 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 W10   1999-01-01  247. #>  2 W10   1999-01-08  222. #>  3 W10   1999-01-15  450. #>  4 W10   1999-01-22  450. #>  5 W10   1999-01-29  450. #>  6 W10   1999-02-05  450. #>  7 W10   1999-02-12  450. #>  8 W10   1999-02-19  450. #>  9 W10   1999-02-26  450. #> 10 W10   1999-03-05  450. #> # ℹ 2,285 more rows"},{"path":"https://business-science.github.io/timetk/reference/m4_yearly.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample of 4 Yearly Time Series Datasets from the M4 Competition — m4_yearly","title":"Sample of 4 Yearly Time Series Datasets from the M4 Competition — m4_yearly","text":"fourth M Competition. M4, started 1 January 2018 ended 31 May 2018. competition included 100,000 time series datasets. dataset includes sample 4 yearly time series competition.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_yearly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample of 4 Yearly Time Series Datasets from the M4 Competition — m4_yearly","text":"","code":"m4_yearly"},{"path":"https://business-science.github.io/timetk/reference/m4_yearly.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sample of 4 Yearly Time Series Datasets from the M4 Competition — m4_yearly","text":"tibble: 9,743 x 3 id Factor. Unique series identifier (4 total) date Date. Timestamp information. Yearly format. value Numeric. Value corresponding timestamp.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_yearly.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Sample of 4 Yearly Time Series Datasets from the M4 Competition — m4_yearly","text":"M4 Competition Website","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_yearly.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sample of 4 Yearly Time Series Datasets from the M4 Competition — m4_yearly","text":"sample 4 Yearly data sets M4 competition.","code":""},{"path":"https://business-science.github.io/timetk/reference/m4_yearly.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample of 4 Yearly Time Series Datasets from the M4 Competition — m4_yearly","text":"","code":"m4_yearly #> # A tibble: 135 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 Y1    1979-01-01 5172. #>  2 Y1    1980-01-01 5134. #>  3 Y1    1981-01-01 5187. #>  4 Y1    1982-01-01 5085. #>  5 Y1    1983-01-01 5182  #>  6 Y1    1984-01-01 5414. #>  7 Y1    1985-01-01 5576. #>  8 Y1    1986-01-01 5753. #>  9 Y1    1987-01-01 5955. #> 10 Y1    1988-01-01 6088. #> # ℹ 125 more rows"},{"path":"https://business-science.github.io/timetk/reference/mutate_by_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Mutate (for Time Series Data) — mutate_by_time","title":"Mutate (for Time Series Data) — mutate_by_time","text":"mutate_by_time() time-based variant popular dplyr::mutate() function uses .date_var specify date date-time column .group calculation groups like \"5 seconds\", \"week\", \"3 months\".","code":""},{"path":"https://business-science.github.io/timetk/reference/mutate_by_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mutate (for Time Series Data) — mutate_by_time","text":"","code":"mutate_by_time(   .data,   .date_var,   .by = \"day\",   ...,   .type = c(\"floor\", \"ceiling\", \"round\") )"},{"path":"https://business-science.github.io/timetk/reference/mutate_by_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mutate (for Time Series Data) — mutate_by_time","text":".data tbl object data.frame .date_var column containing date date-time values summarize. missing, attempts auto-detect date column. .time unit summarise . Time units collapsed using lubridate::floor_date() lubridate::ceiling_date(). value can : second minute hour day week month bimonth quarter season halfyear year Arbitrary unique English abbreviations lubridate::period() constructor allowed. ... Name-value pairs. name gives name column output. value can : vector length 1, recycled correct length. vector length current group (whole data frame ungrouped). NULL, remove column. data frame tibble, create multiple columns output. .type One \"floor\", \"ceiling\", \"round. Defaults \"floor\". See lubridate::round_date.","code":""},{"path":"https://business-science.github.io/timetk/reference/mutate_by_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mutate (for Time Series Data) — mutate_by_time","text":"tibble data.frame","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/mutate_by_time.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Mutate (for Time Series Data) — mutate_by_time","text":"","code":"# Libraries library(dplyr)  # First value in each month m4_daily_first_by_month_tbl <- m4_daily %>%     group_by(id) %>%     mutate_by_time(         .date_var = date,         .by       = \"month\", # Setup for monthly aggregation         # mutate recycles a single value         first_value_by_month  = first(value)     ) m4_daily_first_by_month_tbl #> # A tibble: 9,743 × 4 #> # Groups:   id [4] #>    id    date       value first_value_by_month #>    <fct> <date>     <dbl>                <dbl> #>  1 D10   2014-07-03 2076.                2076. #>  2 D10   2014-07-04 2073.                2076. #>  3 D10   2014-07-05 2049.                2076. #>  4 D10   2014-07-06 2049.                2076. #>  5 D10   2014-07-07 2006.                2076. #>  6 D10   2014-07-08 2018.                2076. #>  7 D10   2014-07-09 2019.                2076. #>  8 D10   2014-07-10 2007.                2076. #>  9 D10   2014-07-11 2010                 2076. #> 10 D10   2014-07-12 2002.                2076. #> # ℹ 9,733 more rows  # Visualize Time Series vs 1st Value Each Month m4_daily_first_by_month_tbl %>%     tidyr::pivot_longer(value:first_value_by_month) %>%     plot_time_series(date, value, name,                      .facet_scale = \"free\", .facet_ncol = 2,                      .smooth = FALSE, .interactive = FALSE)"},{"path":"https://business-science.github.io/timetk/reference/normalize_vec.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize to Range (0, 1) — normalize_vec","title":"Normalize to Range (0, 1) — normalize_vec","text":"Normalization commonly used center scale numeric features prevent one dominating algorithms require data scale.","code":""},{"path":"https://business-science.github.io/timetk/reference/normalize_vec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize to Range (0, 1) — normalize_vec","text":"","code":"normalize_vec(x, min = NULL, max = NULL, silent = FALSE)  normalize_inv_vec(x, min, max)"},{"path":"https://business-science.github.io/timetk/reference/normalize_vec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalize to Range (0, 1) — normalize_vec","text":"x numeric vector. min population min value normalization process. max population max value normalization process. silent Whether report automated min max parameters message.","code":""},{"path":"https://business-science.github.io/timetk/reference/normalize_vec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Normalize to Range (0, 1) — normalize_vec","text":"numeric vector transformation applied.","code":""},{"path":"https://business-science.github.io/timetk/reference/normalize_vec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Normalize to Range (0, 1) — normalize_vec","text":"Standardization vs Normalization Standardization refers transformation reduces range mean 0, standard deviation 1 Normalization refers transformation reduces min-max range: (0, 1)","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/normalize_vec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Normalize to Range (0, 1) — normalize_vec","text":"","code":"library(dplyr)  d10_daily <- m4_daily %>% dplyr::filter(id == \"D10\")  # --- VECTOR ----  value_norm <- normalize_vec(d10_daily$value) #> Normalization Parameters #> min: 1781.6 #> max: 2649.3 value      <- normalize_inv_vec(value_norm,                                 min = 1781.6,                                 max = 2649.3)  # --- MUTATE ----  m4_daily %>%     group_by(id) %>%     mutate(value_norm = normalize_vec(value)) #> Normalization Parameters #> min: 1781.6 #> max: 2649.3 #> Normalization Parameters #> min: 1734.9 #> max: 19432.5 #> Normalization Parameters #> min: 6309.38 #> max: 9540.62 #> Normalization Parameters #> min: 4172.1 #> max: 14954.1 #> # A tibble: 9,743 × 4 #> # Groups:   id [4] #>    id    date       value value_norm #>    <fct> <date>     <dbl>      <dbl> #>  1 D10   2014-07-03 2076.      0.340 #>  2 D10   2014-07-04 2073.      0.336 #>  3 D10   2014-07-05 2049.      0.308 #>  4 D10   2014-07-06 2049.      0.308 #>  5 D10   2014-07-07 2006.      0.259 #>  6 D10   2014-07-08 2018.      0.272 #>  7 D10   2014-07-09 2019.      0.274 #>  8 D10   2014-07-10 2007.      0.260 #>  9 D10   2014-07-11 2010       0.263 #> 10 D10   2014-07-12 2002.      0.253 #> # ℹ 9,733 more rows"},{"path":"https://business-science.github.io/timetk/reference/pad_by_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Insert time series rows with regularly spaced timestamps — pad_by_time","title":"Insert time series rows with regularly spaced timestamps — pad_by_time","text":"easiest way fill missing timestamps convert granular period (e.g. quarter month). Wraps padr::pad() function padding tibbles.","code":""},{"path":"https://business-science.github.io/timetk/reference/pad_by_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Insert time series rows with regularly spaced timestamps — pad_by_time","text":"","code":"pad_by_time(   .data,   .date_var,   .by = \"auto\",   .pad_value = NA,   .fill_na_direction = c(\"none\", \"down\", \"up\", \"downup\", \"updown\"),   .start_date = NULL,   .end_date = NULL )"},{"path":"https://business-science.github.io/timetk/reference/pad_by_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Insert time series rows with regularly spaced timestamps — pad_by_time","text":".data tibble time-based column. .date_var column containing date date-time values pad .Either \"auto\", time-based frequency like \"year\", \"month\", \"day\", \"hour\", etc, time expression like \"5 min\", \"7 days\". See Details. .pad_value Fills padded values. Default NA. .fill_na_direction Users can provide NA fill strategy using tidyr::fill(). Possible values: 'none', '', '', 'downup', 'updown'. Default: 'none' .start_date Specifies start padded series. NULL use lowest value input variable. .end_date Specifies end padded series. NULL use highest value input variable.","code":""},{"path":"https://business-science.github.io/timetk/reference/pad_by_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Insert time series rows with regularly spaced timestamps — pad_by_time","text":"tibble data.frame rows containing missing timestamps added.","code":""},{"path":"https://business-science.github.io/timetk/reference/pad_by_time.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Insert time series rows with regularly spaced timestamps — pad_by_time","text":"Padding Missing Observations common use case pad_by_time() add rows timestamps missing. sales data missing values weekends holidays. high frequency data observations irregularly spaced reset regular frequency. Going Low High Frequency second use case going low frequency (e.g. day) high frequency (e.g. hour). possible supplying higher frequency pad_by_time(). Interval, .Padding can applied following ways: .= \"auto\" - pad_by_time() detect time-stamp frequency apply padding. eight intervals : year, quarter, month, week, day, hour, min, sec. Intervals like 5 minutes, 6 hours, 10 days possible. Pad Value, .pad_value pad value can supplied fills missing numeric data. Note applied numeric columns. Fill NA Direction, .fill_na_directions Uses tidyr::fill() fill missing observations using fill strategy.","code":""},{"path":"https://business-science.github.io/timetk/reference/pad_by_time.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Insert time series rows with regularly spaced timestamps — pad_by_time","text":"function wraps padr::pad() function developed Edwin Thoen.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/pad_by_time.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Insert time series rows with regularly spaced timestamps — pad_by_time","text":"","code":"library(dplyr)  # Create a quarterly series with 1 missing value missing_data_tbl <- tibble::tibble(     date = tk_make_timeseries(\"2014-01-01\", \"2015-01-01\", by = \"quarter\"),     value = 1:5 ) %>%     slice(-4) # Lose the 4th quarter on purpose missing_data_tbl #> # A tibble: 4 × 2 #>   date       value #>   <date>     <int> #> 1 2014-01-01     1 #> 2 2014-04-01     2 #> 3 2014-07-01     3 #> 4 2015-01-01     5   # Detects missing quarter, and pads the missing regularly spaced quarter with NA missing_data_tbl %>% pad_by_time(date, .by = \"quarter\") #> # A tibble: 5 × 2 #>   date       value #>   <date>     <int> #> 1 2014-01-01     1 #> 2 2014-04-01     2 #> 3 2014-07-01     3 #> 4 2014-10-01    NA #> 5 2015-01-01     5  # Can specify a shorter period. This fills monthly. missing_data_tbl %>% pad_by_time(date, .by = \"month\") #> # A tibble: 13 × 2 #>    date       value #>    <date>     <int> #>  1 2014-01-01     1 #>  2 2014-02-01    NA #>  3 2014-03-01    NA #>  4 2014-04-01     2 #>  5 2014-05-01    NA #>  6 2014-06-01    NA #>  7 2014-07-01     3 #>  8 2014-08-01    NA #>  9 2014-09-01    NA #> 10 2014-10-01    NA #> 11 2014-11-01    NA #> 12 2014-12-01    NA #> 13 2015-01-01     5  # Can let pad_by_time() auto-detect date and period missing_data_tbl %>% pad_by_time() #> .date_var is missing. Using: date #> pad applied on the interval: quarter #> # A tibble: 5 × 2 #>   date       value #>   <date>     <int> #> 1 2014-01-01     1 #> 2 2014-04-01     2 #> 3 2014-07-01     3 #> 4 2014-10-01    NA #> 5 2015-01-01     5  # Can specify a .pad_value missing_data_tbl %>% pad_by_time(date, .by = \"quarter\", .pad_value = 0) #> # A tibble: 5 × 2 #>   date       value #>   <date>     <int> #> 1 2014-01-01     1 #> 2 2014-04-01     2 #> 3 2014-07-01     3 #> 4 2014-10-01     0 #> 5 2015-01-01     5  # Can then impute missing values missing_data_tbl %>%     pad_by_time(date, .by = \"quarter\") %>%     mutate(value = ts_impute_vec(value, period = 1)) #> # A tibble: 5 × 2 #>   date       value #>   <date>     <dbl> #> 1 2014-01-01     1 #> 2 2014-04-01     2 #> 3 2014-07-01     3 #> 4 2014-10-01     4 #> 5 2015-01-01     5  # Can specify a custom .start_date and .end_date missing_data_tbl %>%    pad_by_time(date, .by = \"quarter\", .start_date = \"2013\", .end_date = \"2015-07-01\") #> # A tibble: 11 × 2 #>    date       value #>    <date>     <int> #>  1 2013-01-01    NA #>  2 2013-04-01    NA #>  3 2013-07-01    NA #>  4 2013-10-01    NA #>  5 2014-01-01     1 #>  6 2014-04-01     2 #>  7 2014-07-01     3 #>  8 2014-10-01    NA #>  9 2015-01-01     5 #> 10 2015-04-01    NA #> 11 2015-07-01    NA  # Can specify a tidyr::fill() direction missing_data_tbl %>%    pad_by_time(date, .by = \"quarter\",                .fill_na_direction = \"downup\",                .start_date = \"2013\", .end_date = \"2015-07-01\") #> # A tibble: 11 × 2 #>    date       value #>    <date>     <int> #>  1 2013-01-01     1 #>  2 2013-04-01     1 #>  3 2013-07-01     1 #>  4 2013-10-01     1 #>  5 2014-01-01     1 #>  6 2014-04-01     2 #>  7 2014-07-01     3 #>  8 2014-10-01     3 #>  9 2015-01-01     5 #> 10 2015-04-01     5 #> 11 2015-07-01     5  # --- GROUPS ----  # Apply standard NA padding to groups FANG %>%     group_by(symbol) %>%     pad_by_time(.by = \"day\") #> .date_var is missing. Using: date #> # A tibble: 5,836 × 8 #> # Groups:   symbol [4] #>    symbol date        open  high   low close  volume adjusted #>    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>   <dbl>    <dbl> #>  1 AMZN   2013-01-02  256.  258.  253.  257. 3271000     257. #>  2 AMZN   2013-01-03  257.  261.  256.  258. 2750900     258. #>  3 AMZN   2013-01-04  258.  260.  257.  259. 1874200     259. #>  4 AMZN   2013-01-05   NA    NA    NA    NA       NA      NA  #>  5 AMZN   2013-01-06   NA    NA    NA    NA       NA      NA  #>  6 AMZN   2013-01-07  263.  270.  263.  268. 4910000     268. #>  7 AMZN   2013-01-08  267.  269.  264.  266. 3010700     266. #>  8 AMZN   2013-01-09  268.  270.  265.  266. 2265600     266. #>  9 AMZN   2013-01-10  269.  269.  262.  265. 2863400     265. #> 10 AMZN   2013-01-11  265.  268.  264.  268. 2413300     268. #> # ℹ 5,826 more rows  # Apply constant pad value FANG %>%     group_by(symbol) %>%     pad_by_time(.by = \"day\", .pad_value = 0) #> .date_var is missing. Using: date #> # A tibble: 5,836 × 8 #> # Groups:   symbol [4] #>    symbol date        open  high   low close  volume adjusted #>    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>   <dbl>    <dbl> #>  1 AMZN   2013-01-02  256.  258.  253.  257. 3271000     257. #>  2 AMZN   2013-01-03  257.  261.  256.  258. 2750900     258. #>  3 AMZN   2013-01-04  258.  260.  257.  259. 1874200     259. #>  4 AMZN   2013-01-05    0     0     0     0        0       0  #>  5 AMZN   2013-01-06    0     0     0     0        0       0  #>  6 AMZN   2013-01-07  263.  270.  263.  268. 4910000     268. #>  7 AMZN   2013-01-08  267.  269.  264.  266. 3010700     266. #>  8 AMZN   2013-01-09  268.  270.  265.  266. 2265600     266. #>  9 AMZN   2013-01-10  269.  269.  262.  265. 2863400     265. #> 10 AMZN   2013-01-11  265.  268.  264.  268. 2413300     268. #> # ℹ 5,826 more rows  # Apply filled padding to groups FANG %>%     group_by(symbol) %>%     pad_by_time(.by = \"day\", .fill_na_direction = \"down\") #> .date_var is missing. Using: date #> # A tibble: 5,836 × 8 #> # Groups:   symbol [4] #>    symbol date        open  high   low close  volume adjusted #>    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>   <dbl>    <dbl> #>  1 AMZN   2013-01-02  256.  258.  253.  257. 3271000     257. #>  2 AMZN   2013-01-03  257.  261.  256.  258. 2750900     258. #>  3 AMZN   2013-01-04  258.  260.  257.  259. 1874200     259. #>  4 AMZN   2013-01-05  258.  260.  257.  259. 1874200     259. #>  5 AMZN   2013-01-06  258.  260.  257.  259. 1874200     259. #>  6 AMZN   2013-01-07  263.  270.  263.  268. 4910000     268. #>  7 AMZN   2013-01-08  267.  269.  264.  266. 3010700     266. #>  8 AMZN   2013-01-09  268.  270.  265.  266. 2265600     266. #>  9 AMZN   2013-01-10  269.  269.  262.  265. 2863400     265. #> 10 AMZN   2013-01-11  265.  268.  264.  268. 2413300     268. #> # ℹ 5,826 more rows"},{"path":"https://business-science.github.io/timetk/reference/parse_date2.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast, flexible date and datetime parsing — parse_date2","title":"Fast, flexible date and datetime parsing — parse_date2","text":"Significantly faster time series parsing readr::parse_date, readr::parse_datetime, lubridate::as_date(), lubridate::as_datetime(). Uses anytime package, relies Boost.Date_Time C++ library date/datetime parsing.","code":""},{"path":"https://business-science.github.io/timetk/reference/parse_date2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast, flexible date and datetime parsing — parse_date2","text":"","code":"parse_date2(x, ..., silent = FALSE)  parse_datetime2(x, tz = \"UTC\", tz_shift = FALSE, ..., silent = FALSE)"},{"path":"https://business-science.github.io/timetk/reference/parse_date2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast, flexible date and datetime parsing — parse_date2","text":"x character vector ... Additional parameters passed anytime::anytime() anytime::anydate() silent TRUE, warns user parsing failures. tz Datetime . timezone (see OlsenNames()). tz_shift Datetime . FALSE, forces datetime time zone. TRUE, offsets datetime UTC new time zone.","code":""},{"path":"https://business-science.github.io/timetk/reference/parse_date2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast, flexible date and datetime parsing — parse_date2","text":"Returns date datatime vector transformation applied character timestamp vector.","code":""},{"path":"https://business-science.github.io/timetk/reference/parse_date2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast, flexible date and datetime parsing — parse_date2","text":"Parsing Formats Date Formats: Must follow Year, Month, Day sequence. (e.g. parse_date2(\"2011 June\") OK, parse_date2(\"June 2011\") OK). Date Time Formats: Must follow YMD HMS sequence. Refer lubridate::mdy() Month, Day, Year additional formats. Time zones (Datetime) Time zones handled similar way lubridate::as_datetime() time zones forced rather shifted. key difference anytime::anytime(), shifts datetimes specified timezone default.","code":""},{"path":"https://business-science.github.io/timetk/reference/parse_date2.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fast, flexible date and datetime parsing — parse_date2","text":"function wraps anytime::anytime() anytime::anydate() functions developed Dirk Eddelbuettel.","code":""},{"path":"https://business-science.github.io/timetk/reference/parse_date2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast, flexible date and datetime parsing — parse_date2","text":"","code":"# Fast date parsing parse_date2(\"2011\") #> [1] \"2011-01-01\" parse_date2(\"2011 June 3rd\") #> [1] \"2011-06-03\"  # Fast datetime parsing parse_datetime2(\"2011\") #> [1] \"2011-01-01 UTC\" parse_datetime2(\"2011 Jan 1 12:35:21\") #> [1] \"2011-01-01 12:35:21 UTC\"  # Time Zones (datetime only) parse_datetime2(\"2011 Jan 1 12:35:21\", tz = \"Europe/London\") #> [1] \"2011-01-01 12:35:21 GMT\""},{"path":"https://business-science.github.io/timetk/reference/plot_acf_diagnostics.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize the ACF, PACF, and CCFs for One or More Time Series — plot_acf_diagnostics","title":"Visualize the ACF, PACF, and CCFs for One or More Time Series — plot_acf_diagnostics","text":"Returns ACF PACF target optionally CCF's one lagged predictors interactive plotly plots. Scales multiple time series group_by().","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_acf_diagnostics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize the ACF, PACF, and CCFs for One or More Time Series — plot_acf_diagnostics","text":"","code":"plot_acf_diagnostics(   .data,   .date_var,   .value,   .ccf_vars = NULL,   .lags = 1000,   .show_ccf_vars_only = FALSE,   .show_white_noise_bars = TRUE,   .facet_ncol = 1,   .facet_scales = \"fixed\",   .line_color = \"#2c3e50\",   .line_size = 0.5,   .line_alpha = 1,   .point_color = \"#2c3e50\",   .point_size = 1,   .point_alpha = 1,   .x_intercept = NULL,   .x_intercept_color = \"#E31A1C\",   .hline_color = \"#2c3e50\",   .white_noise_line_type = 2,   .white_noise_line_color = \"#A6CEE3\",   .title = \"Lag Diagnostics\",   .x_lab = \"Lag\",   .y_lab = \"Correlation\",   .interactive = TRUE,   .plotly_slider = FALSE )"},{"path":"https://business-science.github.io/timetk/reference/plot_acf_diagnostics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize the ACF, PACF, and CCFs for One or More Time Series — plot_acf_diagnostics","text":".data data frame tibble numeric features (values) descending chronological order .date_var column containing either date date-time values .value numeric column value ACF PACF calculations performed. .ccf_vars Additional features perform Lag Cross Correlations (CCFs) versus .value. Useful evaluating external lagged regressors. .lags sequence one lags evaluate. .show_ccf_vars_only Hides ACF PACF plots can focus CCFs. .show_white_noise_bars Shows white noise significance bounds. .facet_ncol Facets: Number facet columns. effect using grouped_df. .facet_scales Facets: Options include \"fixed\", \"free\", \"free_y\", \"free_x\" .line_color Line color. Use keyword: \"scale_color\" change color facet. .line_size Line size (linewidth) .line_alpha Line opacity. Adjust transparency line. Range: (0, 1) .point_color Point color. Use keyword: \"scale_color\" change color facet. .point_size Point size .point_alpha Opacity. Adjust transparency points. Range: (0, 1) .x_intercept Numeric lag. Adds vertical line. .x_intercept_color Color x-intercept line. .hline_color Color y-intercept = 0 line. .white_noise_line_type Line type white noise bars. Set 2 \"dashed\" default. .white_noise_line_color Line color white noise bars. Set tidyquant::palette_light() \"steel blue\" default. .title Title plot .x_lab X-axis label plot .y_lab Y-axis label plot .interactive Returns either static (ggplot2) visualization interactive (plotly) visualization .plotly_slider TRUE, returns plotly x-axis range slider.","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_acf_diagnostics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize the ACF, PACF, and CCFs for One or More Time Series — plot_acf_diagnostics","text":"static ggplot2 plot interactive plotly plot","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_acf_diagnostics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Visualize the ACF, PACF, and CCFs for One or More Time Series — plot_acf_diagnostics","text":"Simplified ACF, PACF, & CCF often interested 3 functions. get 3+ ? Now can. ACF - Autocorrelation target variable lagged versions PACF - Partial Autocorrelation removes dependence lags lags highlighting key seasonalities. CCF - Shows lagged predictors can used prediction target variable. Lag Specification Lags (.lags) can either specified : time-based phrase indicating duraction (e.g. 2 months) maximum lag (e.g. .lags = 28) sequence lags (e.g. .lags = 7:28) Scales Multiple Time Series Groups plot_acf_diagnostics() works grouped_df's, meaning can group time series one categorical columns dplyr::group_by() apply plot_acf_diagnostics() return group-wise lag diagnostics. Special Note Groups Unlike plotting utilities, .facet_vars arguments included. Use dplyr::group_by() processing multiple time series groups. Calculating White Noise Significance Bars formula significance bars +2/sqrt(T) -2/sqrt(T) T length time series. white noise time series, 95% data points fall within range. may significant autocorrelations.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/plot_acf_diagnostics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize the ACF, PACF, and CCFs for One or More Time Series — plot_acf_diagnostics","text":"","code":"library(dplyr) library(ggplot2)   # Apply Transformations # - Differencing transformation to identify ARIMA & SARIMA Orders m4_hourly %>%     group_by(id) %>%     plot_acf_diagnostics(         date, value,               # ACF & PACF         .lags = \"7 days\",          # 7-Days of hourly lags         .interactive = FALSE     )   # Apply Transformations # - Differencing transformation to identify ARIMA & SARIMA Orders m4_hourly %>%     group_by(id) %>%     plot_acf_diagnostics(         date,         diff_vec(value, lag = 1), # Difference the value column         .lags        = 0:(24*7),   # 7-Days of hourly lags         .interactive = FALSE     ) +     ggtitle(\"ACF Diagnostics\",  subtitle = \"1st Difference\") #> diff_vec(): Initial values: 513 #> diff_vec(): Initial values: 39325 #> diff_vec(): Initial values: 45 #> diff_vec(): Initial values: 153   # CCFs Too! walmart_sales_weekly %>%     select(id, Date, Weekly_Sales, Temperature, Fuel_Price) %>%     group_by(id) %>%     plot_acf_diagnostics(         Date, Weekly_Sales,                        # ACF & PACF         .ccf_vars    = c(Temperature, Fuel_Price), # CCFs         .lags        = \"3 months\", # 3 months of weekly lags         .interactive = FALSE     )"},{"path":"https://business-science.github.io/timetk/reference/plot_anomalies.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize Anomalies for One or More Time Series — plot_anomalies","title":"Visualize Anomalies for One or More Time Series — plot_anomalies","text":"plot_anomalies() interactive scalable function visualizing anomalies time series data. Plots available interactive plotly (default) static ggplot2 format. plot_anomalies_decomp(): Takes data anomalize() function, returns plot anomaly decomposition. Useful interpeting anomalize() function determining outliers \"remainder\". plot_anomalies_cleaned() helps users visualize /cleaning anomalies.","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_anomalies.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize Anomalies for One or More Time Series — plot_anomalies","text":"","code":"plot_anomalies(   .data,   .date_var,   .facet_vars = NULL,   .facet_ncol = 1,   .facet_nrow = 1,   .facet_scales = \"free\",   .facet_dir = \"h\",   .facet_collapse = FALSE,   .facet_collapse_sep = \" \",   .facet_strip_remove = FALSE,   .line_color = \"#2c3e50\",   .line_size = 0.5,   .line_type = 1,   .line_alpha = 1,   .anom_color = \"#e31a1c\",   .anom_alpha = 1,   .anom_size = 1.5,   .ribbon_fill = \"grey20\",   .ribbon_alpha = 0.2,   .legend_show = TRUE,   .title = \"Anomaly Plot\",   .x_lab = \"\",   .y_lab = \"\",   .color_lab = \"Anomaly\",   .interactive = TRUE,   .trelliscope = FALSE,   .trelliscope_params = list() )  plot_anomalies_decomp(   .data,   .date_var,   .facet_vars = NULL,   .facet_scales = \"free\",   .line_color = \"#2c3e50\",   .line_size = 0.5,   .line_type = 1,   .line_alpha = 1,   .title = \"Anomaly Decomposition Plot\",   .x_lab = \"\",   .y_lab = \"\",   .interactive = TRUE )  plot_anomalies_cleaned(   .data,   .date_var,   .facet_vars = NULL,   .facet_ncol = 1,   .facet_nrow = 1,   .facet_scales = \"free\",   .facet_dir = \"h\",   .facet_collapse = FALSE,   .facet_collapse_sep = \" \",   .facet_strip_remove = FALSE,   .line_color = \"#2c3e50\",   .line_size = 0.5,   .line_type = 1,   .line_alpha = 1,   .cleaned_line_color = \"#e31a1c\",   .cleaned_line_size = 0.5,   .cleaned_line_type = 1,   .cleaned_line_alpha = 1,   .legend_show = TRUE,   .title = \"Anomalies Cleaned Plot\",   .x_lab = \"\",   .y_lab = \"\",   .color_lab = \"Legend\",   .interactive = TRUE,   .trelliscope = FALSE,   .trelliscope_params = list() )"},{"path":"https://business-science.github.io/timetk/reference/plot_anomalies.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize Anomalies for One or More Time Series — plot_anomalies","text":".data tibble data.frame anomalized anomalize() .date_var column containing either date date-time values .facet_vars One grouping columns broken ggplot2 facets. can selected using tidyselect() helpers (e.g contains()). .facet_ncol Number facet columns. .facet_nrow Number facet rows (used .trelliscope = TRUE) .facet_scales Control facet x & y-axis ranges. Options include \"fixed\", \"free\", \"free_y\", \"free_x\" .facet_dir direction faceting (\"h\" horizontal, \"v\" vertical). Default \"h\". .facet_collapse Multiple facets included one facet strip instead multiple facet strips. .facet_collapse_sep separator used collapsing facets. .facet_strip_remove Whether remove strip text label facet. .line_color Line color. .line_size Line size. .line_type Line type. .line_alpha Line alpha (opacity). Range: (0, 1). .anom_color Color anomaly dots .anom_alpha Opacity anomaly dots. Range: (0, 1). .anom_size Size anomaly dots .ribbon_fill Fill color acceptable range .ribbon_alpha Fill opacity acceptable range. Range: (0, 1). .legend_show Toggles /Legend .title Plot title. .x_lab Plot x-axis label .y_lab Plot y-axis label .color_lab Plot label color legend .interactive TRUE, returns plotly interactive plot. FALSE, returns static ggplot2 plot. .trelliscope Returns either normal plot trelliscopejs plot (great many time series) Must trelliscopejs installed. .trelliscope_params Pass parameters trelliscopejs::facet_trelliscope() function list(). parameters passed : ncol: use .facet_ncol nrow: use .facet_nrow scales: use facet_scales as_plotly: use .interactive .cleaned_line_color Line color. .cleaned_line_size Line size. .cleaned_line_type Line type. .cleaned_line_alpha Line alpha (opacity). Range: (0, 1).","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_anomalies.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize Anomalies for One or More Time Series — plot_anomalies","text":"plotly ggplot2 visualization","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_anomalies.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize Anomalies for One or More Time Series — plot_anomalies","text":"","code":"# Plot Anomalies library(dplyr)  walmart_sales_weekly %>%     filter(id %in% c(\"1_1\", \"1_3\")) %>%     group_by(id) %>%     anomalize(Date, Weekly_Sales) %>%     plot_anomalies(Date, .facet_ncol = 2, .ribbon_alpha = 0.25, .interactive = FALSE) #> frequency = 13 observations per 1 quarter #> trend = 52 observations per 1 year #> frequency = 13 observations per 1 quarter #> trend = 52 observations per 1 year   # Plot Anomalies Decomposition library(dplyr)  walmart_sales_weekly %>%     filter(id %in% c(\"1_1\", \"1_3\")) %>%     group_by(id) %>%     anomalize(Date, Weekly_Sales, .message = FALSE) %>%     plot_anomalies_decomp(Date, .interactive = FALSE)   # Plot Anomalies Cleaned library(dplyr)  walmart_sales_weekly %>%     filter(id %in% c(\"1_1\", \"1_3\")) %>%     group_by(id) %>%     anomalize(Date, Weekly_Sales, .message = FALSE) %>%     plot_anomalies_cleaned(Date, .facet_ncol = 2, .interactive = FALSE)"},{"path":"https://business-science.github.io/timetk/reference/plot_anomaly_diagnostics.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize Anomalies for One or More Time Series — plot_anomaly_diagnostics","title":"Visualize Anomalies for One or More Time Series — plot_anomaly_diagnostics","text":"interactive scalable function visualizing anomalies time series data. Plots available interactive plotly (default) static ggplot2 format.","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_anomaly_diagnostics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize Anomalies for One or More Time Series — plot_anomaly_diagnostics","text":"","code":"plot_anomaly_diagnostics(   .data,   .date_var,   .value,   .facet_vars = NULL,   .frequency = \"auto\",   .trend = \"auto\",   .alpha = 0.05,   .max_anomalies = 0.2,   .message = TRUE,   .facet_ncol = 1,   .facet_nrow = 1,   .facet_scales = \"free\",   .facet_dir = \"h\",   .facet_collapse = FALSE,   .facet_collapse_sep = \" \",   .facet_strip_remove = FALSE,   .line_color = \"#2c3e50\",   .line_size = 0.5,   .line_type = 1,   .line_alpha = 1,   .anom_color = \"#e31a1c\",   .anom_alpha = 1,   .anom_size = 1.5,   .ribbon_fill = \"grey20\",   .ribbon_alpha = 0.2,   .legend_show = TRUE,   .title = \"Anomaly Diagnostics\",   .x_lab = \"\",   .y_lab = \"\",   .color_lab = \"Anomaly\",   .interactive = TRUE,   .trelliscope = FALSE,   .trelliscope_params = list() )"},{"path":"https://business-science.github.io/timetk/reference/plot_anomaly_diagnostics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize Anomalies for One or More Time Series — plot_anomaly_diagnostics","text":".data tibble data.frame time-based column .date_var column containing either date date-time values .value column containing numeric values .facet_vars One grouping columns broken ggplot2 facets. can selected using tidyselect() helpers (e.g contains()). .frequency Controls seasonal adjustment (removal seasonality). Input can either \"auto\", time-based definition (e.g. \"2 weeks\"), numeric number observations per frequency (e.g. 10). Refer tk_get_frequency(). .trend Controls trend component. STL, trend controls sensitivity LOESS smoother, used remove remainder. Refer tk_get_trend(). .alpha Controls width \"normal\" range. Lower values conservative higher values less prone incorrectly classifying \"normal\" observations. .max_anomalies maximum percent anomalies permitted identified. .message boolean. TRUE, output information related automatic frequency trend selection (applicable). .facet_ncol Number facet columns. .facet_nrow Number facet rows (used .trelliscope = TRUE) .facet_scales Control facet x & y-axis ranges. Options include \"fixed\", \"free\", \"free_y\", \"free_x\" .facet_dir direction faceting (\"h\" horizontal, \"v\" vertical). Default \"h\". .facet_collapse Multiple facets included one facet strip instead multiple facet strips. .facet_collapse_sep separator used collapsing facets. .facet_strip_remove Whether remove strip text label facet. .line_color Line color. .line_size Line size. .line_type Line type. .line_alpha Line alpha (opacity). Range: (0, 1). .anom_color Color anomaly dots .anom_alpha Opacity anomaly dots. Range: (0, 1). .anom_size Size anomaly dots .ribbon_fill Fill color acceptable range .ribbon_alpha Fill opacity acceptable range. Range: (0, 1). .legend_show Toggles /Legend .title Plot title. .x_lab Plot x-axis label .y_lab Plot y-axis label .color_lab Plot label color legend .interactive TRUE, returns plotly interactive plot. FALSE, returns static ggplot2 plot. .trelliscope Returns either normal plot trelliscopejs plot (great many time series) Must trelliscopejs installed. .trelliscope_params Pass parameters trelliscopejs::facet_trelliscope() function list(). parameters passed : ncol: use .facet_ncol nrow: use .facet_nrow scales: use facet_scales as_plotly: use .interactive","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_anomaly_diagnostics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize Anomalies for One or More Time Series — plot_anomaly_diagnostics","text":"plotly ggplot2 visualization","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_anomaly_diagnostics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Visualize Anomalies for One or More Time Series — plot_anomaly_diagnostics","text":"plot_anomaly_diagnostics() visualization wrapper tk_anomaly_diagnostics() group-wise anomaly detection, implements 2-step process detect outliers time series. Step 1: Detrend & Remove Seasonality using STL Decomposition decomposition separates \"season\" \"trend\" components \"observed\" values leaving \"remainder\" anomaly detection. user can control two parameters: frequency trend. .frequency: Adjusts \"season\" component removed \"observed\" values. .trend: Adjusts trend window (t.window parameter stats::stl() used. user may supply .frequency .trend time-based durations (e.g. \"6 weeks\") numeric values (e.g. 180) \"auto\", predetermines frequency /trend based scale time series using tk_time_scale_template(). Step 2: Anomaly Detection \"trend\" \"season\" (seasonality) removed, anomaly detection performed \"remainder\". Anomalies identified, boundaries (recomposed_l1 recomposed_l2) determined. Anomaly Detection Method uses inner quartile range (IQR) +/-25 median. IQR Adjustment, alpha parameter default alpha = 0.05, limits established expanding 25/75 baseline IQR Factor 3 (3X). IQR Factor = 0.15 / alpha (hence 3X alpha = 0.05): increase IQR Factor controlling limits, decrease alpha, makes difficult outlier. Increase alpha make easier outlier. IQR outlier detection method used forecast::tsoutliers(). similar outlier detection method used Twitter's AnomalyDetection package. Twitter Forecast tsoutliers methods implemented Business Science's anomalize package.","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_anomaly_diagnostics.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Visualize Anomalies for One or More Time Series — plot_anomaly_diagnostics","text":"CLEVELAND, R. B., CLEVELAND, W. S., MCRAE, J. E., TERPENNING, . STL: Seasonal-Trend Decomposition Procedure Based Loess. Journal Official Statistics, Vol. 6, . 1 (1990), pp. 3-73. Owen S. Vallis, Jordan Hochenbaum Arun Kejariwal (2014). Novel Technique Long-Term Anomaly Detection Cloud. Twitter Inc.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/plot_anomaly_diagnostics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize Anomalies for One or More Time Series — plot_anomaly_diagnostics","text":"","code":"library(dplyr)  walmart_sales_weekly %>%     group_by(id) %>%     plot_anomaly_diagnostics(Date, Weekly_Sales,                              .message = FALSE,                              .facet_ncol = 3,                              .ribbon_alpha = 0.25,                              .interactive = FALSE)"},{"path":"https://business-science.github.io/timetk/reference/plot_seasonal_diagnostics.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize Multiple Seasonality Features for One or More Time Series — plot_seasonal_diagnostics","title":"Visualize Multiple Seasonality Features for One or More Time Series — plot_seasonal_diagnostics","text":"interactive scalable function visualizing time series seasonality. Plots available interactive plotly (default) static ggplot2 format.","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_seasonal_diagnostics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize Multiple Seasonality Features for One or More Time Series — plot_seasonal_diagnostics","text":"","code":"plot_seasonal_diagnostics(   .data,   .date_var,   .value,   .facet_vars = NULL,   .feature_set = \"auto\",   .geom = c(\"boxplot\", \"violin\"),   .geom_color = \"#2c3e50\",   .geom_outlier_color = \"#2c3e50\",   .title = \"Seasonal Diagnostics\",   .x_lab = \"\",   .y_lab = \"\",   .interactive = TRUE )"},{"path":"https://business-science.github.io/timetk/reference/plot_seasonal_diagnostics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize Multiple Seasonality Features for One or More Time Series — plot_seasonal_diagnostics","text":".data tibble data.frame time-based column .date_var column containing either date date-time values .value column containing numeric values .facet_vars One grouping columns broken ggplot2 facets. can selected using tidyselect() helpers (e.g contains()). .feature_set One multiple selections analyze seasonality. Choices include: \"auto\" - Automatically selects features based time stamps length series. \"second\" - Good analyzing seasonality second minute. \"minute\" - Good analyzing seasonality minute hour \"hour\" - Good analyzing seasonality hour day \"wday.lbl\" - Labeled weekdays. Good analyzing seasonality day week. \"week\" - Good analyzing seasonality week year. \"month.lbl\" - Labeled months. Good analyzing seasonality month year. \"quarter\" - Good analyzing seasonality quarter year \"year\" - Good analyzing seasonality multiple years. .geom Either \"boxplot\" \"violin\" .geom_color Geometry color. Line color. Use keyword: \"scale_color\" change color facet. .geom_outlier_color Color used highlight outliers. .title Plot title. .x_lab Plot x-axis label .y_lab Plot y-axis label .interactive TRUE, returns plotly interactive plot. FALSE, returns static ggplot2 plot.","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_seasonal_diagnostics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize Multiple Seasonality Features for One or More Time Series — plot_seasonal_diagnostics","text":"plotly ggplot2 visualization","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_seasonal_diagnostics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Visualize Multiple Seasonality Features for One or More Time Series — plot_seasonal_diagnostics","text":"Automatic Feature Selection Internal calculations performed detect sub-range features include useing following logic: minimum feature selected based median difference consecutive timestamps maximum feature selected based 2 full periods. Example: Hourly timestamp data lasts 2 weeks following features: \"hour\", \"wday.lbl\", \"week\". Scalable Grouped Data Frames function respects grouped data.frame tibbles made dplyr::group_by(). grouped data, automatic feature selection returned collection features within sub-groups. means extra features returned even though may meaningless groups. Transformations .value parameter respects transformations (e.g. .value = log(sales)).","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_seasonal_diagnostics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize Multiple Seasonality Features for One or More Time Series — plot_seasonal_diagnostics","text":"","code":"# \\donttest{ library(dplyr)  # ---- MULTIPLE FREQUENCY ---- # Taylor 30-minute dataset from forecast package taylor_30_min #> # A tibble: 4,032 × 2 #>    date                value #>    <dttm>              <dbl> #>  1 2000-06-05 00:00:00 22262 #>  2 2000-06-05 00:30:00 21756 #>  3 2000-06-05 01:00:00 22247 #>  4 2000-06-05 01:30:00 22759 #>  5 2000-06-05 02:00:00 22549 #>  6 2000-06-05 02:30:00 22313 #>  7 2000-06-05 03:00:00 22128 #>  8 2000-06-05 03:30:00 21860 #>  9 2000-06-05 04:00:00 21751 #> 10 2000-06-05 04:30:00 21336 #> # ℹ 4,022 more rows  # Visualize series taylor_30_min %>%     plot_time_series(date, value, .interactive = FALSE)   # Visualize seasonality taylor_30_min %>%     plot_seasonal_diagnostics(date, value, .interactive = FALSE)   # ---- GROUPED EXAMPLES ---- # m4 hourly dataset m4_hourly #> # A tibble: 3,060 × 3 #>    id    date                value #>    <fct> <dttm>              <dbl> #>  1 H10   2015-07-01 12:00:00   513 #>  2 H10   2015-07-01 13:00:00   512 #>  3 H10   2015-07-01 14:00:00   506 #>  4 H10   2015-07-01 15:00:00   500 #>  5 H10   2015-07-01 16:00:00   490 #>  6 H10   2015-07-01 17:00:00   484 #>  7 H10   2015-07-01 18:00:00   467 #>  8 H10   2015-07-01 19:00:00   446 #>  9 H10   2015-07-01 20:00:00   434 #> 10 H10   2015-07-01 21:00:00   422 #> # ℹ 3,050 more rows  # Visualize series m4_hourly %>%     group_by(id) %>%     plot_time_series(date, value, .facet_scales = \"free\", .interactive = FALSE)   # Visualize seasonality m4_hourly %>%     group_by(id) %>%     plot_seasonal_diagnostics(date, value, .interactive = FALSE)   # }"},{"path":"https://business-science.github.io/timetk/reference/plot_stl_diagnostics.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize STL Decomposition Features for One or More Time Series — plot_stl_diagnostics","title":"Visualize STL Decomposition Features for One or More Time Series — plot_stl_diagnostics","text":"interactive scalable function visualizing time series STL Decomposition. Plots available interactive plotly (default) static ggplot2 format.","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_stl_diagnostics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize STL Decomposition Features for One or More Time Series — plot_stl_diagnostics","text":"","code":"plot_stl_diagnostics(   .data,   .date_var,   .value,   .facet_vars = NULL,   .feature_set = c(\"observed\", \"season\", \"trend\", \"remainder\", \"seasadj\"),   .frequency = \"auto\",   .trend = \"auto\",   .message = TRUE,   .facet_scales = \"free\",   .line_color = \"#2c3e50\",   .line_size = 0.5,   .line_type = 1,   .line_alpha = 1,   .title = \"STL Diagnostics\",   .x_lab = \"\",   .y_lab = \"\",   .interactive = TRUE )"},{"path":"https://business-science.github.io/timetk/reference/plot_stl_diagnostics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize STL Decomposition Features for One or More Time Series — plot_stl_diagnostics","text":".data tibble data.frame time-based column .date_var column containing either date date-time values .value column containing numeric values .facet_vars One grouping columns broken ggplot2 facets. can selected using tidyselect() helpers (e.g contains()). .feature_set STL decompositions visualize. Select one \"observed\", \"season\", \"trend\", \"remainder\", \"seasadj\". .frequency Controls seasonal adjustment (removal seasonality). Input can either \"auto\", time-based definition (e.g. \"2 weeks\"), numeric number observations per frequency (e.g. 10). Refer tk_get_frequency(). .trend Controls trend component. STL, trend controls sensitivity lowess smoother, used remove remainder. .message boolean. TRUE, output information related automatic frequency trend selection (applicable). .facet_scales Control facet x & y-axis ranges. Options include \"fixed\", \"free\", \"free_y\", \"free_x\" .line_color Line color. .line_size Line size. .line_type Line type. .line_alpha Line alpha (opacity). Range: (0, 1). .title Plot title. .x_lab Plot x-axis label .y_lab Plot y-axis label .interactive TRUE, returns plotly interactive plot. FALSE, returns static ggplot2 plot.","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_stl_diagnostics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize STL Decomposition Features for One or More Time Series — plot_stl_diagnostics","text":"plotly ggplot2 visualization","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_stl_diagnostics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Visualize STL Decomposition Features for One or More Time Series — plot_stl_diagnostics","text":"plot_stl_diagnostics() function generates Seasonal-Trend-Loess decomposition. function \"tidy\" sense works data frames designed work dplyr groups. STL method: STL method implements time series decomposition using underlying stats::stl(). decomposition separates \"season\" \"trend\" components \"observed\" values leaving \"remainder\". Frequency & Trend Selection user can control two parameters: .frequency .trend. .frequency parameter adjusts \"season\" component removed \"observed\" values. .trend parameter adjusts trend window (t.window parameter stl()) used. user may supply .frequency .trend time-based durations (e.g. \"6 weeks\") numeric values (e.g. 180) \"auto\", automatically selects frequency /trend based scale time series.","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_stl_diagnostics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize STL Decomposition Features for One or More Time Series — plot_stl_diagnostics","text":"","code":"library(dplyr)  # ---- SINGLE TIME SERIES DECOMPOSITION ---- m4_hourly %>%     filter(id == \"H10\") %>%     plot_stl_diagnostics(         date, value,         # Set features to return, desired frequency and trend         .feature_set = c(\"observed\", \"season\", \"trend\", \"remainder\"),         .frequency   = \"24 hours\",         .trend       = \"1 week\",         .interactive = FALSE) #> frequency = 24 observations per 24 hours #> trend = 168 observations per 1 week    # ---- GROUPS ---- m4_hourly %>%     group_by(id) %>%     plot_stl_diagnostics(         date, value,         .feature_set = c(\"observed\", \"season\", \"trend\"),         .interactive = FALSE) #> frequency = 24 observations per 1 day #> trend = 336 observations per 14 days #> frequency = 24 observations per 1 day #> trend = 336 observations per 14 days #> frequency = 24 observations per 1 day #> trend = 336 observations per 14 days #> frequency = 24 observations per 1 day #> trend = 336 observations per 14 days"},{"path":"https://business-science.github.io/timetk/reference/plot_time_series.html","id":null,"dir":"Reference","previous_headings":"","what":"Interactive Plotting for One or More Time Series — plot_time_series","title":"Interactive Plotting for One or More Time Series — plot_time_series","text":"workhorse time-series plotting function generates interactive plotly plots, consolidates 20+ lines ggplot2 code, scales well many time series.","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_time_series.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interactive Plotting for One or More Time Series — plot_time_series","text":"","code":"plot_time_series(   .data,   .date_var,   .value,   .color_var = NULL,   .facet_vars = NULL,   .facet_ncol = 1,   .facet_nrow = 1,   .facet_scales = \"free_y\",   .facet_dir = \"h\",   .facet_collapse = FALSE,   .facet_collapse_sep = \" \",   .facet_strip_remove = FALSE,   .line_color = \"#2c3e50\",   .line_size = 0.5,   .line_type = 1,   .line_alpha = 1,   .y_intercept = NULL,   .y_intercept_color = \"#2c3e50\",   .x_intercept = NULL,   .x_intercept_color = \"#2c3e50\",   .smooth = TRUE,   .smooth_period = \"auto\",   .smooth_message = FALSE,   .smooth_span = NULL,   .smooth_degree = 2,   .smooth_color = \"#3366FF\",   .smooth_size = 1,   .smooth_alpha = 1,   .legend_show = TRUE,   .title = \"Time Series Plot\",   .x_lab = \"\",   .y_lab = \"\",   .color_lab = \"Legend\",   .interactive = TRUE,   .plotly_slider = FALSE,   .trelliscope = FALSE,   .trelliscope_params = list() )"},{"path":"https://business-science.github.io/timetk/reference/plot_time_series.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interactive Plotting for One or More Time Series — plot_time_series","text":".data tibble data.frame time-based column .date_var column containing either date date-time values .value column containing numeric values .color_var categorical column can used change line color .facet_vars One grouping columns broken ggplot2 facets. can selected using tidyselect() helpers (e.g contains()). .facet_ncol Number facet columns. .facet_nrow Number facet rows (used .trelliscope = TRUE) .facet_scales Control facet x & y-axis ranges. Options include \"fixed\", \"free\", \"free_y\", \"free_x\" .facet_dir direction faceting (\"h\" horizontal, \"v\" vertical). Default \"h\". .facet_collapse Multiple facets included one facet strip instead multiple facet strips. .facet_collapse_sep separator used collapsing facets. .facet_strip_remove Whether remove strip text label facet. .line_color Line color. Overrided .color_var specified. .line_size Line size. .line_type Line type. .line_alpha Line alpha (opacity). Range: (0, 1). .y_intercept Value y-intercept plot .y_intercept_color Color y-intercept .x_intercept Value x-intercept plot .x_intercept_color Color x-intercept .smooth Logical - Whether include trendline smoother. Uses See smooth_vec() apply LOESS smoother. .smooth_period Number observations include Loess Smoother. Set \"auto\" default, uses tk_get_trend() determine logical trend cycle. .smooth_message Logical. Whether return trend selected message. Useful want see .smooth_period selected. .smooth_span Percentage observations include Loess Smoother. can use either period span. See smooth_vec(). .smooth_degree Flexibility Loess Polynomial. Either 0, 1, 2 (0 = lest flexible, 2 = flexible). .smooth_color Smoother line color .smooth_size Smoother line size .smooth_alpha Smoother alpha (opacity). Range: (0, 1). .legend_show Toggles /Legend .title Title plot .x_lab X-axis label plot .y_lab Y-axis label plot .color_lab Legend label color_var used. .interactive Returns either static (ggplot2) visualization interactive (plotly) visualization .plotly_slider TRUE, returns plotly date range slider. .trelliscope Returns either normal plot trelliscopejs plot (great many time series) Must trelliscopejs installed. .trelliscope_params Pass parameters trelliscopejs::facet_trelliscope() function list(). parameters passed : ncol: use .facet_ncol nrow: use .facet_nrow scales: use facet_scales as_plotly: use .interactive","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_time_series.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interactive Plotting for One or More Time Series — plot_time_series","text":"static ggplot2 plot interactive plotly plot","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_time_series.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Interactive Plotting for One or More Time Series — plot_time_series","text":"plot_time_series() scalable function works ungrouped grouped data.frame objects (tibbles!). Interactive Default plot_time_series() built exploration using: Interactive Plots: plotly (default) - Great exploring! Static Plots: ggplot2 (set .interactive = FALSE) - Great PDF Reports default, interactive plotly visualization returned. Scalable Facets & Dplyr Groups plot_time_series() returns multiple time series plots using ggplot2 facets: group_by() - groups detected, multiple facets returned plot_time_series(.facet_vars) - can manually supply facets well. Can Transform Values just like ggplot .values argument accepts transformations just like ggplot2. example, want take log sales can use call like plot_time_series(date, log(sales)) log transformation applied. Smoother Period / Span Calculation .smooth = TRUE option returns smoother calculated based either: .smooth_period: Number observations .smooth_span: percentage observations default, .smooth_period automatically calculated using 75% observertions. geom_smooth(method = \"loess\", span = 0.75). user can specify time-based window (e.g. .smooth_period = \"1 year\") numeric value (e.g. smooth_period = 365). Time-based windows return median number observations window using tk_get_trend().","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_time_series.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interactive Plotting for One or More Time Series — plot_time_series","text":"","code":"library(dplyr) library(lubridate) #>  #> Attaching package: ‘lubridate’ #> The following objects are masked from ‘package:base’: #>  #>     date, intersect, setdiff, union  # Works with individual time series FANG %>%     filter(symbol == \"FB\") %>%     plot_time_series(date, adjusted, .interactive = FALSE)   # Works with groups FANG %>%     group_by(symbol) %>%     plot_time_series(date, adjusted,                      .facet_ncol  = 2,     # 2-column layout                      .interactive = FALSE)   # Can also group inside & use .color_var FANG %>%     mutate(year = year(date)) %>%     plot_time_series(date, adjusted,                      .facet_vars     = c(symbol, year), # add groups/facets                      .color_var      = year,            # color by year                      .facet_ncol     = 4,                      .facet_scales   = \"free\",                      .facet_collapse = TRUE,  # combine group strip text into 1 line                      .interactive    = FALSE)   # Can apply transformations to .value or .color_var # - .value = log(adjusted) # - .color_var = year(date) FANG %>%     plot_time_series(date, log(adjusted),                      .color_var    = year(date),                      .facet_vars   = contains(\"symbol\"),                      .facet_ncol   = 2,                      .facet_scales = \"free\",                      .y_lab        = \"Log Scale\",                      .interactive  = FALSE)"},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_boxplot.html","id":null,"dir":"Reference","previous_headings":"","what":"Interactive Time Series Box Plots — plot_time_series_boxplot","title":"Interactive Time Series Box Plots — plot_time_series_boxplot","text":"boxplot function generates interactive plotly plots time series.","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_boxplot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interactive Time Series Box Plots — plot_time_series_boxplot","text":"","code":"plot_time_series_boxplot(   .data,   .date_var,   .value,   .period,   .color_var = NULL,   .facet_vars = NULL,   .facet_ncol = 1,   .facet_nrow = 1,   .facet_scales = \"free_y\",   .facet_dir = \"h\",   .facet_collapse = FALSE,   .facet_collapse_sep = \" \",   .facet_strip_remove = FALSE,   .line_color = \"#2c3e50\",   .line_size = 0.5,   .line_type = 1,   .line_alpha = 1,   .y_intercept = NULL,   .y_intercept_color = \"#2c3e50\",   .smooth = TRUE,   .smooth_func = ~mean(.x, na.rm = TRUE),   .smooth_period = \"auto\",   .smooth_message = FALSE,   .smooth_span = NULL,   .smooth_degree = 2,   .smooth_color = \"#3366FF\",   .smooth_size = 1,   .smooth_alpha = 1,   .legend_show = TRUE,   .title = \"Time Series Plot\",   .x_lab = \"\",   .y_lab = \"\",   .color_lab = \"Legend\",   .interactive = TRUE,   .plotly_slider = FALSE,   .trelliscope = FALSE,   .trelliscope_params = list() )"},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_boxplot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interactive Time Series Box Plots — plot_time_series_boxplot","text":".data tibble data.frame time-based column .date_var column containing either date date-time values .value column containing numeric values .period time series unit aggregation boxplot. Examples include: \"1 week\" \"3 years\" \"30 minutes\" .color_var categorical column can used change line color .facet_vars One grouping columns broken ggplot2 facets. can selected using tidyselect() helpers (e.g contains()). .facet_ncol Number facet columns. .facet_nrow Number facet rows (used .trelliscope = TRUE) .facet_scales Control facet x & y-axis ranges. Options include \"fixed\", \"free\", \"free_y\", \"free_x\" .facet_dir direction faceting (\"h\" horizontal, \"v\" vertical). Default \"h\". .facet_collapse Multiple facets included one facet strip instead multiple facet strips. .facet_collapse_sep separator used collapsing facets. .facet_strip_remove Whether remove strip text label facet. .line_color Line color. Overrided .color_var specified. .line_size Line size. .line_type Line type. .line_alpha Line alpha (opacity). Range: (0, 1). .y_intercept Value y-intercept plot .y_intercept_color Color y-intercept .smooth Logical - Whether include trendline smoother. Uses See smooth_vec() apply LOESS smoother. .smooth_func Defines aggregate .value show smoothed trendline. default ~ mean(.x, na.rm = TRUE), uses lambda function ensure NA values removed. Possible values : function, e.g. mean. purrr-style lambda, e.g. ~ mean(.x, na.rm = TRUE) .smooth_period Number observations include Loess Smoother. Set \"auto\" default, uses tk_get_trend() determine logical trend cycle. .smooth_message Logical. Whether return trend selected message. Useful want see .smooth_period selected. .smooth_span Percentage observations include Loess Smoother. can use either period span. See smooth_vec(). .smooth_degree Flexibility Loess Polynomial. Either 0, 1, 2 (0 = lest flexible, 2 = flexible). .smooth_color Smoother line color .smooth_size Smoother line size .smooth_alpha Smoother alpha (opacity). Range: (0, 1). .legend_show Toggles /Legend .title Title plot .x_lab X-axis label plot .y_lab Y-axis label plot .color_lab Legend label color_var used. .interactive Returns either static (ggplot2) visualization interactive (plotly) visualization .plotly_slider TRUE, returns plotly date range slider. .trelliscope Returns either normal plot trelliscopejs plot (great many time series) Must trelliscopejs installed. .trelliscope_params Pass parameters trelliscopejs::facet_trelliscope() function list(). parameters passed : ncol: use .facet_ncol nrow: use .facet_nrow scales: use facet_scales as_plotly: use .interactive","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_boxplot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interactive Time Series Box Plots — plot_time_series_boxplot","text":"static ggplot2 plot interactive plotly plot","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_boxplot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Interactive Time Series Box Plots — plot_time_series_boxplot","text":"plot_time_series_boxplot() scalable function works ungrouped grouped data.frame objects (tibbles!). Interactive Default plot_time_series_boxplot() built exploration using: Interactive Plots: plotly (default) - Great exploring! Static Plots: ggplot2 (set .interactive = FALSE) - Great PDF Reports default, interactive plotly visualization returned. Scalable Facets & Dplyr Groups plot_time_series_boxplot() returns multiple time series plots using ggplot2 facets: group_by() - groups detected, multiple facets returned plot_time_series_boxplot(.facet_vars) - can manually supply facets well. Can Transform Values just like ggplot .values argument accepts transformations just like ggplot2. example, want take log sales can use call like plot_time_series_boxplot(date, log(sales)) log transformation applied. Smoother Period / Span Calculation .smooth = TRUE option returns smoother calculated based either: .smooth_func: method aggregation. Usually aggregation like mean used. purrr-style function syntax can used apply complex functions. .smooth_period: Number observations .smooth_span: percentage observations default, .smooth_period automatically calculated using 75% observertions. geom_smooth(method = \"loess\", span = 0.75). user can specify time-based window (e.g. .smooth_period = \"1 year\") numeric value (e.g. smooth_period = 365). Time-based windows return median number observations window using tk_get_trend().","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_boxplot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interactive Time Series Box Plots — plot_time_series_boxplot","text":"","code":"# \\donttest{ library(dplyr) library(lubridate)  # Works with individual time series FANG %>%     filter(symbol == \"FB\") %>%     plot_time_series_boxplot(         date, adjusted,         .period      = \"3 month\",         .interactive = FALSE)   # Works with groups FANG %>%     group_by(symbol) %>%     plot_time_series_boxplot(         date, adjusted,         .period      = \"3 months\",         .facet_ncol  = 2,     # 2-column layout         .interactive = FALSE)    # Can also group inside & use .color_var FANG %>%     mutate(year = year(date)) %>%     plot_time_series_boxplot(         date, adjusted,         .period      = \"3 months\",         .facet_vars   = c(symbol, year), # add groups/facets         .color_var    = year,            # color by year         .facet_ncol   = 4,         .facet_scales = \"free\",         .interactive  = FALSE)    # Can apply transformations to .value or .color_var # - .value = log(adjusted) # - .color_var = year(date) FANG %>%     plot_time_series_boxplot(         date, log(adjusted),         .period      = \"3 months\",         .color_var    = year(date),         .facet_vars   = contains(\"symbol\"),         .facet_ncol   = 2,         .facet_scales = \"free\",         .y_lab        = \"Log Scale\",         .interactive  = FALSE)   # Can adjust the smoother FANG %>%     group_by(symbol) %>%     plot_time_series_boxplot(         date, adjusted,         .period           = \"3 months\",         .smooth           = TRUE,         .smooth_func      = median,    # Smoother function         .smooth_period    = \"5 years\", # Smoother Period         .facet_ncol       = 2,         .interactive      = FALSE)  # }"},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_cv_plan.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize a Time Series Resample Plan — plot_time_series_cv_plan","title":"Visualize a Time Series Resample Plan — plot_time_series_cv_plan","text":"plot_time_series_cv_plan() function provides visualization time series resample specification (rset) either rolling_origin time_series_cv class.","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_cv_plan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize a Time Series Resample Plan — plot_time_series_cv_plan","text":"","code":"plot_time_series_cv_plan(   .data,   .date_var,   .value,   ...,   .smooth = FALSE,   .title = \"Time Series Cross Validation Plan\" )"},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_cv_plan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize a Time Series Resample Plan — plot_time_series_cv_plan","text":".data time series resample specification either rolling_origin time_series_cv class data frame (tibble) prepared using tk_time_series_cv_plan(). .date_var column containing either date date-time values .value column containing numeric values ... Additional parameters passed plot_time_series() .smooth Logical - Whether include trendline smoother. Uses See smooth_vec() apply LOESS smoother. .title Title plot","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_cv_plan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize a Time Series Resample Plan — plot_time_series_cv_plan","text":"Returns static ggplot interactive plotly object depending whether .interactive FALSE TRUE, respectively.","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_cv_plan.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Visualize a Time Series Resample Plan — plot_time_series_cv_plan","text":"Resample Set resample set output timetk::time_series_cv() function rsample::rolling_origin() function.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_cv_plan.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize a Time Series Resample Plan — plot_time_series_cv_plan","text":"","code":"library(dplyr) library(rsample)  FB_tbl <- FANG %>%     filter(symbol == \"FB\") %>%     select(symbol, date, adjusted)  resample_spec <- time_series_cv(     FB_tbl,     initial = \"1 year\",     assess  = \"6 weeks\",     skip    = \"3 months\",     lag     = \"1 month\",     cumulative  = FALSE,     slice_limit = 6 ) #> Using date_var: date  resample_spec %>% tk_time_series_cv_plan() #> # A tibble: 1,812 × 5 #>    .id    .key     symbol date       adjusted #>    <chr>  <fct>    <chr>  <date>        <dbl> #>  1 Slice1 training FB     2015-11-19     106. #>  2 Slice1 training FB     2015-11-20     107. #>  3 Slice1 training FB     2015-11-23     107. #>  4 Slice1 training FB     2015-11-24     106. #>  5 Slice1 training FB     2015-11-25     105. #>  6 Slice1 training FB     2015-11-27     105. #>  7 Slice1 training FB     2015-11-30     104. #>  8 Slice1 training FB     2015-12-01     107. #>  9 Slice1 training FB     2015-12-02     106. #> 10 Slice1 training FB     2015-12-03     104. #> # ℹ 1,802 more rows  resample_spec %>%     tk_time_series_cv_plan() %>%     plot_time_series_cv_plan(         date, adjusted, # date variable and value variable         # Additional arguments passed to plot_time_series(),         .facet_ncol = 2,         .line_alpha = 0.5,         .interactive = FALSE     )"},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_regression.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize a Time Series Linear Regression Formula — plot_time_series_regression","title":"Visualize a Time Series Linear Regression Formula — plot_time_series_regression","text":"wrapper stats::lm() overlays linear regression fitted model time series, can help show effect feature engineering","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_regression.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize a Time Series Linear Regression Formula — plot_time_series_regression","text":"","code":"plot_time_series_regression(   .data,   .date_var,   .formula,   .show_summary = FALSE,   ... )"},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_regression.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize a Time Series Linear Regression Formula — plot_time_series_regression","text":".data tibble data.frame time-based column .date_var column containing either date date-time values .formula linear regression formula. left-hand side formula used y-axis value. right-hand side formula used develop linear regression model. See stats::lm() details. .show_summary TRUE, prints summary.lm(). ... Additional arguments passed plot_time_series()","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_regression.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize a Time Series Linear Regression Formula — plot_time_series_regression","text":"static ggplot2 plot interactive plotly plot","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_regression.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Visualize a Time Series Linear Regression Formula — plot_time_series_regression","text":"plot_time_series_regression() scalable function works ungrouped grouped data.frame objects (tibbles!). Time Series Formula .formula uses stats::lm() apply linear regression, used visualize effect feature engineering time series. left-hand side formula used y-axis value. right-hand side formula used develop linear regression model. Interactive Default plot_time_series_regression() built exploration using: Interactive Plots: plotly (default) - Great exploring! Static Plots: ggplot2 (set .interactive = FALSE) - Great PDF Reports default, interactive plotly visualization returned. Scalable Facets & Dplyr Groups plot_time_series_regression() returns multiple time series plots using ggplot2 facets: group_by() - groups detected, multiple facets returned plot_time_series_regression(.facet_vars) - can manually supply facets well.","code":""},{"path":"https://business-science.github.io/timetk/reference/plot_time_series_regression.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize a Time Series Linear Regression Formula — plot_time_series_regression","text":"","code":"library(dplyr) library(lubridate)  # ---- SINGLE SERIES ---- m4_monthly %>%     filter(id == \"M750\") %>%     plot_time_series_regression(         .date_var     = date,         .formula      = log(value) ~ as.numeric(date) + month(date, label = TRUE),         .show_summary = TRUE,         .facet_ncol   = 2,         .interactive  = FALSE     ) #>  #> Call: #> stats::lm(formula = .formula, data = df) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -0.12770 -0.05159 -0.01753  0.05142  0.17828  #>  #> Coefficients: #>                                Estimate Std. Error t value Pr(>|t|)     #> (Intercept)                   8.407e+00  1.651e-02 509.199  < 2e-16 *** #> as.numeric(date)              5.679e-05  1.348e-06  42.118  < 2e-16 *** #> month(date, label = TRUE).L  -3.584e-02  1.256e-02  -2.854 0.004625 **  #> month(date, label = TRUE).Q   7.509e-02  1.256e-02   5.979 6.51e-09 *** #> month(date, label = TRUE).C   7.879e-02  1.256e-02   6.273 1.27e-09 *** #> month(date, label = TRUE)^4  -4.931e-02  1.256e-02  -3.926 0.000108 *** #> month(date, label = TRUE)^5  -7.964e-02  1.256e-02  -6.341 8.61e-10 *** #> month(date, label = TRUE)^6   1.215e-02  1.256e-02   0.967 0.334270     #> month(date, label = TRUE)^7   5.196e-02  1.256e-02   4.137 4.60e-05 *** #> month(date, label = TRUE)^8   1.200e-02  1.256e-02   0.955 0.340143     #> month(date, label = TRUE)^9  -3.433e-02  1.256e-02  -2.733 0.006652 **  #> month(date, label = TRUE)^10 -1.566e-02  1.256e-02  -1.247 0.213483     #> month(date, label = TRUE)^11  1.182e-02  1.256e-02   0.941 0.347375     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 0.06341 on 293 degrees of freedom #> Multiple R-squared:  0.8695,\tAdjusted R-squared:  0.8641  #> F-statistic: 162.6 on 12 and 293 DF,  p-value: < 2.2e-16 #>     # ---- GROUPED SERIES ---- m4_monthly %>%     group_by(id) %>%     plot_time_series_regression(         .date_var    = date,         .formula     = log(value) ~ as.numeric(date) + month(date, label = TRUE),         .facet_ncol  = 2,         .interactive = FALSE     )"},{"path":"https://business-science.github.io/timetk/reference/required_pkgs.timetk.html","id":null,"dir":"Reference","previous_headings":"","what":"S3 methods for tracking which additional packages are needed for steps. — required_pkgs.step_box_cox","title":"S3 methods for tracking which additional packages are needed for steps. — required_pkgs.step_box_cox","text":"Recipe-adjacent packages always list required package steps can function properly within parallel processing schemes.","code":""},{"path":"https://business-science.github.io/timetk/reference/required_pkgs.timetk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"S3 methods for tracking which additional packages are needed for steps. — required_pkgs.step_box_cox","text":"","code":"# S3 method for class 'step_box_cox' required_pkgs(x, ...)  # S3 method for class 'step_diff' required_pkgs(x, ...)  # S3 method for class 'step_fourier' required_pkgs(x, ...)  # S3 method for class 'step_holiday_signature' required_pkgs(x, ...)  # S3 method for class 'step_log_interval' required_pkgs(x, ...)  # S3 method for class 'step_slidify' required_pkgs(x, ...)  # S3 method for class 'step_slidify_augment' required_pkgs(x, ...)  # S3 method for class 'step_smooth' required_pkgs(x, ...)  # S3 method for class 'step_timeseries_signature' required_pkgs(x, ...)  # S3 method for class 'step_ts_clean' required_pkgs(x, ...)  # S3 method for class 'step_ts_impute' required_pkgs(x, ...)  # S3 method for class 'step_ts_pad' required_pkgs(x, ...)"},{"path":"https://business-science.github.io/timetk/reference/required_pkgs.timetk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"S3 methods for tracking which additional packages are needed for steps. — required_pkgs.step_box_cox","text":"x recipe step","code":""},{"path":"https://business-science.github.io/timetk/reference/required_pkgs.timetk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"S3 methods for tracking which additional packages are needed for steps. — required_pkgs.step_box_cox","text":"character vector","code":""},{"path":"https://business-science.github.io/timetk/reference/slice_period.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply slice inside periods (windows) — slice_period","title":"Apply slice inside periods (windows) — slice_period","text":"Applies dplyr slice inside time-based period (window).","code":""},{"path":"https://business-science.github.io/timetk/reference/slice_period.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply slice inside periods (windows) — slice_period","text":"","code":"slice_period(.data, ..., .date_var, .period = \"1 day\")"},{"path":"https://business-science.github.io/timetk/reference/slice_period.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply slice inside periods (windows) — slice_period","text":".data tbl object data.frame ... slice(): <data-masking> Integer row values. Provide either positive values keep, negative values drop. values provided must either positive negative. Indices beyond number rows input silently ignored. slice_*(), arguments passed methods. .date_var column containing date date-time values. missing, attempts auto-detect date column. .period period slice within. Time units grouped using lubridate::floor_date() lubridate::ceiling_date(). value can : second minute hour day week month bimonth quarter season halfyear year Arbitrary unique English abbreviations lubridate::period() constructor allowed: \"1 year\" \"2 months\" \"30 seconds\"","code":""},{"path":"https://business-science.github.io/timetk/reference/slice_period.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply slice inside periods (windows) — slice_period","text":"tibble data.frame","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/slice_period.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply slice inside periods (windows) — slice_period","text":"","code":"# Libraries library(dplyr)  # First 5 observations in each month m4_daily %>%     group_by(id) %>%     slice_period(1:5, .period = \"1 month\") #> .date_var is missing. Using: date #> # A tibble: 1,612 × 3 #> # Groups:   id [4] #>    id    date       value #>    <fct> <date>     <dbl> #>  1 D10   2014-07-03 2076. #>  2 D10   2014-07-04 2073. #>  3 D10   2014-07-05 2049. #>  4 D10   2014-07-06 2049. #>  5 D10   2014-07-07 2006. #>  6 D10   2014-08-01 1923. #>  7 D10   2014-08-02 1957. #>  8 D10   2014-08-03 1956. #>  9 D10   2014-08-04 1999. #> 10 D10   2014-08-05 2003. #> # ℹ 1,602 more rows  # Last observation in each month m4_daily %>%     group_by(id) %>%     slice_period(n(), .period = \"1 month\") #> .date_var is missing. Using: date #> # A tibble: 323 × 3 #> # Groups:   id [4] #>    id    date       value #>    <fct> <date>     <dbl> #>  1 D10   2014-07-31 1917. #>  2 D10   2014-08-31 1921. #>  3 D10   2014-09-30 2024. #>  4 D10   2014-10-31 2130  #>  5 D10   2014-11-30 2217. #>  6 D10   2014-12-31 2328. #>  7 D10   2015-01-31 2210. #>  8 D10   2015-02-28 2293. #>  9 D10   2015-03-31 2392. #> 10 D10   2015-04-30 2368. #> # ℹ 313 more rows"},{"path":"https://business-science.github.io/timetk/reference/slidify.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a rolling (sliding) version of any function — slidify","title":"Create a rolling (sliding) version of any function — slidify","text":"slidify returns rolling (sliding) version input function, rolling (sliding) .period specified user.","code":""},{"path":"https://business-science.github.io/timetk/reference/slidify.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a rolling (sliding) version of any function — slidify","text":"","code":"slidify(   .f,   .period = 1,   .align = c(\"center\", \"left\", \"right\"),   .partial = FALSE,   .unlist = TRUE )"},{"path":"https://business-science.github.io/timetk/reference/slidify.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a rolling (sliding) version of any function — slidify","text":".f function, formula, vector (necessarily atomic). function, used . formula, e.g. ~ .x + 2, converted function. three ways refer arguments: single argument function, use . two argument function, use .x .y arguments, use ..1, ..2, ..3 etc syntax allows create compact anonymous functions. Note formula functions conceptually take dots (can use ..1 etc). silently ignore additional arguments used formula expression. character vector, numeric vector, list, converted extractor function. Character vectors index name numeric vectors index position; use list index position name different levels. component present, value .default returned. .period period size roll .align One \"center\", \"left\" \"right\". .partial moving window allowed return partial (incomplete) windows instead NA values. Set FALSE default, can switched TRUE remove NA's. .unlist function returns single value time called, use .unlist = TRUE. function returns one value, complicated object (like linear model), use .unlist = FALSE create list-column rolling results.","code":""},{"path":"https://business-science.github.io/timetk/reference/slidify.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a rolling (sliding) version of any function — slidify","text":"function rolling/sliding conversion applied.","code":""},{"path":"https://business-science.github.io/timetk/reference/slidify.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a rolling (sliding) version of any function — slidify","text":"slidify() function almost identical tibbletime::rollify() 3 improvements: Alignment (\"center\", \"left\", \"right\") Partial windows allowed Uses slider hood, improves speed reliability implementing code C++ level Make function Sliding (Rolling) Function slidify() turns function sliding version use inside call dplyr::mutate(), however works equally well called purrr::map(). intended use dplyr::mutate(), slidify creates function always returns output length input Alignment Rolling / Sliding functions generate .period - 1 fewer values incoming vector. Thus, vector needs aligned. Alignment vector follows 3 types: center (default): NA .partial values divided added beginning end series \"Center\" moving average. common Time Series applications (e.g. denoising). left: NA .partial values added end shift series Left. right: NA .partial values added beginning shift series Right. common Financial Applications (e.g moving average cross-overs). Allowing Partial Windows key improvement tibbletime::slidify() timetk::slidify() implements .partial rolling windows. Just set .partial = TRUE.","code":""},{"path":"https://business-science.github.io/timetk/reference/slidify.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create a rolling (sliding) version of any function — slidify","text":"Tibbletime R Package Davis Vaughan, includes original rollify() Function","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/slidify.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a rolling (sliding) version of any function — slidify","text":"","code":"library(dplyr)  FB <- FANG %>% dplyr::filter(symbol == \"FB\")   # --- ROLLING MEAN (SINGLE ARG EXAMPLE) ---  # Turn the normal mean function into a rolling mean with a 5 row .period mean_roll_5 <- slidify(mean, .period = 5, .align = \"right\")  FB %>%     mutate(rolling_mean_5 = mean_roll_5(adjusted)) #> # A tibble: 1,008 × 9 #>    symbol date        open  high   low close    volume adjusted rolling_mean_5 #>    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>          <dbl> #>  1 FB     2013-01-02  27.4  28.2  27.4  28    69846400     28             NA   #>  2 FB     2013-01-03  27.9  28.5  27.6  27.8  63140600     27.8           NA   #>  3 FB     2013-01-04  28.0  28.9  27.8  28.8  72715400     28.8           NA   #>  4 FB     2013-01-07  28.7  29.8  28.6  29.4  83781800     29.4           NA   #>  5 FB     2013-01-08  29.5  29.6  28.9  29.1  45871300     29.1           28.6 #>  6 FB     2013-01-09  29.7  30.6  29.5  30.6 104787700     30.6           29.1 #>  7 FB     2013-01-10  30.6  31.5  30.3  31.3  95316400     31.3           29.8 #>  8 FB     2013-01-11  31.3  32.0  31.1  31.7  89598000     31.7           30.4 #>  9 FB     2013-01-14  32.1  32.2  30.6  31.0  98892800     31.0           30.7 #> 10 FB     2013-01-15  30.6  31.7  29.9  30.1 173242600     30.1           30.9 #> # ℹ 998 more rows  # Use `partial = TRUE` to allow partial windows (those with less than the full .period) mean_roll_5_partial <- slidify(mean, .period = 5, .align = \"right\", .partial = TRUE)  FB %>%     mutate(rolling_mean_5 = mean_roll_5_partial(adjusted)) #> # A tibble: 1,008 × 9 #>    symbol date        open  high   low close    volume adjusted rolling_mean_5 #>    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>          <dbl> #>  1 FB     2013-01-02  27.4  28.2  27.4  28    69846400     28             28   #>  2 FB     2013-01-03  27.9  28.5  27.6  27.8  63140600     27.8           27.9 #>  3 FB     2013-01-04  28.0  28.9  27.8  28.8  72715400     28.8           28.2 #>  4 FB     2013-01-07  28.7  29.8  28.6  29.4  83781800     29.4           28.5 #>  5 FB     2013-01-08  29.5  29.6  28.9  29.1  45871300     29.1           28.6 #>  6 FB     2013-01-09  29.7  30.6  29.5  30.6 104787700     30.6           29.1 #>  7 FB     2013-01-10  30.6  31.5  30.3  31.3  95316400     31.3           29.8 #>  8 FB     2013-01-11  31.3  32.0  31.1  31.7  89598000     31.7           30.4 #>  9 FB     2013-01-14  32.1  32.2  30.6  31.0  98892800     31.0           30.7 #> 10 FB     2013-01-15  30.6  31.7  29.9  30.1 173242600     30.1           30.9 #> # ℹ 998 more rows  # There's nothing stopping you from combining multiple rolling functions with # different .period sizes in the same mutate call  mean_roll_10 <- slidify(mean, .period = 10, .align = \"right\")  FB %>%     select(symbol, date, adjusted) %>%     mutate(         rolling_mean_5  = mean_roll_5(adjusted),         rolling_mean_10 = mean_roll_10(adjusted)     ) #> # A tibble: 1,008 × 5 #>    symbol date       adjusted rolling_mean_5 rolling_mean_10 #>    <chr>  <date>        <dbl>          <dbl>           <dbl> #>  1 FB     2013-01-02     28             NA              NA   #>  2 FB     2013-01-03     27.8           NA              NA   #>  3 FB     2013-01-04     28.8           NA              NA   #>  4 FB     2013-01-07     29.4           NA              NA   #>  5 FB     2013-01-08     29.1           28.6            NA   #>  6 FB     2013-01-09     30.6           29.1            NA   #>  7 FB     2013-01-10     31.3           29.8            NA   #>  8 FB     2013-01-11     31.7           30.4            NA   #>  9 FB     2013-01-14     31.0           30.7            NA   #> 10 FB     2013-01-15     30.1           30.9            29.8 #> # ℹ 998 more rows  # For summary operations like rolling means, we can accomplish large-scale # multi-rolls with tk_augment_slidify()  FB %>%     select(symbol, date, adjusted) %>%     tk_augment_slidify(         adjusted, .period = 5:10, .f = mean, .align = \"right\",         .names = stringr::str_c(\"MA_\", 5:10)     ) #> # A tibble: 1,008 × 9 #>    symbol date       adjusted  MA_5  MA_6  MA_7  MA_8  MA_9 MA_10 #>    <chr>  <date>        <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #>  1 FB     2013-01-02     28    NA    NA    NA    NA    NA    NA   #>  2 FB     2013-01-03     27.8  NA    NA    NA    NA    NA    NA   #>  3 FB     2013-01-04     28.8  NA    NA    NA    NA    NA    NA   #>  4 FB     2013-01-07     29.4  NA    NA    NA    NA    NA    NA   #>  5 FB     2013-01-08     29.1  28.6  NA    NA    NA    NA    NA   #>  6 FB     2013-01-09     30.6  29.1  28.9  NA    NA    NA    NA   #>  7 FB     2013-01-10     31.3  29.8  29.5  29.3  NA    NA    NA   #>  8 FB     2013-01-11     31.7  30.4  30.1  29.8  29.6  NA    NA   #>  9 FB     2013-01-14     31.0  30.7  30.5  30.3  29.9  29.7  NA   #> 10 FB     2013-01-15     30.1  30.9  30.6  30.4  30.2  30.0  29.8 #> # ℹ 998 more rows  # --- GROUPS AND ROLLING ----  # One of the most powerful things about this is that it works with # groups since `mutate` is being used  mean_roll_3 <- slidify(mean, .period = 3, .align = \"right\")  FANG %>%     group_by(symbol) %>%     mutate(mean_roll = mean_roll_3(adjusted)) %>%     slice(1:5) #> # A tibble: 20 × 9 #> # Groups:   symbol [4] #>    symbol date        open  high   low close   volume adjusted mean_roll #>    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>     <dbl> #>  1 AMZN   2013-01-02 256.  258.  253.  257.   3271000    257.       NA   #>  2 AMZN   2013-01-03 257.  261.  256.  258.   2750900    258.       NA   #>  3 AMZN   2013-01-04 258.  260.  257.  259.   1874200    259.      258.  #>  4 AMZN   2013-01-07 263.  270.  263.  268.   4910000    268.      262.  #>  5 AMZN   2013-01-08 267.  269.  264.  266.   3010700    266.      265.  #>  6 FB     2013-01-02  27.4  28.2  27.4  28   69846400     28        NA   #>  7 FB     2013-01-03  27.9  28.5  27.6  27.8 63140600     27.8      NA   #>  8 FB     2013-01-04  28.0  28.9  27.8  28.8 72715400     28.8      28.2 #>  9 FB     2013-01-07  28.7  29.8  28.6  29.4 83781800     29.4      28.6 #> 10 FB     2013-01-08  29.5  29.6  28.9  29.1 45871300     29.1      29.1 #> 11 GOOG   2013-01-02 719.  727.  717.  723.   5101500    361.       NA   #> 12 GOOG   2013-01-03 725.  732.  721.  724.   4653700    361.       NA   #> 13 GOOG   2013-01-04 729.  741.  728.  738.   5547600    369.      364.  #> 14 GOOG   2013-01-07 735.  739.  731.  735.   3323800    367.      366.  #> 15 GOOG   2013-01-08 736.  736.  724.  733.   3364700    366.      367.  #> 16 NFLX   2013-01-02  95.2  95.8  90.7  92.0 19431300     13.1      NA   #> 17 NFLX   2013-01-03  92.0  97.9  91.5  96.6 27912500     13.8      NA   #> 18 NFLX   2013-01-04  96.5  97.7  95.5  96.0 17761100     13.7      13.6 #> 19 NFLX   2013-01-07  96.4 102.   96.1  99.2 45550400     14.2      13.9 #> 20 NFLX   2013-01-08 100.  101.   96.8  97.2 24714900     13.9      13.9   # --- ROLLING CORRELATION (MULTIPLE ARG EXAMPLE) ---  # With 2 args, use the purrr syntax of ~ and .x, .y # Rolling correlation example cor_roll <- slidify(~cor(.x, .y), .period = 5, .align = \"right\")  FB %>%     mutate(running_cor = cor_roll(adjusted, open)) #> # A tibble: 1,008 × 9 #>    symbol date        open  high   low close    volume adjusted running_cor #>    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>       <dbl> #>  1 FB     2013-01-02  27.4  28.2  27.4  28    69846400     28        NA     #>  2 FB     2013-01-03  27.9  28.5  27.6  27.8  63140600     27.8      NA     #>  3 FB     2013-01-04  28.0  28.9  27.8  28.8  72715400     28.8      NA     #>  4 FB     2013-01-07  28.7  29.8  28.6  29.4  83781800     29.4      NA     #>  5 FB     2013-01-08  29.5  29.6  28.9  29.1  45871300     29.1       0.749 #>  6 FB     2013-01-09  29.7  30.6  29.5  30.6 104787700     30.6       0.805 #>  7 FB     2013-01-10  30.6  31.5  30.3  31.3  95316400     31.3       0.859 #>  8 FB     2013-01-11  31.3  32.0  31.1  31.7  89598000     31.7       0.884 #>  9 FB     2013-01-14  32.1  32.2  30.6  31.0  98892800     31.0       0.667 #> 10 FB     2013-01-15  30.6  31.7  29.9  30.1 173242600     30.1       0.379 #> # ℹ 998 more rows  # With >2 args, create an anonymous function with >2 args or use # the purrr convention of ..1, ..2, ..3 to refer to the arguments avg_of_avgs <- slidify(     function(x, y, z) (mean(x) + mean(y) + mean(z)) / 3,     .period = 10,     .align = \"right\" )  # Or avg_of_avgs <- slidify(     ~(mean(..1) + mean(..2) + mean(..3)) / 3,     .period = 10,     .align  = \"right\" )  FB %>%     mutate(avg_of_avgs = avg_of_avgs(open, high, low)) #> # A tibble: 1,008 × 9 #>    symbol date        open  high   low close    volume adjusted avg_of_avgs #>    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>       <dbl> #>  1 FB     2013-01-02  27.4  28.2  27.4  28    69846400     28          NA   #>  2 FB     2013-01-03  27.9  28.5  27.6  27.8  63140600     27.8        NA   #>  3 FB     2013-01-04  28.0  28.9  27.8  28.8  72715400     28.8        NA   #>  4 FB     2013-01-07  28.7  29.8  28.6  29.4  83781800     29.4        NA   #>  5 FB     2013-01-08  29.5  29.6  28.9  29.1  45871300     29.1        NA   #>  6 FB     2013-01-09  29.7  30.6  29.5  30.6 104787700     30.6        NA   #>  7 FB     2013-01-10  30.6  31.5  30.3  31.3  95316400     31.3        NA   #>  8 FB     2013-01-11  31.3  32.0  31.1  31.7  89598000     31.7        NA   #>  9 FB     2013-01-14  32.1  32.2  30.6  31.0  98892800     31.0        NA   #> 10 FB     2013-01-15  30.6  31.7  29.9  30.1 173242600     30.1        29.7 #> # ℹ 998 more rows  # Optional arguments MUST be passed at the creation of the rolling function # Only data arguments that are \"rolled over\" are allowed when calling the # rolling version of the function FB$adjusted[1] <- NA  roll_mean_na_rm <- slidify(~mean(.x, na.rm = TRUE), .period = 5, .align = \"right\")  FB %>%     mutate(roll_mean = roll_mean_na_rm(adjusted)) #> # A tibble: 1,008 × 9 #>    symbol date        open  high   low close    volume adjusted roll_mean #>    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>     <dbl> #>  1 FB     2013-01-02  27.4  28.2  27.4  28    69846400     NA        NA   #>  2 FB     2013-01-03  27.9  28.5  27.6  27.8  63140600     27.8      NA   #>  3 FB     2013-01-04  28.0  28.9  27.8  28.8  72715400     28.8      NA   #>  4 FB     2013-01-07  28.7  29.8  28.6  29.4  83781800     29.4      NA   #>  5 FB     2013-01-08  29.5  29.6  28.9  29.1  45871300     29.1      28.8 #>  6 FB     2013-01-09  29.7  30.6  29.5  30.6 104787700     30.6      29.1 #>  7 FB     2013-01-10  30.6  31.5  30.3  31.3  95316400     31.3      29.8 #>  8 FB     2013-01-11  31.3  32.0  31.1  31.7  89598000     31.7      30.4 #>  9 FB     2013-01-14  32.1  32.2  30.6  31.0  98892800     31.0      30.7 #> 10 FB     2013-01-15  30.6  31.7  29.9  30.1 173242600     30.1      30.9 #> # ℹ 998 more rows   # --- ROLLING REGRESSIONS ----  # Rolling regressions are easy to implement using `.unlist = FALSE` lm_roll <- slidify(~lm(.x ~ .y), .period = 90, .unlist = FALSE, .align = \"right\")  FB %>%     tidyr::drop_na() %>%     mutate(numeric_date = as.numeric(date)) %>%     mutate(rolling_lm = lm_roll(adjusted, numeric_date)) %>%     filter(!is.na(rolling_lm)) #> # A tibble: 918 × 10 #>    symbol date        open  high   low close   volume adjusted numeric_date #>    <chr>  <date>     <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>        <dbl> #>  1 FB     2013-05-13  26.6  27.3  26.5  26.8 29068800     26.8        15838 #>  2 FB     2013-05-14  26.9  27.3  26.8  27.1 24930300     27.1        15839 #>  3 FB     2013-05-15  26.9  27.0  26.4  26.6 30299800     26.6        15840 #>  4 FB     2013-05-16  26.5  26.5  25.9  26.1 35499100     26.1        15841 #>  5 FB     2013-05-17  26.4  26.6  26.2  26.2 29462700     26.2        15842 #>  6 FB     2013-05-20  26.2  26.2  25.7  25.8 42402900     25.8        15845 #>  7 FB     2013-05-21  25.9  26.1  25.6  25.7 26261300     25.7        15846 #>  8 FB     2013-05-22  25.6  25.8  24.9  25.2 45314500     25.2        15847 #>  9 FB     2013-05-23  24.8  25.5  24.8  25.1 37663100     25.1        15848 #> 10 FB     2013-05-24  25.0  25.0  24.1  24.3 58727900     24.3        15849 #> # ℹ 908 more rows #> # ℹ 1 more variable: rolling_lm <list>"},{"path":"https://business-science.github.io/timetk/reference/slidify_vec.html","id":null,"dir":"Reference","previous_headings":"","what":"Rolling Window Transformation — slidify_vec","title":"Rolling Window Transformation — slidify_vec","text":"slidify_vec() applies summary function rolling sequence windows.","code":""},{"path":"https://business-science.github.io/timetk/reference/slidify_vec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rolling Window Transformation — slidify_vec","text":"","code":"slidify_vec(   .x,   .f,   ...,   .period = 1,   .align = c(\"center\", \"left\", \"right\"),   .partial = FALSE )"},{"path":"https://business-science.github.io/timetk/reference/slidify_vec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rolling Window Transformation — slidify_vec","text":".x vector rolling window transformation applied. .f summary [function / formula] function, e.g. mean, function used additional arguments, .... formula, e.g. ~ mean(., na.rm = TRUE), converted function. syntax allows create compact anonymous functions. ... Additional arguments passed .f function. .period number periods include local rolling window. effectively \"window size\". .align One \"center\", \"left\" \"right\". .partial moving window allowed return partial (incomplete) windows instead NA values. Set FALSE default, can switched TRUE remove NA's.","code":""},{"path":"https://business-science.github.io/timetk/reference/slidify_vec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rolling Window Transformation — slidify_vec","text":"numeric vector","code":""},{"path":"https://business-science.github.io/timetk/reference/slidify_vec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Rolling Window Transformation — slidify_vec","text":"slidify_vec() function wrapper slider::slide_vec() parameters simplified \"center\", \"left\", \"right\" alignment. Vector Length == Vector Length NA values .partial values always returned ensure length return vector length incoming vector. ensures easier use dplyr::mutate(). Alignment Rolling functions generate .period - 1 fewer values incoming vector. Thus, vector needs aligned. Alignment vector follows 3 types: Center: NA .partial values divided added beginning end series \"Center\" moving average. common de-noising operations. See also [smooth_vec()] LOESS without NA values. Left: NA .partial values added end shift series Left. Right: NA .partial values added beginning shif series Right. common Financial Applications moving average cross-overs. Partial Values advantage using .partial values vs NA padding series can filled (good time-series de-noising operations). downside partial values partials can become less stable regions incomplete windows used. instability desirable de-noising operations, suitable alternative smooth_vec(), implements local polynomial regression.","code":""},{"path":"https://business-science.github.io/timetk/reference/slidify_vec.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Rolling Window Transformation — slidify_vec","text":"Slider R Package Davis Vaughan","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/slidify_vec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rolling Window Transformation — slidify_vec","text":"","code":"library(dplyr) library(ggplot2)  # Training Data FB_tbl <- FANG %>%     filter(symbol == \"FB\") %>%     select(symbol, date, adjusted)  # ---- FUNCTION FORMAT ---- # - The `.f = mean` function is used. Argument `na.rm = TRUE` is passed as ... FB_tbl %>%     mutate(adjusted_30_ma = slidify_vec(         .x      = adjusted,         .period = 30,         .f      = mean,         na.rm   = TRUE,         .align  = \"center\")) %>%         ggplot(aes(date, adjusted)) +         geom_line() +         geom_line(aes(y = adjusted_30_ma), color = \"blue\", na.rm = TRUE)   # ---- FORMULA FORMAT ---- # - Anonymous function `.f = ~ mean(., na.rm = TRUE)` is used FB_tbl %>%     mutate(adjusted_30_ma = slidify_vec(         .x      = adjusted,         .period = 30,         .f      = ~ mean(., na.rm = TRUE),         .align  = \"center\")) %>%         ggplot(aes(date, adjusted)) +         geom_line() +         geom_line(aes(y = adjusted_30_ma), color = \"blue\", na.rm = TRUE)   # ---- PARTIAL VALUES ---- # - set `.partial = TRUE` FB_tbl %>%     mutate(adjusted_30_ma = slidify_vec(         .x       = adjusted,         .f       = ~ mean(., na.rm = TRUE),         .period  = 30,         .align   = \"center\",         .partial = TRUE)) %>%         ggplot(aes(date, adjusted)) +         geom_line() +         geom_line(aes(y = adjusted_30_ma), color = \"blue\")   # ---- Loess vs Moving Average ---- # - Loess: Using `.degree = 0` to make less flexible. Comparable to a moving average.  FB_tbl %>%     mutate(         adjusted_loess_30 = smooth_vec(adjusted, period = 30, degree = 0),         adjusted_ma_30    = slidify_vec(adjusted, .f = mean,                                            .period = 30, .partial = TRUE)     ) %>%     ggplot(aes(date, adjusted)) +     geom_line() +     geom_line(aes(y = adjusted_loess_30), color = \"red\") +     geom_line(aes(y = adjusted_ma_30), color = \"blue\") +     labs(title = \"Loess vs Moving Average\")"},{"path":"https://business-science.github.io/timetk/reference/smooth_vec.html","id":null,"dir":"Reference","previous_headings":"","what":"Smoothing Transformation using Loess — smooth_vec","title":"Smoothing Transformation using Loess — smooth_vec","text":"smooth_vec() applies LOESS transformation numeric vector.","code":""},{"path":"https://business-science.github.io/timetk/reference/smooth_vec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Smoothing Transformation using Loess — smooth_vec","text":"","code":"smooth_vec(x, period = 30, span = NULL, degree = 2)"},{"path":"https://business-science.github.io/timetk/reference/smooth_vec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Smoothing Transformation using Loess — smooth_vec","text":"x numeric vector smoothing transformation applied. period number periods include local smoothing. Similar window size moving average. See details explanation period vs span specification. span span percentage data included smoothing window. Period preferred shorter windows fix window size. See details explanation period vs span specification. degree degree polynomials used. Accetable values (least flexible): 0, 1, 2. Set 2 default 2nd order polynomial (flexible).","code":""},{"path":"https://business-science.github.io/timetk/reference/smooth_vec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Smoothing Transformation using Loess — smooth_vec","text":"numeric vector","code":""},{"path":"https://business-science.github.io/timetk/reference/smooth_vec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Smoothing Transformation using Loess — smooth_vec","text":"Benefits: using period, effect similar moving average without creating missing values. using span, effect detect trend series using percentage total number observations. Loess Smoother Algorithm function simplified wrapper stats::loess() modification set fixed period rather percentage data points via span. Period vs Span? period fixed whereas span changes number observations change. use Period? effect using period similar Moving Average Window Size Fixed Period. helps trying smooth local trends. want 30-day moving average, specify period = 30. use Span? Span easier specify want Long-Term Trendline window size unknown. can specify span = 0.75 locally regress using window 75% data.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/smooth_vec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Smoothing Transformation using Loess — smooth_vec","text":"","code":"library(dplyr) library(ggplot2)  # Training Data FB_tbl <- FANG %>%     filter(symbol == \"FB\") %>%     select(symbol, date, adjusted)  # ---- PERIOD ----  FB_tbl %>%     mutate(adjusted_30 = smooth_vec(adjusted, period = 30, degree = 2)) %>%     ggplot(aes(date, adjusted)) +     geom_line() +     geom_line(aes(y = adjusted_30), color = \"red\")   # ---- SPAN ----  FB_tbl %>%     mutate(adjusted_30 = smooth_vec(adjusted, span = 0.75, degree = 2)) %>%     ggplot(aes(date, adjusted)) +     geom_line() +     geom_line(aes(y = adjusted_30), color = \"red\")   # ---- Loess vs Moving Average ---- # - Loess: Using `degree = 0` to make less flexible. Comperable to a moving average.  FB_tbl %>%     mutate(         adjusted_loess_30 = smooth_vec(adjusted, period = 30, degree = 0),         adjusted_ma_30    = slidify_vec(adjusted, .period = 30,                                         .f = mean, .partial = TRUE)     ) %>%     ggplot(aes(date, adjusted)) +     geom_line() +     geom_line(aes(y = adjusted_loess_30), color = \"red\") +     geom_line(aes(y = adjusted_ma_30), color = \"blue\") +     labs(title = \"Loess vs Moving Average\")"},{"path":"https://business-science.github.io/timetk/reference/standardize_vec.html","id":null,"dir":"Reference","previous_headings":"","what":"Standardize to Mean 0, Standard Deviation 1 (Center & Scale) — standardize_vec","title":"Standardize to Mean 0, Standard Deviation 1 (Center & Scale) — standardize_vec","text":"Standardization commonly used center scale numeric features prevent one dominating algorithms require data scale.","code":""},{"path":"https://business-science.github.io/timetk/reference/standardize_vec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Standardize to Mean 0, Standard Deviation 1 (Center & Scale) — standardize_vec","text":"","code":"standardize_vec(x, mean = NULL, sd = NULL, silent = FALSE)  standardize_inv_vec(x, mean, sd)"},{"path":"https://business-science.github.io/timetk/reference/standardize_vec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Standardize to Mean 0, Standard Deviation 1 (Center & Scale) — standardize_vec","text":"x numeric vector. mean mean used invert standardization sd standard deviation used invert standardization process. silent Whether report automated mean sd parameters message.","code":""},{"path":"https://business-science.github.io/timetk/reference/standardize_vec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Standardize to Mean 0, Standard Deviation 1 (Center & Scale) — standardize_vec","text":"Returns numeric vector standardization transformation applied.","code":""},{"path":"https://business-science.github.io/timetk/reference/standardize_vec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Standardize to Mean 0, Standard Deviation 1 (Center & Scale) — standardize_vec","text":"Standardization vs Normalization Standardization refers transformation reduces range mean 0, standard deviation 1 Normalization refers transformation reduces min-max range: (0, 1)","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/standardize_vec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Standardize to Mean 0, Standard Deviation 1 (Center & Scale) — standardize_vec","text":"","code":"library(dplyr)  d10_daily <- m4_daily %>% dplyr::filter(id == \"D10\")  # --- VECTOR ----  value_std <- standardize_vec(d10_daily$value) #> Standardization Parameters #> mean: 2261.60682492582 #> standard deviation: 175.603721730477 value     <- standardize_inv_vec(value_std,                                  mean = 2261.60682492582,                                  sd   = 175.603721730477)  # --- MUTATE ----  m4_daily %>%     group_by(id) %>%     mutate(value_std = standardize_vec(value)) #> Standardization Parameters #> mean: 2261.60682492582 #> standard deviation: 175.603721730477 #> Standardization Parameters #> mean: 9243.15525375268 #> standard deviation: 4663.16194403596 #> Standardization Parameters #> mean: 8259.78634615385 #> standard deviation: 927.592527167825 #> Standardization Parameters #> mean: 8287.72878932316 #> standard deviation: 2456.05840988041 #> # A tibble: 9,743 × 4 #> # Groups:   id [4] #>    id    date       value value_std #>    <fct> <date>     <dbl>     <dbl> #>  1 D10   2014-07-03 2076.     -1.06 #>  2 D10   2014-07-04 2073.     -1.07 #>  3 D10   2014-07-05 2049.     -1.21 #>  4 D10   2014-07-06 2049.     -1.21 #>  5 D10   2014-07-07 2006.     -1.45 #>  6 D10   2014-07-08 2018.     -1.39 #>  7 D10   2014-07-09 2019.     -1.38 #>  8 D10   2014-07-10 2007.     -1.45 #>  9 D10   2014-07-11 2010      -1.43 #> 10 D10   2014-07-12 2002.     -1.48 #> # ℹ 9,733 more rows"},{"path":"https://business-science.github.io/timetk/reference/step_box_cox.html","id":null,"dir":"Reference","previous_headings":"","what":"Box-Cox Transformation using Forecast Methods — step_box_cox","title":"Box-Cox Transformation using Forecast Methods — step_box_cox","text":"step_box_cox creates specification recipe step transform data using Box-Cox transformation. function differs recipes::step_BoxCox adding multiple methods including Guerrero lambda optimization handling negative data used Forecast R Package.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_box_cox.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Box-Cox Transformation using Forecast Methods — step_box_cox","text":"","code":"step_box_cox(   recipe,   ...,   method = c(\"guerrero\", \"loglik\"),   limits = c(-1, 2),   role = NA,   trained = FALSE,   lambdas_trained = NULL,   skip = FALSE,   id = rand_id(\"box_cox\") )  # S3 method for class 'step_box_cox' tidy(x, ...)"},{"path":"https://business-science.github.io/timetk/reference/step_box_cox.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Box-Cox Transformation using Forecast Methods — step_box_cox","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. tidy method, currently used. method One \"guerrero\" \"loglik\" limits length 2 numeric vector defining range compute transformation parameter lambda. role used step since new variables created. trained logical indicate quantities preprocessing estimated. lambdas_trained numeric vector transformation values. NULL computed prep(). skip logical. step skipped recipe baked bake.recipe()? operations baked prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify . x step_box_cox object.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_box_cox.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Box-Cox Transformation using Forecast Methods — step_box_cox","text":"updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables selected) value (lambda estimate).","code":""},{"path":"https://business-science.github.io/timetk/reference/step_box_cox.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Box-Cox Transformation using Forecast Methods — step_box_cox","text":"step_box_cox() function designed specifically handle time series using methods implemented Forecast R Package. Negative Data function can applied Negative Data. Lambda Optimization Methods function uses 2 methods optimizing lambda selection Forecast R Package: method = \"guerrero\": Guerrero's (1993) method used, lambda minimizes coefficient variation subseries x. method = loglik: value lambda chosen maximize profile log likelihood linear model fitted x. non-seasonal data, linear time trend fitted seasonal data, linear time trend seasonal dummy variables used.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_box_cox.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Box-Cox Transformation using Forecast Methods — step_box_cox","text":"Guerrero, V.M. (1993) Time-series analysis supported power transformations. Journal Forecasting, 12, 37–48. Box, G. E. P. Cox, D. R. (1964) analysis transformations. JRSS B 26 211–246.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/step_box_cox.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Box-Cox Transformation using Forecast Methods — step_box_cox","text":"","code":"library(dplyr) library(recipes) #>  #> Attaching package: ‘recipes’ #> The following object is masked from ‘package:stats’: #>  #>     step  FANG_wide <- FANG %>%     select(symbol, date, adjusted) %>%     tidyr::pivot_wider(names_from = symbol, values_from = adjusted)  recipe_box_cox <- recipe(~ ., data = FANG_wide) %>%     step_box_cox(FB, AMZN, NFLX, GOOG) %>%     prep()  recipe_box_cox %>% bake(FANG_wide) #> # A tibble: 1,008 × 5 #>    date          FB  AMZN  NFLX  GOOG #>    <date>     <dbl> <dbl> <dbl> <dbl> #>  1 2013-01-02  12.5  8.28  4.92  5.26 #>  2 2013-01-03  12.4  8.28  5.08  5.27 #>  3 2013-01-04  12.7  8.29  5.06  5.28 #>  4 2013-01-07  12.9  8.37  5.17  5.28 #>  5 2013-01-08  12.8  8.35  5.10  5.28 #>  6 2013-01-09  13.3  8.35  5.06  5.28 #>  7 2013-01-10  13.5  8.34  5.13  5.28 #>  8 2013-01-11  13.7  8.36  5.24  5.28 #>  9 2013-01-14  13.4  8.40  5.32  5.26 #> 10 2013-01-15  13.1  8.39  5.26  5.27 #> # ℹ 998 more rows  recipe_box_cox %>% tidy(1) #> # A tibble: 4 × 3 #>   terms  lambda id            #>   <chr>   <dbl> <chr>         #> 1 FB     0.671  box_cox_bwPcL #> 2 AMZN   0.135  box_cox_bwPcL #> 3 NFLX   0.458  box_cox_bwPcL #> 4 GOOG  -0.0388 box_cox_bwPcL"},{"path":"https://business-science.github.io/timetk/reference/step_diff.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a differenced predictor — step_diff","title":"Create a differenced predictor — step_diff","text":"step_diff creates specification recipe step add new columns differenced data. Differenced data include NA values difference induced. can removed recipes::step_naomit().","code":""},{"path":"https://business-science.github.io/timetk/reference/step_diff.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a differenced predictor — step_diff","text":"","code":"step_diff(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   lag = 1,   difference = 1,   log = FALSE,   prefix = \"diff_\",   columns = NULL,   skip = FALSE,   id = rand_id(\"diff\") )  # S3 method for class 'step_diff' tidy(x, ...)"},{"path":"https://business-science.github.io/timetk/reference/step_diff.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a differenced predictor — step_diff","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role Defaults \"predictor\" trained logical indicate quantities preprocessing estimated. lag vector positive integers identifying lags (far back) included differencing calculation. difference number differences perform. log Calculates log differences instead differences. prefix prefix generated column names, default \"diff_\". columns character string variable names populated (eventually) terms argument. skip logical. step skipped recipe baked bake.recipe()? operations baked prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations id character string unique step identify . x step_diff object.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_diff.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a differenced predictor — step_diff","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://business-science.github.io/timetk/reference/step_diff.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a differenced predictor — step_diff","text":"step assumes data already proper sequential order lagging.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/step_diff.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a differenced predictor — step_diff","text":"","code":"library(recipes)   FANG_wide <- FANG %>%     dplyr::select(symbol, date, adjusted) %>%     tidyr::pivot_wider(names_from = symbol, values_from = adjusted)   # Make and apply recipe ----  recipe_diff <- recipe(~ ., data = FANG_wide) %>%   step_diff(FB, AMZN, NFLX, GOOG, lag = 1:3, difference = 1) %>%   prep()  recipe_diff %>% bake(FANG_wide) #> # A tibble: 1,008 × 17 #>    date          FB  AMZN  NFLX  GOOG diff_1_1_FB diff_1_1_AMZN diff_1_1_NFLX #>    <date>     <dbl> <dbl> <dbl> <dbl>       <dbl>         <dbl>         <dbl> #>  1 2013-01-02  28    257.  13.1  361.      NA           NA            NA      #>  2 2013-01-03  27.8  258.  13.8  361.      -0.230        1.17          0.654  #>  3 2013-01-04  28.8  259.  13.7  369.       0.990        0.670        -0.0871 #>  4 2013-01-07  29.4  268.  14.2  367.       0.66         9.31          0.460  #>  5 2013-01-08  29.1  266.  13.9  366.      -0.360       -2.08         -0.291  #>  6 2013-01-09  30.6  266.  13.7  369.       1.53        -0.0300       -0.179  #>  7 2013-01-10  31.3  265.  14    370.       0.710       -1.01          0.299  #>  8 2013-01-11  31.7  268.  14.5  370.       0.420        2.60          0.470  #>  9 2013-01-14  31.0  273.  14.8  361.      -0.770        4.79          0.309  #> 10 2013-01-15  30.1  272.  14.5  362.      -0.850       -0.830        -0.251  #> # ℹ 998 more rows #> # ℹ 9 more variables: diff_1_1_GOOG <dbl>, diff_2_1_FB <dbl>, #> #   diff_2_1_AMZN <dbl>, diff_2_1_NFLX <dbl>, diff_2_1_GOOG <dbl>, #> #   diff_3_1_FB <dbl>, diff_3_1_AMZN <dbl>, diff_3_1_NFLX <dbl>, #> #   diff_3_1_GOOG <dbl>   # Get information with tidy ----  recipe_diff %>% tidy() #> # A tibble: 1 × 6 #>   number operation type  trained skip  id         #>    <int> <chr>     <chr> <lgl>   <lgl> <chr>      #> 1      1 step      diff  TRUE    FALSE diff_aMfvs  recipe_diff %>% tidy(1) #> # A tibble: 12 × 5 #>    terms           lag  diff log   id         #>    <chr>         <int> <dbl> <lgl> <chr>      #>  1 diff_FB_1_1       1     1 FALSE diff_aMfvs #>  2 diff_AMZN_1_1     1     1 FALSE diff_aMfvs #>  3 diff_NFLX_1_1     1     1 FALSE diff_aMfvs #>  4 diff_GOOG_1_1     1     1 FALSE diff_aMfvs #>  5 diff_FB_2_1       2     1 FALSE diff_aMfvs #>  6 diff_AMZN_2_1     2     1 FALSE diff_aMfvs #>  7 diff_NFLX_2_1     2     1 FALSE diff_aMfvs #>  8 diff_GOOG_2_1     2     1 FALSE diff_aMfvs #>  9 diff_FB_3_1       3     1 FALSE diff_aMfvs #> 10 diff_AMZN_3_1     3     1 FALSE diff_aMfvs #> 11 diff_NFLX_3_1     3     1 FALSE diff_aMfvs #> 12 diff_GOOG_3_1     3     1 FALSE diff_aMfvs"},{"path":"https://business-science.github.io/timetk/reference/step_fourier.html","id":null,"dir":"Reference","previous_headings":"","what":"Fourier Features for Modeling Seasonality — step_fourier","title":"Fourier Features for Modeling Seasonality — step_fourier","text":"step_fourier creates specification recipe step convert Date Date-time column Fourier series","code":""},{"path":"https://business-science.github.io/timetk/reference/step_fourier.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fourier Features for Modeling Seasonality — step_fourier","text":"","code":"step_fourier(   recipe,   ...,   period,   K,   role = \"predictor\",   trained = FALSE,   columns = NULL,   scale_factor = NULL,   skip = FALSE,   id = rand_id(\"fourier\") )  # S3 method for class 'step_fourier' tidy(x, ...)"},{"path":"https://business-science.github.io/timetk/reference/step_fourier.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fourier Features for Modeling Seasonality — step_fourier","text":"recipe recipe object. step added sequence operations recipe. ... single column class Date POSIXct. See recipes::selections() details. tidy method, currently used. period numeric period oscillation frequency. See details examples period specification. K number orders include sine/cosine fourier series. orders increase number fourier terms therefore variance fitted model expense bias. See details examples K specification. role model terms created step, analysis role assigned?. default, function assumes new variable columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variables used inputs. field placeholder populated recipes::prep() used. scale_factor factor scaling numeric index extracted date date-time feature. placeholder populated recipes::prep() used. skip logical. step skipped recipe baked bake.recipe()? operations baked prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify . x step_fourier object.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_fourier.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fourier Features for Modeling Seasonality — step_fourier","text":"step_fourier, updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables selected), value (feature names).","code":""},{"path":"https://business-science.github.io/timetk/reference/step_fourier.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fourier Features for Modeling Seasonality — step_fourier","text":"Date Variable Unlike steps, step_fourier remove original date variables. recipes::step_rm() can used purpose. Period Specification period argument used generate distance peaks fourier sequence. key line peaks unique seasonalities data. Daily Data, typical period specifications : Yearly frequency 365 Quarterly frequency 365 / 4 = 91.25 Monthly frequency 365 / 12 = 30.42 K Specification K argument specifies maximum number orders Fourier terms. Examples: Specifying period = 365 K = 1 return cos365_K1 sin365_K1 fourier series Specifying period = 365 K = 2 return cos365_K1, cos365_K2, sin365_K1 sin365_K2 sequence, tends increase models ability fit vs K = 1 specification (expense possibly overfitting). Multiple values period K possible specify multiple values period single step step_fourier(period = c(91.25, 365), K = 2. returns 8 Fouriers series: cos91.25_K1, sin91.25_K1, cos91.25_K2, sin91.25_K2 cos365_K1, sin365_K1, cos365_K2, sin365_K2","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/step_fourier.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fourier Features for Modeling Seasonality — step_fourier","text":"","code":"library(recipes) library(dplyr)  FB_tbl <- FANG %>%     filter(symbol == \"FB\") %>%     select(symbol, date, adjusted)  # Create a recipe object with a timeseries signature step # - 252 Trade days per year # - period = c(252/4, 252): Adds quarterly and yearly fourier series # - K = 2: Adds 1st and 2nd fourier orders  rec_obj <- recipe(adjusted ~ ., data = FB_tbl) %>%     step_fourier(date, period = c(252/4, 252), K = 2)  # View the recipe object rec_obj #>  #> ── Recipe ────────────────────────────────────────────────────────────────────── #>  #> ── Inputs  #> Number of variables by role #> outcome:   1 #> predictor: 2 #>  #> ── Operations  #> • Fourier series features from: date  # Prepare the recipe object prep(rec_obj) #>  #> ── Recipe ────────────────────────────────────────────────────────────────────── #>  #> ── Inputs  #> Number of variables by role #> outcome:   1 #> predictor: 2 #>  #> ── Training information  #> Training data contained 1008 data points and no incomplete rows. #>  #> ── Operations  #> • Fourier series features from: date | Trained  # Bake the recipe object - Adds the Fourier Series bake(prep(rec_obj), FB_tbl) #> # A tibble: 1,008 × 11 #>    symbol date       adjusted date_sin63_K1 date_cos63_K1 date_sin63_K2 #>    <fct>  <date>        <dbl>         <dbl>         <dbl>         <dbl> #>  1 FB     2013-01-02     28          0.912         -0.411       -0.750  #>  2 FB     2013-01-03     27.8        0.866         -0.500       -0.866  #>  3 FB     2013-01-04     28.8        0.812         -0.584       -0.948  #>  4 FB     2013-01-07     29.4        0.604         -0.797       -0.963  #>  5 FB     2013-01-08     29.1        0.521         -0.853       -0.890  #>  6 FB     2013-01-09     30.6        0.434         -0.901       -0.782  #>  7 FB     2013-01-10     31.3        0.342         -0.940       -0.643  #>  8 FB     2013-01-11     31.7        0.247         -0.969       -0.478  #>  9 FB     2013-01-14     31.0       -0.0498        -0.999        0.0996 #> 10 FB     2013-01-15     30.1       -0.149         -0.989        0.295  #> # ℹ 998 more rows #> # ℹ 5 more variables: date_cos63_K2 <dbl>, date_sin252_K1 <dbl>, #> #   date_cos252_K1 <dbl>, date_sin252_K2 <dbl>, date_cos252_K2 <dbl>  # Tidy shows which features have been added during the 1st step #  in this case, step 1 is the step_timeseries_signature step tidy(prep(rec_obj)) #> # A tibble: 1 × 6 #>   number operation type    trained skip  id            #>    <int> <chr>     <chr>   <lgl>   <lgl> <chr>         #> 1      1 step      fourier TRUE    FALSE fourier_NaBGg tidy(prep(rec_obj), number = 1) #> # A tibble: 8 × 5 #>   terms          type      K period id            #>   <chr>          <chr> <int>  <dbl> <chr>         #> 1 date_sin63_K1  sin       1     63 fourier_NaBGg #> 2 date_cos63_K1  cos       1     63 fourier_NaBGg #> 3 date_sin63_K2  sin       2     63 fourier_NaBGg #> 4 date_cos63_K2  cos       2     63 fourier_NaBGg #> 5 date_sin252_K1 sin       1    252 fourier_NaBGg #> 6 date_cos252_K1 cos       1    252 fourier_NaBGg #> 7 date_sin252_K2 sin       2    252 fourier_NaBGg #> 8 date_cos252_K2 cos       2    252 fourier_NaBGg"},{"path":"https://business-science.github.io/timetk/reference/step_holiday_signature.html","id":null,"dir":"Reference","previous_headings":"","what":"Holiday Feature (Signature) Generator — step_holiday_signature","title":"Holiday Feature (Signature) Generator — step_holiday_signature","text":"step_holiday_signature creates specification recipe step convert date date-time data many holiday features can aid machine learning time-series data. default, many features returned different holidays, locales, stock exchanges.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_holiday_signature.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Holiday Feature (Signature) Generator — step_holiday_signature","text":"","code":"step_holiday_signature(   recipe,   ...,   holiday_pattern = \".\",   locale_set = \"all\",   exchange_set = \"all\",   role = \"predictor\",   trained = FALSE,   columns = NULL,   features = NULL,   skip = FALSE,   id = rand_id(\"holiday_signature\") )  # S3 method for class 'step_holiday_signature' tidy(x, ...)"},{"path":"https://business-science.github.io/timetk/reference/step_holiday_signature.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Holiday Feature (Signature) Generator — step_holiday_signature","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables used create new variables. selected variables class Date POSIXct. See recipes::selections() details. tidy method, currently used. holiday_pattern regular expression pattern search \"Holiday Set\". locale_set Return binary holidays based locale. One : \"\", \"none\", \"World\", \"US\", \"CA\", \"GB\", \"FR\", \"\", \"JP\", \"CH\", \"DE\". exchange_set Return binary holidays based Stock Exchange Calendars. One : \"\", \"none\", \"NYSE\", \"LONDON\", \"NERC\", \"TSX\", \"ZURICH\". role model terms created step, analysis role assigned?. default, function assumes new variable columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variables used inputs. field placeholder populated recipes::prep() used. features character string features generated. field placeholder populated recipes::prep() used. skip logical. step skipped recipe baked bake.recipe()? operations baked prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify . x step_holiday_signature object.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_holiday_signature.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Holiday Feature (Signature) Generator — step_holiday_signature","text":"step_holiday_signature, updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables selected), value (feature names).","code":""},{"path":"https://business-science.github.io/timetk/reference/step_holiday_signature.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Holiday Feature (Signature) Generator — step_holiday_signature","text":"Use Holiday Pattern Feature Sets Pare Features default, going get LOT Features. good thing many machine learning algorithms regularization built . , many cases still want reduce number unnecessary features. : Holiday Pattern: Regular Expression pattern can used filter. Try holiday_pattern = \"(US_Christ)|(US_Thanks)\" return just Christmas Thanksgiving features. Locale Sets: logical whether locale holiday. locales outside US may want combine multiple locales. example, locale_set = c(\"World\", \"GB\") returns World Holidays Great Britain. Exchange Sets: logical whether Business due holiday. Different Stock Exchanges used proxy business holiday calendars. example, exchange_set = \"NYSE\" returns business holidays New York Stock Exchange. Removing Unnecessary Features default, many features created automatically. Unnecessary features can removed using recipes::step_rm() recipes::selections() details.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/step_holiday_signature.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Holiday Feature (Signature) Generator — step_holiday_signature","text":"","code":"library(recipes) library(dplyr)  # Sample Data dates_in_2017_tbl <- tibble::tibble(     index = tk_make_timeseries(\"2017-01-01\", \"2017-12-31\", by = \"day\") )  # Add US holidays and Non-Working Days due to Holidays # - Physical Holidays are added with holiday pattern (individual) and locale_set rec_holiday <- recipe(~ ., dates_in_2017_tbl) %>%     step_holiday_signature(index,                            holiday_pattern = \"^US_\",                            locale_set      = \"US\",                            exchange_set    = \"NYSE\")  # Not yet prep'ed - just returns parameters selected rec_holiday %>% tidy(1) #> # A tibble: 3 × 4 #>   terms param           value id                      #>   <chr> <chr>           <chr> <chr>                   #> 1 index holiday_pattern ^US_  holiday_signature_OPbLM #> 2 index locale_set      US    holiday_signature_OPbLM #> 3 index exchange_set    NYSE  holiday_signature_OPbLM  # Prep the recipe rec_holiday_prep <- prep(rec_holiday)  # Now prep'ed - returns new features that will be created rec_holiday_prep %>% tidy(1) #> # A tibble: 19 × 3 #>    terms value                    id                      #>    <fct> <fct>                    <chr>                   #>  1 index exch_NYSE                holiday_signature_OPbLM #>  2 index locale_US                holiday_signature_OPbLM #>  3 index US_NewYearsDay           holiday_signature_OPbLM #>  4 index US_MLKingsBirthday       holiday_signature_OPbLM #>  5 index US_InaugurationDay       holiday_signature_OPbLM #>  6 index US_LincolnsBirthday      holiday_signature_OPbLM #>  7 index US_PresidentsDay         holiday_signature_OPbLM #>  8 index US_WashingtonsBirthday   holiday_signature_OPbLM #>  9 index US_CPulaskisBirthday     holiday_signature_OPbLM #> 10 index US_GoodFriday            holiday_signature_OPbLM #> 11 index US_DecorationMemorialDay holiday_signature_OPbLM #> 12 index US_MemorialDay           holiday_signature_OPbLM #> 13 index US_IndependenceDay       holiday_signature_OPbLM #> 14 index US_LaborDay              holiday_signature_OPbLM #> 15 index US_ColumbusDay           holiday_signature_OPbLM #> 16 index US_ElectionDay           holiday_signature_OPbLM #> 17 index US_VeteransDay           holiday_signature_OPbLM #> 18 index US_ThanksgivingDay       holiday_signature_OPbLM #> 19 index US_ChristmasDay          holiday_signature_OPbLM  # Apply the recipe to add new holiday features! bake(rec_holiday_prep, dates_in_2017_tbl) #> # A tibble: 365 × 20 #>    index      index_exch_NYSE index_locale_US index_US_NewYearsDay #>    <date>               <dbl>           <dbl>                <dbl> #>  1 2017-01-01               0               1                    1 #>  2 2017-01-02               1               0                    0 #>  3 2017-01-03               0               0                    0 #>  4 2017-01-04               0               0                    0 #>  5 2017-01-05               0               0                    0 #>  6 2017-01-06               0               0                    0 #>  7 2017-01-07               0               0                    0 #>  8 2017-01-08               0               0                    0 #>  9 2017-01-09               0               0                    0 #> 10 2017-01-10               0               0                    0 #> # ℹ 355 more rows #> # ℹ 16 more variables: index_US_MLKingsBirthday <dbl>, #> #   index_US_InaugurationDay <dbl>, index_US_LincolnsBirthday <dbl>, #> #   index_US_PresidentsDay <dbl>, index_US_WashingtonsBirthday <dbl>, #> #   index_US_CPulaskisBirthday <dbl>, index_US_GoodFriday <dbl>, #> #   index_US_MemorialDay <dbl>, index_US_DecorationMemorialDay <dbl>, #> #   index_US_IndependenceDay <dbl>, index_US_LaborDay <dbl>, …"},{"path":"https://business-science.github.io/timetk/reference/step_log_interval.html","id":null,"dir":"Reference","previous_headings":"","what":"Log Interval Transformation for Constrained Interval Forecasting — step_log_interval","title":"Log Interval Transformation for Constrained Interval Forecasting — step_log_interval","text":"step_log_interval creates specification recipe step transform data using Log-Inerval transformation. function provides recipes interface log_interval_vec() transformation function.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_log_interval.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log Interval Transformation for Constrained Interval Forecasting — step_log_interval","text":"","code":"step_log_interval(   recipe,   ...,   limit_lower = \"auto\",   limit_upper = \"auto\",   offset = 0,   role = NA,   trained = FALSE,   limit_lower_trained = NULL,   limit_upper_trained = NULL,   skip = FALSE,   id = rand_id(\"log_interval\") )  # S3 method for class 'step_log_interval' tidy(x, ...)"},{"path":"https://business-science.github.io/timetk/reference/step_log_interval.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log Interval Transformation for Constrained Interval Forecasting — step_log_interval","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. tidy method, currently used. limit_lower lower limit. Must less minimum value. set \"auto\", selects zero. limit_upper upper limit. Must greater maximum value. set \"auto\",  selects value 10% greater maximum value. offset offset include log transformation. Useful data contains values less equal zero. role used step since new variables created. trained logical indicate quantities preprocessing estimated. limit_lower_trained numeric vector transformation values. NULL computed prep(). limit_upper_trained numeric vector transformation values. NULL computed prep(). skip logical. step skipped recipe baked bake.recipe()? operations baked prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify . x step_log_interval object.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_log_interval.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Log Interval Transformation for Constrained Interval Forecasting — step_log_interval","text":"updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables selected) value (lambda estimate).","code":""},{"path":"https://business-science.github.io/timetk/reference/step_log_interval.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Log Interval Transformation for Constrained Interval Forecasting — step_log_interval","text":"step_log_interval() function designed specifically handle time series using methods implemented Forecast R Package. Positive Data data includes values zero, use offset adjust series make values positive. Implementation Refer log_interval_vec() function transformation implementation details.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/step_log_interval.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Log Interval Transformation for Constrained Interval Forecasting — step_log_interval","text":"","code":"library(dplyr) library(recipes)  FANG_wide <- FANG %>%     select(symbol, date, adjusted) %>%     tidyr::pivot_wider(names_from = symbol, values_from = adjusted)  recipe_log_interval <- recipe(~ ., data = FANG_wide) %>%     step_log_interval(FB, AMZN, NFLX, GOOG, offset = 1) %>%     prep() #> $FB #> [1] 0 #>  #> $FB #> [1] 145.318 #>  #> $AMZN #> [1] 0 #>  #> $AMZN #> [1] 904.973 #>  #> $NFLX #> [1] 0 #>  #> $NFLX #> [1] 143.7086 #>  #> $GOOG #> [1] 0 #>  #> $GOOG #> [1] 860.3125 #>   recipe_log_interval %>%     bake(FANG_wide) %>%     tidyr::pivot_longer(-date) %>%     plot_time_series(date, value, name, .smooth = FALSE, .interactive = FALSE) #> $FB #> [1] 0 #>  #> $FB #> [1] 145.318 #>  #> $AMZN #> [1] 0 #>  #> $AMZN #> [1] 904.973 #>  #> $NFLX #> [1] 0 #>  #> $NFLX #> [1] 143.7086 #>  #> $GOOG #> [1] 0 #>  #> $GOOG #> [1] 860.3125 #>    recipe_log_interval %>% tidy(1) #> # A tibble: 4 × 5 #>   terms limit_lower limit_upper offset id                 #>   <chr>       <dbl>       <dbl>  <dbl> <chr>              #> 1 FB              0        145.      1 log_interval_71neM #> 2 AMZN            0        905.      1 log_interval_71neM #> 3 NFLX            0        144.      1 log_interval_71neM #> 4 GOOG            0        860.      1 log_interval_71neM"},{"path":"https://business-science.github.io/timetk/reference/step_slidify.html","id":null,"dir":"Reference","previous_headings":"","what":"Slidify Rolling Window Transformation — step_slidify","title":"Slidify Rolling Window Transformation — step_slidify","text":"step_slidify creates specification recipe step apply function one Numeric column(s).","code":""},{"path":"https://business-science.github.io/timetk/reference/step_slidify.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Slidify Rolling Window Transformation — step_slidify","text":"","code":"step_slidify(   recipe,   ...,   period,   .f,   align = c(\"center\", \"left\", \"right\"),   partial = FALSE,   names = NULL,   role = \"predictor\",   trained = FALSE,   columns = NULL,   f_name = NULL,   skip = FALSE,   id = rand_id(\"slidify\") )  # S3 method for class 'step_slidify' tidy(x, ...)"},{"path":"https://business-science.github.io/timetk/reference/step_slidify.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Slidify Rolling Window Transformation — step_slidify","text":"recipe recipe object. step added sequence operations recipe. ... One numeric columns smoothed. See recipes::selections() details. tidy method, currently used. period number periods include local rolling window. effectively \"window size\". .f summary formula one following formats: mean arguments function(x) mean(x, na.rm = TRUE) ~ mean(.x, na.rm = TRUE), converted function. align Rolling functions generate period - 1 fewer values incoming vector. Thus, vector needs aligned. Alignment vector follows 3 types: Center: NA .partial values divided added beginning end series \"Center\" moving average. common de-noising operations. See also [smooth_vec()] LOESS without NA values. Left: NA .partial values added end shift series Left. Right: NA .partial values added beginning shif series Right. common Financial Applications moving average cross-overs. partial moving window allowed return partial (incomplete) windows instead NA values. Set FALSE default, can switched TRUE remove NA's. names optional character string length number terms selected terms. names new columns created step. NULL, existing columns transformed. NULL, new columns created. role model terms created step, analysis role assigned?. default, function assumes new variable columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variables used inputs. field placeholder populated recipes::prep() used. f_name character string function applied. field placeholder populated tidy() step. skip logical. step skipped recipe baked bake.recipe()? operations baked prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify . x step_slidify object.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_slidify.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Slidify Rolling Window Transformation — step_slidify","text":"step_slidify, updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables selected), value (feature names).","code":""},{"path":"https://business-science.github.io/timetk/reference/step_slidify.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Slidify Rolling Window Transformation — step_slidify","text":"Alignment Rolling functions generate period - 1 fewer values incoming vector. Thus, vector needs aligned. Alignment vector follows 3 types: Center: NA partial values divided added beginning end series \"Center\" moving average. common de-noising operations. See also [smooth_vec()] LOESS without NA values. Left: NA partial values added end shift series Left. Right: NA partial values added beginning shif series Right. common Financial Applications moving average cross-overs. Partial Values advantage using partial values vs NA padding series can filled (good time-series de-noising operations). downside partial values partials can become less stable regions incomplete windows used. instability desirable de-noising operations, suitable alternative step_smooth(), implements local polynomial regression.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/step_slidify.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Slidify Rolling Window Transformation — step_slidify","text":"","code":"library(recipes) library(dplyr) library(ggplot2)  # Training Data FB_tbl <- FANG %>%     filter(symbol == \"FB\") %>%     select(symbol, date, adjusted)  # New Data - Make some fake new data next 90 time stamps new_data <- FB_tbl %>%     tail(90) %>%     mutate(date = date %>% tk_make_future_timeseries(length_out = 90))  # OVERWRITE EXISTING COLUMNS -----  # Create a recipe object with a step_slidify rec_ma_50 <- recipe(adjusted ~ ., data = FB_tbl) %>%     step_slidify(adjusted, period = 50, .f = ~ mean(.x))  # Bake the recipe object - Applies the Moving Average Transformation training_data_baked <- bake(prep(rec_ma_50), FB_tbl)  # Apply to New Data new_data_baked <- bake(prep(rec_ma_50), new_data)  # Visualize effect training_data_baked %>%     ggplot(aes(date, adjusted)) +     geom_line() +     geom_line(color = \"red\", data = new_data_baked) #> Warning: Removed 49 rows containing missing values or values outside the scale range #> (`geom_line()`). #> Warning: Removed 49 rows containing missing values or values outside the scale range #> (`geom_line()`).   # ---- NEW COLUMNS ---- # Use the `names` argument to create new columns instead of overwriting existing  rec_ma_30_names <- recipe(adjusted ~ ., data = FB_tbl) %>%     step_slidify(adjusted, period = 30, .f = mean, names = \"adjusted_ma_30\")  bake(prep(rec_ma_30_names), FB_tbl) %>%     ggplot(aes(date, adjusted)) +     geom_line(alpha = 0.5) +     geom_line(aes(y = adjusted_ma_30), color = \"red\", size = 1) #> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #> ℹ Please use `linewidth` instead. #> Warning: Removed 29 rows containing missing values or values outside the scale range #> (`geom_line()`)."},{"path":"https://business-science.github.io/timetk/reference/step_slidify_augment.html","id":null,"dir":"Reference","previous_headings":"","what":"Slidify Rolling Window Transformation (Augmented Version) — step_slidify_augment","title":"Slidify Rolling Window Transformation (Augmented Version) — step_slidify_augment","text":"step_slidify_augment creates specification recipe step \"augment\" (add multiple new columns) sliding function applied.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_slidify_augment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Slidify Rolling Window Transformation (Augmented Version) — step_slidify_augment","text":"","code":"step_slidify_augment(   recipe,   ...,   period,   .f,   align = c(\"center\", \"left\", \"right\"),   partial = FALSE,   prefix = \"slidify_\",   role = \"predictor\",   trained = FALSE,   columns = NULL,   f_name = NULL,   skip = FALSE,   id = rand_id(\"slidify_augment\") )  # S3 method for class 'step_slidify_augment' tidy(x, ...)"},{"path":"https://business-science.github.io/timetk/reference/step_slidify_augment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Slidify Rolling Window Transformation (Augmented Version) — step_slidify_augment","text":"recipe recipe object. step added sequence operations recipe. ... One numeric columns smoothed. See recipes::selections() details. tidy method, currently used. period number periods include local rolling window. effectively \"window size\". .f summary formula one following formats: mean arguments function(x) mean(x, na.rm = TRUE) ~ mean(.x, na.rm = TRUE), converted function. align Rolling functions generate period - 1 fewer values incoming vector. Thus, vector needs aligned. Alignment vector follows 3 types: Center: NA .partial values divided added beginning end series \"Center\" moving average. common de-noising operations. See also [smooth_vec()] LOESS without NA values. Left: NA .partial values added end shift series Left. Right: NA .partial values added beginning shif series Right. common Financial Applications moving average cross-overs. partial moving window allowed return partial (incomplete) windows instead NA values. Set FALSE default, can switched TRUE remove NA's. prefix prefix generated column names, default \"slidify_\". role model terms created step, analysis role assigned?. default, function assumes new variable columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. f_name character string function applied. field placeholder populated tidy() step. skip logical. step skipped recipe baked bake.recipe()? operations baked prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations id character string unique step identify . x step_slidify_augment object.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_slidify_augment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Slidify Rolling Window Transformation (Augmented Version) — step_slidify_augment","text":"step_slidify_augment, updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables selected), value (feature names).","code":""},{"path":"https://business-science.github.io/timetk/reference/step_slidify_augment.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Slidify Rolling Window Transformation (Augmented Version) — step_slidify_augment","text":"Alignment Rolling functions generate period - 1 fewer values incoming vector. Thus, vector needs aligned. Alignment vector follows 3 types: Center: NA partial values divided added beginning end series \"Center\" moving average. common de-noising operations. See also [smooth_vec()] LOESS without NA values. Left: NA partial values added end shift series Left. Right: NA partial values added beginning shif series Right. common Financial Applications moving average cross-overs. Partial Values advantage using partial values vs NA padding series can filled (good time-series de-noising operations). downside partial values partials can become less stable regions incomplete windows used. instability desirable de-noising operations, suitable alternative step_smooth(), implements local polynomial regression.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/step_slidify_augment.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Slidify Rolling Window Transformation (Augmented Version) — step_slidify_augment","text":"","code":"# library(tidymodels) library(dplyr) library(recipes) library(parsnip)  m750 <- m4_monthly %>%     filter(id == \"M750\") %>%     mutate(value_2 = value / 2)  m750_splits <- time_series_split(m750, assess = \"2 years\", cumulative = TRUE) #> Using date_var: date  # Make a recipe recipe_spec <- recipe(value ~ date + value_2, rsample::training(m750_splits)) %>%     step_slidify_augment(         value, value_2,         period = c(6, 12, 24),         .f = ~ mean(.x),         align = \"center\",         partial = FALSE     )  recipe_spec %>% prep() %>% juice() #> # A tibble: 282 × 9 #>    date       value_2 value slidify_6_value slidify_6_value_2 slidify_12_value #>    <date>       <dbl> <dbl>           <dbl>             <dbl>            <dbl> #>  1 1990-01-01    3185  6370             NA                NA               NA  #>  2 1990-02-01    3215  6430             NA                NA               NA  #>  3 1990-03-01    3260  6520           6535              3268.              NA  #>  4 1990-04-01    3290  6580           6473.             3237.              NA  #>  5 1990-05-01    3310  6620           6310              3155               NA  #>  6 1990-06-01    3345  6690           6303.             3152.            6481. #>  7 1990-07-01    3000  6000           6343.             3172.            6527. #>  8 1990-08-01    2725  5450           6383.             3192.            6567. #>  9 1990-09-01    3240  6480           6427.             3213.            6603. #> 10 1990-10-01    3410  6820           6580              3290             6638. #> # ℹ 272 more rows #> # ℹ 3 more variables: slidify_12_value_2 <dbl>, slidify_24_value <dbl>, #> #   slidify_24_value_2 <dbl>  bake(prep(recipe_spec), rsample::testing(m750_splits)) #> # A tibble: 24 × 9 #>    date       value_2 value slidify_6_value slidify_6_value_2 slidify_12_value #>    <date>       <dbl> <dbl>           <dbl>             <dbl>            <dbl> #>  1 2013-07-01    4515  9030             NA                NA               NA  #>  2 2013-08-01    4810  9620             NA                NA               NA  #>  3 2013-09-01    5025 10050          10107.             5053.              NA  #>  4 2013-10-01    5305 10610          10390              5195               NA  #>  5 2013-11-01    5380 10760          10563.             5282.              NA  #>  6 2013-12-01    5285 10570          10705              5352.           10472. #>  7 2014-01-01    5365 10730          10773.             5387.           10498. #>  8 2014-02-01    5330 10660          10803.             5402.           10521. #>  9 2014-03-01    5450 10900          10837.             5418.           10547. #> 10 2014-04-01    5510 11020          10605              5302.           10575  #> # ℹ 14 more rows #> # ℹ 3 more variables: slidify_12_value_2 <dbl>, slidify_24_value <dbl>, #> #   slidify_24_value_2 <dbl>"},{"path":"https://business-science.github.io/timetk/reference/step_smooth.html","id":null,"dir":"Reference","previous_headings":"","what":"Smoothing Transformation using Loess — step_smooth","title":"Smoothing Transformation using Loess — step_smooth","text":"step_smooth creates specification recipe step apply local polynomial regression one Numeric column(s). effect smoothing time series similar moving average without creating missing values using partial smoothing.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_smooth.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Smoothing Transformation using Loess — step_smooth","text":"","code":"step_smooth(   recipe,   ...,   period = 30,   span = NULL,   degree = 2,   names = NULL,   role = \"predictor\",   trained = FALSE,   columns = NULL,   skip = FALSE,   id = rand_id(\"smooth\") )  # S3 method for class 'step_smooth' tidy(x, ...)"},{"path":"https://business-science.github.io/timetk/reference/step_smooth.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Smoothing Transformation using Loess — step_smooth","text":"recipe recipe object. step added sequence operations recipe. ... One numeric columns smoothed. See recipes::selections() details. tidy method, currently used. period number periods include local smoothing. Similar window size moving average. See details explanation period vs span specification. span span percentage data included smoothing window. Period preferred shorter windows fix window size. See details explanation period vs span specification. degree degree polynomials used. Set 2 default 2nd order polynomial. names optional character string length number terms selected terms. names new columns created step. NULL, existing columns transformed. NULL, new columns created. role model terms created step, analysis role assigned?. default, function assumes new variable columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variables used inputs. field placeholder populated recipes::prep() used. skip logical. step skipped recipe baked bake.recipe()? operations baked prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify . x step_smooth object.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_smooth.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Smoothing Transformation using Loess — step_smooth","text":"step_smooth, updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables selected), value (feature names).","code":""},{"path":"https://business-science.github.io/timetk/reference/step_smooth.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Smoothing Transformation using Loess — step_smooth","text":"Smoother Algorithm function recipe specification wraps stats::loess() modification set fixed period rather percentage data points via span. Period vs Span? period fixed whereas span changes number observations change. use Period? effect using period similar Moving Average Window Size Fixed Period. helps trying smooth local trends. want 30-day moving average, specify period = 30. use Span? Span easier specify want Long-Term Trendline window size unknown. can specify span = 0.75 locally regress using window 75% data. Warning - Using Span New Data using span New Data, number observations likely different trained . means trendline / smoother can vastly different smoother trained . Solution Span New Data use span. Rather, use period fix window size. ensures new data includes number observations local polynomial regression (loess) training data.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/step_smooth.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Smoothing Transformation using Loess — step_smooth","text":"","code":"library(recipes) library(dplyr) library(ggplot2)  # Training Data FB_tbl <- FANG %>%     filter(symbol == \"FB\") %>%     select(symbol, date, adjusted)  # New Data - Make some fake new data next 90 time stamps new_data <- FB_tbl %>%     tail(90) %>%     mutate(date = date %>% tk_make_future_timeseries(length_out = 90))  # ---- PERIOD ----  # Create a recipe object with a step_smooth() rec_smooth_period <- recipe(adjusted ~ ., data = FB_tbl) %>%     step_smooth(adjusted, period = 30)  # Bake the recipe object - Applies the Loess Transformation training_data_baked <- bake(prep(rec_smooth_period), FB_tbl)  # \"Period\" Effect on New Data new_data_baked <- bake(prep(rec_smooth_period), new_data)  # Smoother's fit on new data is very similar because # 30 days are used in the new data regardless of the new data being 90 days training_data_baked %>%     ggplot(aes(date, adjusted)) +     geom_line() +     geom_line(color = \"red\", data = new_data_baked)   # ---- SPAN ----  # Create a recipe object with a step_smooth rec_smooth_span <- recipe(adjusted ~ ., data = FB_tbl) %>%     step_smooth(adjusted, span = 0.03)  # Bake the recipe object - Applies the Loess Transformation training_data_baked <- bake(prep(rec_smooth_span), FB_tbl)  # \"Period\" Effect on New Data new_data_baked <- bake(prep(rec_smooth_span), new_data) #> Warning: span too small.   fewer data values than degrees of freedom. #> Warning: pseudoinverse used at 0.555 #> Warning: neighborhood radius 1.445 #> Warning: reciprocal condition number  0 #> Warning: There are other near singularities as well. 2.088  # Smoother's fit is not the same using span because new data is only 90 days # and 0.03 x 90 = 2.7 days training_data_baked %>%     ggplot(aes(date, adjusted)) +     geom_line() +     geom_line(color = \"red\", data = new_data_baked)   # ---- NEW COLUMNS ---- # Use the `names` argument to create new columns instead of overwriting existing  rec_smooth_names <- recipe(adjusted ~ ., data = FB_tbl) %>%     step_smooth(adjusted, period = 30, names = \"adjusted_smooth_30\") %>%     step_smooth(adjusted, period = 180, names = \"adjusted_smooth_180\") %>%     step_smooth(adjusted, span = 0.75, names = \"long_term_trend\")  bake(prep(rec_smooth_names), FB_tbl) %>%     ggplot(aes(date, adjusted)) +     geom_line(alpha = 0.5) +     geom_line(aes(y = adjusted_smooth_30), color = \"red\", size = 1) +     geom_line(aes(y = adjusted_smooth_180), color = \"blue\", size = 1) +     geom_line(aes(y = long_term_trend), color = \"orange\", size = 1)"},{"path":"https://business-science.github.io/timetk/reference/step_timeseries_signature.html","id":null,"dir":"Reference","previous_headings":"","what":"Time Series Feature (Signature) Generator — step_timeseries_signature","title":"Time Series Feature (Signature) Generator — step_timeseries_signature","text":"step_timeseries_signature creates specification recipe step convert date date-time data many features can aid machine learning time-series data","code":""},{"path":"https://business-science.github.io/timetk/reference/step_timeseries_signature.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Time Series Feature (Signature) Generator — step_timeseries_signature","text":"","code":"step_timeseries_signature(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   columns = NULL,   skip = FALSE,   id = rand_id(\"timeseries_signature\") )  # S3 method for class 'step_timeseries_signature' tidy(x, ...)"},{"path":"https://business-science.github.io/timetk/reference/step_timeseries_signature.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Time Series Feature (Signature) Generator — step_timeseries_signature","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables used create new variables. selected variables class Date POSIXct. See recipes::selections() details. tidy method, currently used. role model terms created step, analysis role assigned?. default, function assumes new variable columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variables used inputs. field placeholder populated recipes::prep() used. skip logical. step skipped recipe baked bake.recipe()? operations baked prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify . x step_timeseries_signature object.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_timeseries_signature.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Time Series Feature (Signature) Generator — step_timeseries_signature","text":"step_timeseries_signature, updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables selected), value (feature names).","code":""},{"path":"https://business-science.github.io/timetk/reference/step_timeseries_signature.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Time Series Feature (Signature) Generator — step_timeseries_signature","text":"Date Variable Unlike steps, step_timeseries_signature remove original date variables. recipes::step_rm() can used purpose. Scaling index.num index.num feature created large magnitude (number seconds since 1970-01-01). good idea scale center feature (e.g. use  recipes::step_normalize()). Removing Unnecessary Features default, many features created automatically. Unnecessary features can removed using recipes::step_rm().","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/step_timeseries_signature.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Time Series Feature (Signature) Generator — step_timeseries_signature","text":"","code":"library(recipes) library(dplyr)  FB_tbl <- FANG %>% dplyr::filter(symbol == \"FB\")  # Create a recipe object with a timeseries signature step rec_obj <- recipe(adjusted ~ ., data = FB_tbl) %>%     step_timeseries_signature(date)  # View the recipe object rec_obj #>  #> ── Recipe ────────────────────────────────────────────────────────────────────── #>  #> ── Inputs  #> Number of variables by role #> outcome:   1 #> predictor: 7 #>  #> ── Operations  #> • Timeseries signature features from: date  # Prepare the recipe object prep(rec_obj) #>  #> ── Recipe ────────────────────────────────────────────────────────────────────── #>  #> ── Inputs  #> Number of variables by role #> outcome:   1 #> predictor: 7 #>  #> ── Training information  #> Training data contained 1008 data points and no incomplete rows. #>  #> ── Operations  #> • Timeseries signature features from: date | Trained  # Bake the recipe object - Adds the Time Series Signature bake(prep(rec_obj), FB_tbl) #> # A tibble: 1,008 × 35 #>    symbol date        open  high   low close    volume adjusted date_index.num #>    <fct>  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>          <dbl> #>  1 FB     2013-01-02  27.4  28.2  27.4  28    69846400     28       1357084800 #>  2 FB     2013-01-03  27.9  28.5  27.6  27.8  63140600     27.8     1357171200 #>  3 FB     2013-01-04  28.0  28.9  27.8  28.8  72715400     28.8     1357257600 #>  4 FB     2013-01-07  28.7  29.8  28.6  29.4  83781800     29.4     1357516800 #>  5 FB     2013-01-08  29.5  29.6  28.9  29.1  45871300     29.1     1357603200 #>  6 FB     2013-01-09  29.7  30.6  29.5  30.6 104787700     30.6     1357689600 #>  7 FB     2013-01-10  30.6  31.5  30.3  31.3  95316400     31.3     1357776000 #>  8 FB     2013-01-11  31.3  32.0  31.1  31.7  89598000     31.7     1357862400 #>  9 FB     2013-01-14  32.1  32.2  30.6  31.0  98892800     31.0     1358121600 #> 10 FB     2013-01-15  30.6  31.7  29.9  30.1 173242600     30.1     1358208000 #> # ℹ 998 more rows #> # ℹ 26 more variables: date_year <int>, date_year.iso <int>, date_half <int>, #> #   date_quarter <int>, date_month <int>, date_month.xts <int>, #> #   date_month.lbl <ord>, date_day <int>, date_hour <int>, date_minute <int>, #> #   date_second <int>, date_hour12 <int>, date_am.pm <int>, date_wday <int>, #> #   date_wday.xts <int>, date_wday.lbl <ord>, date_mday <int>, date_qday <int>, #> #   date_yday <int>, date_mweek <int>, date_week <int>, date_week.iso <int>, …  # Tidy shows which features have been added during the 1st step #  in this case, step 1 is the step_timeseries_signature step tidy(rec_obj) #> # A tibble: 1 × 6 #>   number operation type                 trained skip  id                         #>    <int> <chr>     <chr>                <lgl>   <lgl> <chr>                      #> 1      1 step      timeseries_signature FALSE   FALSE timeseries_signature_jRmVM tidy(rec_obj, number = 1) #> # A tibble: 27 × 3 #>    terms value     id                         #>    <fct> <fct>     <chr>                      #>  1 date  index.num timeseries_signature_jRmVM #>  2 date  year      timeseries_signature_jRmVM #>  3 date  year.iso  timeseries_signature_jRmVM #>  4 date  half      timeseries_signature_jRmVM #>  5 date  quarter   timeseries_signature_jRmVM #>  6 date  month     timeseries_signature_jRmVM #>  7 date  month.xts timeseries_signature_jRmVM #>  8 date  month.lbl timeseries_signature_jRmVM #>  9 date  day       timeseries_signature_jRmVM #> 10 date  hour      timeseries_signature_jRmVM #> # ℹ 17 more rows"},{"path":"https://business-science.github.io/timetk/reference/step_ts_clean.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean Outliers and Missing Data for Time Series — step_ts_clean","title":"Clean Outliers and Missing Data for Time Series — step_ts_clean","text":"step_ts_clean creates specification recipe step clean outliers impute time series data.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_ts_clean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean Outliers and Missing Data for Time Series — step_ts_clean","text":"","code":"step_ts_clean(   recipe,   ...,   period = 1,   lambda = \"auto\",   role = NA,   trained = FALSE,   lambdas_trained = NULL,   skip = FALSE,   id = rand_id(\"ts_clean\") )  # S3 method for class 'step_ts_clean' tidy(x, ...)"},{"path":"https://business-science.github.io/timetk/reference/step_ts_clean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean Outliers and Missing Data for Time Series — step_ts_clean","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. tidy method, currently used. period seasonal period use transformation. period = 1, linear interpolation performed. period > 1, robust STL decomposition first performed linear interpolation applied seasonally adjusted data. lambda box cox transformation parameter. set \"auto\", performs automated lambda selection. role used step since new variables created. trained logical indicate quantities preprocessing estimated. lambdas_trained named numeric vector lambdas. NULL computed recipes::prep(). Note , original data integers, mean converted integer maintain data type. skip logical. step skipped recipe baked bake.recipe()? operations baked prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify . x step_ts_clean object.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_ts_clean.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean Outliers and Missing Data for Time Series — step_ts_clean","text":"updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables selected) value (lambda estimate).","code":""},{"path":"https://business-science.github.io/timetk/reference/step_ts_clean.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Clean Outliers and Missing Data for Time Series — step_ts_clean","text":"step_ts_clean() function designed specifically handle time series using seasonal outlier detection methods implemented Forecast R Package. Cleaning Outliers #' Outliers replaced missing values using following methods: Non-Seasonal (period = 1): Uses stats::supsmu() Seasonal (period > 1): Uses forecast::mstl() robust = TRUE (robust STL decomposition) seasonal series. Imputation using Linear Interpolation Three circumstances cause strictly linear interpolation: Period 1: period = 1, seasonality interpreted therefore linear used. Number Non-Missing Values less 2-Periods: Insufficient values exist detect seasonality. Number Total Values less 3-Periods: Insufficient values exist detect seasonality. Seasonal Imputation using Linear Interpolation seasonal series period > 1, robust Seasonal Trend Loess (STL) decomposition first computed. linear interpolation applied seasonally adjusted data, seasonal component added back. Box Cox Transformation many circumstances, Box Cox transformation can help. Especially series multiplicative meaning variance grows exponentially. Box Cox transformation can automated setting lambda = \"auto\" can specified setting lambda = numeric value.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_ts_clean.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Clean Outliers and Missing Data for Time Series — step_ts_clean","text":"Forecast R Package Forecasting Principles & Practices: Dealing missing values outliers","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/step_ts_clean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean Outliers and Missing Data for Time Series — step_ts_clean","text":"","code":"library(dplyr) library(tidyr) library(recipes)  # Get missing values FANG_wide <- FANG %>%     select(symbol, date, adjusted) %>%     pivot_wider(names_from = symbol, values_from = adjusted) %>%     pad_by_time() #> .date_var is missing. Using: date #> pad applied on the interval: day  FANG_wide #> # A tibble: 1,459 × 5 #>    date          FB  AMZN  NFLX  GOOG #>    <date>     <dbl> <dbl> <dbl> <dbl> #>  1 2013-01-02  28    257.  13.1  361. #>  2 2013-01-03  27.8  258.  13.8  361. #>  3 2013-01-04  28.8  259.  13.7  369. #>  4 2013-01-05  NA     NA   NA     NA  #>  5 2013-01-06  NA     NA   NA     NA  #>  6 2013-01-07  29.4  268.  14.2  367. #>  7 2013-01-08  29.1  266.  13.9  366. #>  8 2013-01-09  30.6  266.  13.7  369. #>  9 2013-01-10  31.3  265.  14    370. #> 10 2013-01-11  31.7  268.  14.5  370. #> # ℹ 1,449 more rows  # Apply Imputation recipe_box_cox <- recipe(~ ., data = FANG_wide) %>%     step_ts_clean(FB, AMZN, NFLX, GOOG, period = 252) %>%     prep()  recipe_box_cox %>% bake(FANG_wide) #> # A tibble: 1,459 × 5 #>    date          FB  AMZN  NFLX  GOOG #>    <date>     <dbl> <dbl> <dbl> <dbl> #>  1 2013-01-02  28    257.  13.1  361. #>  2 2013-01-03  27.8  258.  13.8  361. #>  3 2013-01-04  28.8  259.  13.7  369. #>  4 2013-01-05  28.2  262.  14.1  365. #>  5 2013-01-06  28.4  264.  14.6  366. #>  6 2013-01-07  29.4  268.  14.2  367. #>  7 2013-01-08  29.1  266.  13.9  366. #>  8 2013-01-09  30.6  266.  13.7  369. #>  9 2013-01-10  31.3  265.  14    370. #> 10 2013-01-11  31.7  268.  14.5  370. #> # ℹ 1,449 more rows  # Lambda parameter used during imputation process recipe_box_cox %>% tidy(1) #> # A tibble: 4 × 3 #>   terms lambda id             #>   <chr>  <dbl> <chr>          #> 1 FB     0.912 ts_clean_acgWf #> 2 AMZN   0.557 ts_clean_acgWf #> 3 NFLX   0.532 ts_clean_acgWf #> 4 GOOG  -1.000 ts_clean_acgWf"},{"path":"https://business-science.github.io/timetk/reference/step_ts_impute.html","id":null,"dir":"Reference","previous_headings":"","what":"Missing Data Imputation for Time Series — step_ts_impute","title":"Missing Data Imputation for Time Series — step_ts_impute","text":"step_ts_impute creates specification recipe step impute time series data.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_ts_impute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Missing Data Imputation for Time Series — step_ts_impute","text":"","code":"step_ts_impute(   recipe,   ...,   period = 1,   lambda = NULL,   role = NA,   trained = FALSE,   lambdas_trained = NULL,   skip = FALSE,   id = rand_id(\"ts_impute\") )  # S3 method for class 'step_ts_impute' tidy(x, ...)"},{"path":"https://business-science.github.io/timetk/reference/step_ts_impute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Missing Data Imputation for Time Series — step_ts_impute","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. tidy method, currently used. period seasonal period use transformation. period = 1, linear interpolation performed. period > 1, robust STL decomposition first performed linear interpolation applied seasonally adjusted data. lambda box cox transformation parameter. set \"auto\", performs automated lambda selection. role used step since new variables created. trained logical indicate quantities preprocessing estimated. lambdas_trained named numeric vector lambdas. NULL computed recipes::prep(). Note , original data integers, mean converted integer maintain data type. skip logical. step skipped recipe baked bake.recipe()? operations baked prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify . x step_ts_impute object.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_ts_impute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Missing Data Imputation for Time Series — step_ts_impute","text":"updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables selected) value (lambda estimate).","code":""},{"path":"https://business-science.github.io/timetk/reference/step_ts_impute.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Missing Data Imputation for Time Series — step_ts_impute","text":"step_ts_impute() function designed specifically handle time series Imputation using Linear Interpolation Three circumstances cause strictly linear interpolation: Period 1: period = 1, seasonality interpreted therefore linear used. Number Non-Missing Values less 2-Periods: Insufficient values exist detect seasonality. Number Total Values less 3-Periods: Insufficient values exist detect seasonality. Seasonal Imputation using Linear Interpolation seasonal series period > 1, robust Seasonal Trend Loess (STL) decomposition first computed. linear interpolation applied seasonally adjusted data, seasonal component added back. Box Cox Transformation many circumstances, Box Cox transformation can help. Especially series multiplicative meaning variance grows exponentially. Box Cox transformation can automated setting lambda = \"auto\" can specified setting lambda = numeric value.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_ts_impute.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Missing Data Imputation for Time Series — step_ts_impute","text":"Forecast R Package Forecasting Principles & Practices: Dealing missing values outliers","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/step_ts_impute.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Missing Data Imputation for Time Series — step_ts_impute","text":"","code":"library(dplyr) library(recipes)  # Get missing values FANG_wide <- FANG %>%     select(symbol, date, adjusted) %>%     tidyr::pivot_wider(names_from = symbol, values_from = adjusted) %>%     pad_by_time() #> .date_var is missing. Using: date #> pad applied on the interval: day  FANG_wide #> # A tibble: 1,459 × 5 #>    date          FB  AMZN  NFLX  GOOG #>    <date>     <dbl> <dbl> <dbl> <dbl> #>  1 2013-01-02  28    257.  13.1  361. #>  2 2013-01-03  27.8  258.  13.8  361. #>  3 2013-01-04  28.8  259.  13.7  369. #>  4 2013-01-05  NA     NA   NA     NA  #>  5 2013-01-06  NA     NA   NA     NA  #>  6 2013-01-07  29.4  268.  14.2  367. #>  7 2013-01-08  29.1  266.  13.9  366. #>  8 2013-01-09  30.6  266.  13.7  369. #>  9 2013-01-10  31.3  265.  14    370. #> 10 2013-01-11  31.7  268.  14.5  370. #> # ℹ 1,449 more rows  # Apply Imputation recipe_box_cox <- recipe(~ ., data = FANG_wide) %>%     step_ts_impute(FB, AMZN, NFLX, GOOG, period = 252, lambda = \"auto\") %>%     prep()  recipe_box_cox %>% bake(FANG_wide) #> # A tibble: 1,459 × 5 #>    date          FB  AMZN  NFLX  GOOG #>    <date>     <dbl> <dbl> <dbl> <dbl> #>  1 2013-01-02  28    257.  13.1  361. #>  2 2013-01-03  27.8  258.  13.8  361. #>  3 2013-01-04  28.8  259.  13.7  369. #>  4 2013-01-05  28.2  262.  14.1  365. #>  5 2013-01-06  28.4  264.  14.6  366. #>  6 2013-01-07  29.4  268.  14.2  367. #>  7 2013-01-08  29.1  266.  13.9  366. #>  8 2013-01-09  30.6  266.  13.7  369. #>  9 2013-01-10  31.3  265.  14    370. #> 10 2013-01-11  31.7  268.  14.5  370. #> # ℹ 1,449 more rows  # Lambda parameter used during imputation process recipe_box_cox %>% tidy(1) #> # A tibble: 4 × 3 #>   terms lambda id              #>   <chr>  <dbl> <chr>           #> 1 FB     0.912 ts_impute_MkPN1 #> 2 AMZN   0.557 ts_impute_MkPN1 #> 3 NFLX   0.532 ts_impute_MkPN1 #> 4 GOOG  -1.000 ts_impute_MkPN1"},{"path":"https://business-science.github.io/timetk/reference/step_ts_pad.html","id":null,"dir":"Reference","previous_headings":"","what":"Pad: Add rows to fill gaps and go from low to high frequency — step_ts_pad","title":"Pad: Add rows to fill gaps and go from low to high frequency — step_ts_pad","text":"step_ts_pad creates specification recipe step analyze Date Date-time column adding rows specified interval.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_ts_pad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pad: Add rows to fill gaps and go from low to high frequency — step_ts_pad","text":"","code":"step_ts_pad(   recipe,   ...,   by = \"day\",   pad_value = NA,   role = \"predictor\",   trained = FALSE,   columns = NULL,   skip = FALSE,   id = rand_id(\"ts_padding\") )  # S3 method for class 'step_ts_pad' tidy(x, ...)"},{"path":"https://business-science.github.io/timetk/reference/step_ts_pad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pad: Add rows to fill gaps and go from low to high frequency — step_ts_pad","text":"recipe recipe object. step added sequence operations recipe. ... single column class Date POSIXct. See recipes::selections() details. tidy method, currently used. Either \"auto\", time-based frequency like \"year\", \"month\", \"day\", \"hour\", etc, time expression like \"5 min\", \"7 days\". See Details. pad_value Fills padded values. Default NA. role model terms created step, analysis role assigned?. default, function assumes new variable columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variables used inputs. field placeholder populated recipes::prep() used. skip logical. step skipped recipe baked bake.recipe()? operations baked prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify . x step_ts_pad object.","code":""},{"path":"https://business-science.github.io/timetk/reference/step_ts_pad.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pad: Add rows to fill gaps and go from low to high frequency — step_ts_pad","text":"step_ts_pad, updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables selected), value (feature names).","code":""},{"path":"https://business-science.github.io/timetk/reference/step_ts_pad.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Pad: Add rows to fill gaps and go from low to high frequency — step_ts_pad","text":"Date Variable one date date-time variable may supplied. step_ts_pad()) remove original date variables. Interval Specification () Padding can applied following ways: eight intervals : year, quarter, month, week, day, hour, min, sec. Intervals like 30 minutes, 1 hours, 14 days possible. Imputing Missing Values generic pad_value defaults NA, typically requires imputation. common strategies include: Numeric data: step_ts_impute() preprocessing step can used impute numeric time series data without seasonality Nominal data: step_mode_impute() preprocessing step can used replace missing values common value.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/step_ts_pad.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Pad: Add rows to fill gaps and go from low to high frequency — step_ts_pad","text":"","code":"library(recipes) library(dplyr)  FB_tbl <- FANG %>%     filter(symbol == \"FB\") %>%     select(symbol, date, adjusted)   rec_obj <- recipe(adjusted ~ ., data = FB_tbl) %>%     step_ts_pad(date, by = \"day\", pad_value = NA)  # View the recipe object rec_obj #>  #> ── Recipe ────────────────────────────────────────────────────────────────────── #>  #> ── Inputs  #> Number of variables by role #> outcome:   1 #> predictor: 2 #>  #> ── Operations  #> • Padded time series features from: date  # Prepare the recipe object prep(rec_obj) #>  #> ── Recipe ────────────────────────────────────────────────────────────────────── #>  #> ── Inputs  #> Number of variables by role #> outcome:   1 #> predictor: 2 #>  #> ── Training information  #> Training data contained 1008 data points and no incomplete rows. #>  #> ── Operations  #> • Padded time series features from: date | Trained  # Bake the recipe object - Adds the padding bake(prep(rec_obj), FB_tbl) #> # A tibble: 1,459 × 3 #>    symbol date       adjusted #>    <fct>  <date>        <dbl> #>  1 FB     2013-01-02     28   #>  2 FB     2013-01-03     27.8 #>  3 FB     2013-01-04     28.8 #>  4 NA     2013-01-05     NA   #>  5 NA     2013-01-06     NA   #>  6 FB     2013-01-07     29.4 #>  7 FB     2013-01-08     29.1 #>  8 FB     2013-01-09     30.6 #>  9 FB     2013-01-10     31.3 #> 10 FB     2013-01-11     31.7 #> # ℹ 1,449 more rows  # Tidy shows which features have been added during the 1st step #  in this case, step 1 is the step_timeseries_signature step tidy(prep(rec_obj)) #> # A tibble: 1 × 6 #>   number operation type   trained skip  id               #>    <int> <chr>     <chr>  <lgl>   <lgl> <chr>            #> 1      1 step      ts_pad TRUE    FALSE ts_padding_ZorR1 tidy(prep(rec_obj), number = 1) #> # A tibble: 1 × 4 #>   terms by    pad_value id               #>   <chr> <chr> <lgl>     <chr>            #> 1 date  day   NA        ts_padding_ZorR1"},{"path":"https://business-science.github.io/timetk/reference/summarise_by_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarise (for Time Series Data) — summarise_by_time","title":"Summarise (for Time Series Data) — summarise_by_time","text":"summarise_by_time() time-based variant popular dplyr::summarise() function uses .date_var specify date date-time column .group calculation groups like \"5 seconds\", \"week\", \"3 months\". summarise_by_time() summarize_by_time() synonyms.","code":""},{"path":"https://business-science.github.io/timetk/reference/summarise_by_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarise (for Time Series Data) — summarise_by_time","text":"","code":"summarise_by_time(   .data,   .date_var,   .by = \"day\",   ...,   .type = c(\"floor\", \"ceiling\", \"round\"),   .week_start = NULL )  summarize_by_time(   .data,   .date_var,   .by = \"day\",   ...,   .type = c(\"floor\", \"ceiling\", \"round\"),   .week_start = NULL )"},{"path":"https://business-science.github.io/timetk/reference/summarise_by_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarise (for Time Series Data) — summarise_by_time","text":".data tbl object data.frame .date_var column containing date date-time values summarize. missing, attempts auto-detect date column. .time unit summarise . Time units collapsed using lubridate::floor_date() lubridate::ceiling_date(). value can : second minute hour day week month bimonth quarter season halfyear year Arbitrary unique English abbreviations lubridate::period() constructor allowed. ... Name-value pairs summary functions. name name variable result. value can : vector length 1, e.g. min(x), n(), sum(.na(y)). vector length n, e.g. quantile(). data frame, add multiple columns single expression. .type One \"floor\", \"ceiling\", \"round. Defaults \"floor\". See lubridate::round_date. .week_start unit weeks, specify reference day. 7 represents Sunday 1 represents Monday.","code":""},{"path":"https://business-science.github.io/timetk/reference/summarise_by_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarise (for Time Series Data) — summarise_by_time","text":"tibble data.frame","code":""},{"path":"https://business-science.github.io/timetk/reference/summarise_by_time.html","id":"useful-summary-functions","dir":"Reference","previous_headings":"","what":"Useful summary functions","title":"Summarise (for Time Series Data) — summarise_by_time","text":"Sum: sum() Center: mean(), median() Spread: sd(), var() Range: min(), max() Count: dplyr::n(), dplyr::n_distinct() Position: dplyr::first(), dplyr::last(), dplyr::nth() Correlation: cor(), cov()","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/summarise_by_time.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarise (for Time Series Data) — summarise_by_time","text":"","code":"# Libraries library(dplyr)  # First value in each month m4_daily %>%     group_by(id) %>%     summarise_by_time(         .date_var = date,         .by       = \"month\", # Setup for monthly aggregation         # Summarization         value  = first(value)     ) #> # A tibble: 323 × 3 #> # Groups:   id [4] #>    id    date       value #>    <fct> <date>     <dbl> #>  1 D10   2014-07-01 2076. #>  2 D10   2014-08-01 1923. #>  3 D10   2014-09-01 1908. #>  4 D10   2014-10-01 2049. #>  5 D10   2014-11-01 2133. #>  6 D10   2014-12-01 2244. #>  7 D10   2015-01-01 2351  #>  8 D10   2015-02-01 2286. #>  9 D10   2015-03-01 2291. #> 10 D10   2015-04-01 2396. #> # ℹ 313 more rows  # Last value in each month (day is first day of next month with ceiling option) m4_daily %>%     group_by(id) %>%     summarise_by_time(         .by        = \"month\",         value      = last(value),         .type      = \"ceiling\"     ) %>%     # Shift to the last day of the month     mutate(date = date %-time% \"1 day\") #> .date_var is missing. Using: date #> # A tibble: 323 × 3 #> # Groups:   id [4] #>    id    date       value #>    <fct> <date>     <dbl> #>  1 D10   2014-07-31 1917. #>  2 D10   2014-08-31 1921. #>  3 D10   2014-09-30 2024. #>  4 D10   2014-10-31 2130  #>  5 D10   2014-11-30 2217. #>  6 D10   2014-12-31 2328. #>  7 D10   2015-01-31 2210. #>  8 D10   2015-02-28 2293. #>  9 D10   2015-03-31 2392. #> 10 D10   2015-04-30 2368. #> # ℹ 313 more rows  # Total each year (.by is set to \"year\" now) m4_daily %>%     group_by(id) %>%     summarise_by_time(         .by        = \"year\",         value      = sum(value)     ) #> .date_var is missing. Using: date #> # A tibble: 30 × 3 #> # Groups:   id [4] #>    id    date          value #>    <fct> <date>        <dbl> #>  1 D10   2014-01-01  377389. #>  2 D10   2015-01-01  839533. #>  3 D10   2016-01-01  307401. #>  4 D160  2000-01-01  767500. #>  5 D160  2001-01-01  871497. #>  6 D160  2002-01-01 1464374. #>  7 D160  2003-01-01 3160291. #>  8 D160  2004-01-01 5424860  #>  9 D160  2005-01-01 6127995. #> 10 D160  2006-01-01 3694712. #> # ℹ 20 more rows"},{"path":"https://business-science.github.io/timetk/reference/taylor_30_min.html","id":null,"dir":"Reference","previous_headings":"","what":"Half-hourly electricity demand — taylor_30_min","title":"Half-hourly electricity demand — taylor_30_min","text":"Half-hourly electricity demand England Wales Monday 5 June 2000 Sunday 27 August 2000. Discussed Taylor (2003).","code":""},{"path":"https://business-science.github.io/timetk/reference/taylor_30_min.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Half-hourly electricity demand — taylor_30_min","text":"","code":"taylor_30_min"},{"path":"https://business-science.github.io/timetk/reference/taylor_30_min.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Half-hourly electricity demand — taylor_30_min","text":"tibble: 4,032 x 2 date: date-time variable 30-minute increments value: Electricity demand Megawatts","code":""},{"path":"https://business-science.github.io/timetk/reference/taylor_30_min.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Half-hourly electricity demand — taylor_30_min","text":"James W Taylor","code":""},{"path":"https://business-science.github.io/timetk/reference/taylor_30_min.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Half-hourly electricity demand — taylor_30_min","text":"Taylor, J.W. (2003) Short-term electricity demand forecasting using double seasonal exponential smoothing. Journal Operational Research Society, 54, 799-805.","code":""},{"path":"https://business-science.github.io/timetk/reference/taylor_30_min.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Half-hourly electricity demand — taylor_30_min","text":"","code":"taylor_30_min #> # A tibble: 4,032 × 2 #>    date                value #>    <dttm>              <dbl> #>  1 2000-06-05 00:00:00 22262 #>  2 2000-06-05 00:30:00 21756 #>  3 2000-06-05 01:00:00 22247 #>  4 2000-06-05 01:30:00 22759 #>  5 2000-06-05 02:00:00 22549 #>  6 2000-06-05 02:30:00 22313 #>  7 2000-06-05 03:00:00 22128 #>  8 2000-06-05 03:30:00 21860 #>  9 2000-06-05 04:00:00 21751 #> 10 2000-06-05 04:30:00 21336 #> # ℹ 4,022 more rows"},{"path":"https://business-science.github.io/timetk/reference/tidyeval.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy eval helpers — tidyeval","title":"Tidy eval helpers — tidyeval","text":"sym creates symbol string syms creates list symbols character vector. enquo enquos delay execution one several function arguments. enquo() returns single quoted expression, like blueprint delayed computation. enquos() returns list quoted expressions. expr quotes new expression locally. mostly useful build new expressions around arguments captured enquo() enquos(): expr(mean(!!enquo(arg), na.rm = TRUE)). as_name transforms quoted variable name string. Supplying something else quoted variable name error. unlike as_label also returns single string supports kind R object input, including quoted function calls vectors. purpose summarise object single label. label often suitable default name. know quoted expression contains (instance expressions captured enquo() variable name, call function, unquoted constant), use as_label(). know quoted simple variable name, like enforce , use as_name(). learn tidy eval use tools, visit Metaprogramming section Advanced R.","code":""},{"path":"https://business-science.github.io/timetk/reference/tidyeval.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tidy eval helpers — tidyeval","text":"Nothing returned. document important functions rlang used internal R package.","code":""},{"path":"https://business-science.github.io/timetk/reference/time_arithmetic.html","id":null,"dir":"Reference","previous_headings":"","what":"Add / Subtract (For Time Series) — time_arithmetic","title":"Add / Subtract (For Time Series) — time_arithmetic","text":"easiest way add / subtract period time series date date-time vector.","code":""},{"path":"https://business-science.github.io/timetk/reference/time_arithmetic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add / Subtract (For Time Series) — time_arithmetic","text":"","code":"add_time(index, period)  subtract_time(index, period)  index %+time% period  index %-time% period"},{"path":"https://business-science.github.io/timetk/reference/time_arithmetic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add / Subtract (For Time Series) — time_arithmetic","text":"index date date-time vector. Can also accept character representation. period period add. Accepts character strings like \"5 seconds\", \"2 days\", complex strings like \"1 month 4 days 34 minutes\".","code":""},{"path":"https://business-science.github.io/timetk/reference/time_arithmetic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add / Subtract (For Time Series) — time_arithmetic","text":"date datetime (POSIXct) vector length index time values shifted +/- period.","code":""},{"path":"https://business-science.github.io/timetk/reference/time_arithmetic.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add / Subtract (For Time Series) — time_arithmetic","text":"convenient wrapper lubridate::period(). Adds subtracts period time-based index. Great : Finding timestamp n-periods future past Shifting time-based index. Note NA values may present dates exist. Period Specification period argument accepts complex strings like: \"1 month 4 days 43 minutes\" \"second = 3, minute = 1, hour = 2, day = 13, week = 1\"","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/time_arithmetic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add / Subtract (For Time Series) — time_arithmetic","text":"","code":"# ---- LOCATING A DATE N-PERIODS IN FUTURE / PAST ----  # Forward (Plus Time) \"2021\" %+time% \"1 hour 34 seconds\" #> [1] \"2021-01-01 01:00:34 UTC\" \"2021\" %+time% \"3 months\" #> [1] \"2021-04-01\" \"2021\" %+time% \"1 year 3 months 6 days\" #> [1] \"2022-04-07\"  # Backward (Minus Time) \"2021\" %-time% \"1 hour 34 seconds\" #> [1] \"2020-12-31 22:59:26 UTC\" \"2021\" %-time% \"3 months\" #> [1] \"2020-10-01\" \"2021\" %-time% \"1 year 3 months 6 days\" #> [1] \"2019-09-25\"  # ---- INDEX SHIFTING ----  index_daily <- tk_make_timeseries(\"2016\", \"2016-02-01\") #> Using by: day  # ADD TIME # - Note `NA` values created where a daily dates aren't possible #   (e.g. Feb 29 & 30, 2016 doesn't exist). index_daily %+time% \"1 month\" #> Warning: Missing values created during time addition. This can happen if dates do not exist. #>  [1] \"2016-02-01\" \"2016-02-02\" \"2016-02-03\" \"2016-02-04\" \"2016-02-05\" #>  [6] \"2016-02-06\" \"2016-02-07\" \"2016-02-08\" \"2016-02-09\" \"2016-02-10\" #> [11] \"2016-02-11\" \"2016-02-12\" \"2016-02-13\" \"2016-02-14\" \"2016-02-15\" #> [16] \"2016-02-16\" \"2016-02-17\" \"2016-02-18\" \"2016-02-19\" \"2016-02-20\" #> [21] \"2016-02-21\" \"2016-02-22\" \"2016-02-23\" \"2016-02-24\" \"2016-02-25\" #> [26] \"2016-02-26\" \"2016-02-27\" \"2016-02-28\" \"2016-02-29\" NA           #> [31] NA           \"2016-03-01\"  # Subtracting Time index_daily %-time% \"1 month\" #>  [1] \"2015-12-01\" \"2015-12-02\" \"2015-12-03\" \"2015-12-04\" \"2015-12-05\" #>  [6] \"2015-12-06\" \"2015-12-07\" \"2015-12-08\" \"2015-12-09\" \"2015-12-10\" #> [11] \"2015-12-11\" \"2015-12-12\" \"2015-12-13\" \"2015-12-14\" \"2015-12-15\" #> [16] \"2015-12-16\" \"2015-12-17\" \"2015-12-18\" \"2015-12-19\" \"2015-12-20\" #> [21] \"2015-12-21\" \"2015-12-22\" \"2015-12-23\" \"2015-12-24\" \"2015-12-25\" #> [26] \"2015-12-26\" \"2015-12-27\" \"2015-12-28\" \"2015-12-29\" \"2015-12-30\" #> [31] \"2015-12-31\" \"2016-01-01\""},{"path":"https://business-science.github.io/timetk/reference/time_series_cv.html","id":null,"dir":"Reference","previous_headings":"","what":"Time Series Cross Validation — time_series_cv","title":"Time Series Cross Validation — time_series_cv","text":"Create rsample cross validation sets time series. function produces sampling plan starting recent time series observations, rolling backwards. sampling procedure similar rsample::rolling_origin(), places focus cross validation recent time series data.","code":""},{"path":"https://business-science.github.io/timetk/reference/time_series_cv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Time Series Cross Validation — time_series_cv","text":"","code":"time_series_cv(   data,   date_var = NULL,   initial = 5,   assess = 1,   skip = 1,   lag = 0,   cumulative = FALSE,   slice_limit = n(),   point_forecast = FALSE,   ... )"},{"path":"https://business-science.github.io/timetk/reference/time_series_cv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Time Series Cross Validation — time_series_cv","text":"data data frame. date_var date date-time variable. initial number samples used analysis/modeling initial resample. assess number samples used assessment resample. skip integer indicating many () additional resamples skip thin total amount data points analysis resample. See example . lag value include lag assessment analysis set. useful lagged predictors used training testing. cumulative logical. analysis resample grow beyond size specified initial resample?. slice_limit number slices return. Set dplyr::n(), returns maximum number slices. point_forecast Whether testing set single point forecast forecast horizon. default forecast horizon. Default: FALSE ... dots future extensions must empty.","code":""},{"path":"https://business-science.github.io/timetk/reference/time_series_cv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Time Series Cross Validation — time_series_cv","text":"tibble classes time_series_cv, rset, tbl_df, tbl, data.frame. results include column data split objects column called id character string resample identifier.","code":""},{"path":"https://business-science.github.io/timetk/reference/time_series_cv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Time Series Cross Validation — time_series_cv","text":"Time-Based Specification initial, assess, skip, lag variables can specified : Numeric: initial = 24 Time-Based Phrases: initial = \"2 years\", data contains date_var (date datetime) Initial (Training Set) Assess (Testing Set) main options, initial assess, control number data points original data analysis (training set) assessment (testing set), respectively. Skip skip enables function use every data point resamples. skip = 1, resampling data sets increment one position. Example: Suppose rows data set consecutive days. Using skip = 7 make analysis data set operate weeks instead days. assessment set size affected option. Lag Lag parameter creates overlap Testing set. needed lagged predictors used. Cumulative vs Sliding Window cumulative = TRUE, initial parameter ignored analysis (training) set grow resampling continues assessment (testing) set size always remain static. cumulative = FALSE, initial parameter fixes analysis (training) set resampling occurs fixed window. Slice Limit controls number slices. slice_limit = 5, recent 5 slices returned. Point Forecast point forecast sometimes desired want forecast value \"4-weeks\" future. can setting following parameters: assess = \"4 weeks\" point_forecast = TRUE Panel Data / Time Series Groups / Overlapping Timestamps Overlapping timestamps occur data one time series group. sometimes called Panel Data Time Series Groups. timestamps duplicated (case \"Panel Data\" \"Time Series Groups\"), resample calculation applies sliding window fixed length dataset. See example using walmart_sales_weekly dataset .","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/time_series_cv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Time Series Cross Validation — time_series_cv","text":"","code":"library(dplyr)  # DATA ---- m750 <- m4_monthly %>% dplyr::filter(id == \"M750\")   # RESAMPLE SPEC ---- resample_spec <- time_series_cv(data = m750,                                 initial     = \"6 years\",                                 assess      = \"24 months\",                                 skip        = \"24 months\",                                 cumulative  = FALSE,                                 slice_limit = 3) #> Using date_var: date  resample_spec #> # Time Series Cross Validation Plan  #> # A tibble: 3 × 2 #>   splits          id     #>   <list>          <chr>  #> 1 <split [72/24]> Slice1 #> 2 <split [72/24]> Slice2 #> 3 <split [72/24]> Slice3  # VISUALIZE CV PLAN ----  # Select date and value columns from the tscv diagnostic tool resample_spec %>% tk_time_series_cv_plan() #> # A tibble: 288 × 5 #>    .id    .key     id    date       value #>    <chr>  <fct>    <fct> <date>     <dbl> #>  1 Slice1 training M750  2007-07-01  8710 #>  2 Slice1 training M750  2007-08-01  8300 #>  3 Slice1 training M750  2007-09-01  8910 #>  4 Slice1 training M750  2007-10-01  9710 #>  5 Slice1 training M750  2007-11-01  9870 #>  6 Slice1 training M750  2007-12-01  9980 #>  7 Slice1 training M750  2008-01-01  9970 #>  8 Slice1 training M750  2008-02-01  9970 #>  9 Slice1 training M750  2008-03-01 10120 #> 10 Slice1 training M750  2008-04-01 10150 #> # ℹ 278 more rows  # Plot the date and value columns to see the CV Plan resample_spec %>%     plot_time_series_cv_plan(date, value, .interactive = FALSE)    # PANEL DATA / TIME SERIES GROUPS ---- # - Time Series Groups are processed using an *ungrouped* data set # - The data has sliding windows applied starting with the beginning of the series # - The seven groups of weekly time series are #   processed together for <split [358/78]> dimensions  walmart_tscv <- walmart_sales_weekly %>%     time_series_cv(         date_var    = Date,         initial     = \"12 months\",         assess      = \"3 months\",         skip        = \"3 months\",         slice_limit = 4     ) #> Data is not ordered by the 'date_var'. Resamples will be arranged by `Date`. #> Overlapping Timestamps Detected. Processing overlapping time series together using sliding windows.  walmart_tscv #> # Time Series Cross Validation Plan  #> # A tibble: 4 × 2 #>   splits           id     #>   <list>           <chr>  #> 1 <split [364/84]> Slice1 #> 2 <split [364/84]> Slice2 #> 3 <split [364/84]> Slice3 #> 4 <split [364/84]> Slice4  walmart_tscv %>%     plot_time_series_cv_plan(Date, Weekly_Sales, .interactive = FALSE)"},{"path":"https://business-science.github.io/timetk/reference/time_series_split.html","id":null,"dir":"Reference","previous_headings":"","what":"Simple Training/Test Set Splitting for Time Series — time_series_split","title":"Simple Training/Test Set Splitting for Time Series — time_series_split","text":"time_series_split creates resample splits using time_series_cv() returns single split. useful creating single train/test split.","code":""},{"path":"https://business-science.github.io/timetk/reference/time_series_split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simple Training/Test Set Splitting for Time Series — time_series_split","text":"","code":"time_series_split(   data,   date_var = NULL,   initial = 5,   assess = 1,   skip = 1,   lag = 0,   cumulative = FALSE,   slice = 1,   point_forecast = FALSE,   ... )"},{"path":"https://business-science.github.io/timetk/reference/time_series_split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simple Training/Test Set Splitting for Time Series — time_series_split","text":"data data frame. date_var date date-time variable. initial number samples used analysis/modeling initial resample. assess number samples used assessment resample. skip integer indicating many () additional resamples skip thin total amount data points analysis resample. See example . lag value include lag assessment analysis set. useful lagged predictors used training testing. cumulative logical. analysis resample grow beyond size specified initial resample?. slice Returns single slice time_series_cv point_forecast Whether testing set single point forecast forecast horizon. default forecast horizon. Default: FALSE ... dots future extensions must empty.","code":""},{"path":"https://business-science.github.io/timetk/reference/time_series_split.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simple Training/Test Set Splitting for Time Series — time_series_split","text":"rsplit object can used training testing functions extract data split.","code":""},{"path":"https://business-science.github.io/timetk/reference/time_series_split.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simple Training/Test Set Splitting for Time Series — time_series_split","text":"Time-Based Specification initial, assess, skip, lag variables can specified : Numeric: initial = 24 Time-Based Phrases: initial = \"2 years\", data contains date_var (date datetime) Initial (Training Set) Assess (Testing Set) main options, initial assess, control number data points original data analysis (training set) assessment (testing set), respectively. Skip skip enables function use every data point resamples. skip = 1, resampling data sets increment one position. Example: Suppose rows data set consecutive days. Using skip = 7 make analysis data set operate weeks instead days. assessment set size affected option. Lag Lag parameter creates overlap Testing set. needed lagged predictors used. Cumulative vs Sliding Window cumulative = TRUE, initial parameter ignored analysis (training) set grow resampling continues assessment (testing) set size always remain static. cumulative = FALSE, initial parameter fixes analysis (training) set resampling occurs fixed window. Slice controls slice returned. slice = 1, recent slice returned.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/time_series_split.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simple Training/Test Set Splitting for Time Series — time_series_split","text":"","code":"library(dplyr)  # DATA ---- m750 <- m4_monthly %>% dplyr::filter(id == \"M750\")  # Get the most recent 3 years as testing, and previous 10 years as training m750 %>%     time_series_split(initial = \"10 years\", assess = \"3 years\") #> Using date_var: date #> <Analysis/Assess/Total> #> <120/36/306>  # Skip the most recent 3 years m750 %>%     time_series_split(         initial = \"10 years\",         assess  = \"3 years\",         skip    = \"3 years\",         slice   = 2          # <- Returns 2nd slice, 3-years back     ) #> Using date_var: date #> <Analysis/Assess/Total> #> <120/36/306>  # Add 1 year lag for testing overlap m750 %>%     time_series_split(         initial = \"10 years\",         assess  = \"3 years\",         skip    = \"3 years\",         slice   = 2,         lag     = \"1 year\"   # <- Overlaps training/testing by 1 year     ) #> Using date_var: date #> <Analysis/Assess/Total> #> <120/48/306>"},{"path":"https://business-science.github.io/timetk/reference/timetk-package.html","id":null,"dir":"Reference","previous_headings":"","what":"timetk: Time Series Analysis in the Tidyverse — timetk-package","title":"timetk: Time Series Analysis in the Tidyverse — timetk-package","text":"timetk package combines collection coercion tools time series analysis.","code":""},{"path":"https://business-science.github.io/timetk/reference/timetk-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"timetk: Time Series Analysis in the Tidyverse — timetk-package","text":"timetk package several benefits: Visualizing Time Series Wrangling Time Series. Preprocessing Feature Engineering. learn timetk, start documentation: https://business-science.github.io/timetk/","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/timetk-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"timetk: Time Series Analysis in the Tidyverse — timetk-package","text":"Maintainer: Matt Dancho mdancho@business-science.io Authors: Davis Vaughan dvaughan@business-science.io","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_acf_diagnostics.html","id":null,"dir":"Reference","previous_headings":"","what":"Group-wise ACF, PACF, and CCF Data Preparation — tk_acf_diagnostics","title":"Group-wise ACF, PACF, and CCF Data Preparation — tk_acf_diagnostics","text":"tk_acf_diagnostics() function provides simple interface detect Autocorrelation (ACF), Partial Autocorrelation (PACF), Cross Correlation (CCF) Lagged Predictors one tibble. function powers plot_acf_diagnostics() visualization.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_acf_diagnostics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group-wise ACF, PACF, and CCF Data Preparation — tk_acf_diagnostics","text":"","code":"tk_acf_diagnostics(.data, .date_var, .value, .ccf_vars = NULL, .lags = 1000)"},{"path":"https://business-science.github.io/timetk/reference/tk_acf_diagnostics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group-wise ACF, PACF, and CCF Data Preparation — tk_acf_diagnostics","text":".data data frame tibble numeric features (values) descending chronological order .date_var column containing either date date-time values .value numeric column value ACF PACF calculations performed. .ccf_vars Additional features perform Lag Cross Correlations (CCFs) versus .value. Useful evaluating external lagged regressors. .lags seqence one lags evaluate.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_acf_diagnostics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Group-wise ACF, PACF, and CCF Data Preparation — tk_acf_diagnostics","text":"tibble data.frame containing autocorrelation, partial autocorrelation cross correlation data.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_acf_diagnostics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Group-wise ACF, PACF, and CCF Data Preparation — tk_acf_diagnostics","text":"Simplified ACF, PACF, & CCF often interested 3 functions. get 3 ? Now can! ACF - Autocorrelation target variable lagged versions PACF - Partial Autocorrelation removes dependence lags lags highlighting key seasonalities. CCF - Shows lagged predictors can used prediction target variable. Lag Specification Lags (.lags) can either specified : time-based phrase indicating duraction (e.g. 2 months) maximum lag (e.g. .lags = 28) sequence lags (e.g. .lags = 7:28) Scales Multiple Time Series Groupes tk_acf_diagnostics() works grouped_df's, meaning can group time series one categorical columns dplyr::group_by() apply tk_acf_diagnostics() return group-wise lag diagnostics. Special Note Dots (...) Unlike plotting utilities, ... arguments used group-wise analysis. Rather, used processing Cross Correlations (CCFs). Use dplyr::group_by() processing multiple time series groups.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_acf_diagnostics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group-wise ACF, PACF, and CCF Data Preparation — tk_acf_diagnostics","text":"","code":"library(dplyr)  # ACF, PACF, & CCF in 1 Data Frame # - Get ACF & PACF for target (adjusted) # - Get CCF between adjusted and volume and close FANG %>%     filter(symbol == \"FB\") %>%     tk_acf_diagnostics(date, adjusted,                # ACF & PACF                        .ccf_vars = c(volume, close),  # CCFs                        .lags     = 500) #> # A tibble: 501 × 7 #>      lag   ACF      PACF CCF_volume CCF_close .white_noise_upper #>    <dbl> <dbl>     <dbl>      <dbl>     <dbl>              <dbl> #>  1     0 1      1            -0.447     1                 0.0630 #>  2     1 0.997  0.997        -0.444     0.997             0.0630 #>  3     2 0.994 -0.0227       -0.442     0.994             0.0630 #>  4     3 0.990  0.0101       -0.438     0.990             0.0630 #>  5     4 0.987  0.0311       -0.437     0.987             0.0630 #>  6     5 0.985  0.0180       -0.438     0.985             0.0630 #>  7     6 0.982  0.00502      -0.437     0.982             0.0630 #>  8     7 0.979  0.0171       -0.437     0.979             0.0630 #>  9     8 0.976 -0.000118     -0.436     0.976             0.0630 #> 10     9 0.974 -0.00243      -0.435     0.974             0.0630 #> # ℹ 491 more rows #> # ℹ 1 more variable: .white_noise_lower <dbl>  # Scale with groups using group_by() FANG %>%     group_by(symbol) %>%     tk_acf_diagnostics(date, adjusted,                        .ccf_vars = c(volume, close),                        .lags     = \"3 months\") #> # A tibble: 248 × 8 #> # Groups:   symbol [4] #>    symbol   lag   ACF      PACF CCF_volume CCF_close .white_noise_upper #>    <chr>  <dbl> <dbl>     <dbl>      <dbl>     <dbl>              <dbl> #>  1 FB         0 1      1            -0.447     1                 0.0630 #>  2 FB         1 0.997  0.997        -0.444     0.997             0.0630 #>  3 FB         2 0.994 -0.0227       -0.442     0.994             0.0630 #>  4 FB         3 0.990  0.0101       -0.438     0.990             0.0630 #>  5 FB         4 0.987  0.0311       -0.437     0.987             0.0630 #>  6 FB         5 0.985  0.0180       -0.438     0.985             0.0630 #>  7 FB         6 0.982  0.00502      -0.437     0.982             0.0630 #>  8 FB         7 0.979  0.0171       -0.437     0.979             0.0630 #>  9 FB         8 0.976 -0.000118     -0.436     0.976             0.0630 #> 10 FB         9 0.974 -0.00243      -0.435     0.974             0.0630 #> # ℹ 238 more rows #> # ℹ 1 more variable: .white_noise_lower <dbl>  # Apply Transformations FANG %>%     group_by(symbol) %>%     tk_acf_diagnostics(         date, diff_vec(adjusted),  # Apply differencing transformation         .lags = 0:500     ) #> diff_vec(): Initial values: 257.309998 #> diff_vec(): Initial values: 28 #> diff_vec(): Initial values: 361.264351 #> diff_vec(): Initial values: 13.144286 #> # A tibble: 2,004 × 6 #> # Groups:   symbol [4] #>    symbol   lag      ACF     PACF .white_noise_upper .white_noise_lower #>    <chr>  <dbl>    <dbl>    <dbl>              <dbl>              <dbl> #>  1 FB         0  1        1                   0.0630            -0.0630 #>  2 FB         1  0.0272   0.0272              0.0630            -0.0630 #>  3 FB         2 -0.0219  -0.0226              0.0630            -0.0630 #>  4 FB         3 -0.0973  -0.0962              0.0630            -0.0630 #>  5 FB         4 -0.0554  -0.0512              0.0630            -0.0630 #>  6 FB         5  0.0104   0.00896             0.0630            -0.0630 #>  7 FB         6 -0.0622  -0.0751              0.0630            -0.0630 #>  8 FB         7  0.00363 -0.00334             0.0630            -0.0630 #>  9 FB         8 -0.0168  -0.0212              0.0630            -0.0630 #> 10 FB         9  0.0300   0.0187              0.0630            -0.0630 #> # ℹ 1,994 more rows"},{"path":"https://business-science.github.io/timetk/reference/tk_anomaly_diagnostics.html","id":null,"dir":"Reference","previous_headings":"","what":"Automatic group-wise Anomaly Detection by STL Decomposition — tk_anomaly_diagnostics","title":"Automatic group-wise Anomaly Detection by STL Decomposition — tk_anomaly_diagnostics","text":"tk_anomaly_diagnostics() preprocessor plot_anomaly_diagnostics(). performs automatic anomaly detection one time series groups.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_anomaly_diagnostics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Automatic group-wise Anomaly Detection by STL Decomposition — tk_anomaly_diagnostics","text":"","code":"tk_anomaly_diagnostics(   .data,   .date_var,   .value,   .frequency = \"auto\",   .trend = \"auto\",   .alpha = 0.05,   .max_anomalies = 0.2,   .message = TRUE )"},{"path":"https://business-science.github.io/timetk/reference/tk_anomaly_diagnostics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Automatic group-wise Anomaly Detection by STL Decomposition — tk_anomaly_diagnostics","text":".data tibble data.frame time-based column .date_var column containing either date date-time values .value column containing numeric values .frequency Controls seasonal adjustment (removal seasonality). Input can either \"auto\", time-based definition (e.g. \"2 weeks\"), numeric number observations per frequency (e.g. 10). Refer tk_get_frequency(). .trend Controls trend component. STL, trend controls sensitivity LOESS smoother, used remove remainder. Refer tk_get_trend(). .alpha Controls width \"normal\" range. Lower values conservative higher values less prone incorrectly classifying \"normal\" observations. .max_anomalies maximum percent anomalies permitted identified. .message boolean. TRUE, output information related automatic frequency trend selection (applicable).","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_anomaly_diagnostics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Automatic group-wise Anomaly Detection by STL Decomposition — tk_anomaly_diagnostics","text":"tibble data.frame STL Decomposition Features (observed, season, trend, remainder, seasadj) Anomaly Features (remainder_l1, remainder_l2, anomaly, recomposed_l1, recomposed_l2)","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_anomaly_diagnostics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Automatic group-wise Anomaly Detection by STL Decomposition — tk_anomaly_diagnostics","text":"tk_anomaly_diagnostics() method anomaly detection implements 2-step process detect outliers time series. Step 1: Detrend & Remove Seasonality using STL Decomposition decomposition separates \"season\" \"trend\" components \"observed\" values leaving \"remainder\" anomaly detection. user can control two parameters: frequency trend. .frequency: Adjusts \"season\" component removed \"observed\" values. .trend: Adjusts trend window (t.window parameter stats::stl() used. user may supply .frequency .trend time-based durations (e.g. \"6 weeks\") numeric values (e.g. 180) \"auto\", predetermines frequency /trend based scale time series using tk_time_scale_template(). Step 2: Anomaly Detection \"trend\" \"season\" (seasonality) removed, anomaly detection performed \"remainder\". Anomalies identified, boundaries (recomposed_l1 recomposed_l2) determined. Anomaly Detection Method uses inner quartile range (IQR) +/-25 median. IQR Adjustment, alpha parameter default alpha = 0.05, limits established expanding 25/75 baseline IQR Factor 3 (3X). IQR Factor = 0.15 / alpha (hence 3X alpha = 0.05): increase IQR Factor controlling limits, decrease alpha, makes difficult outlier. Increase alpha make easier outlier. IQR outlier detection method used forecast::tsoutliers(). similar outlier detection method used Twitter's AnomalyDetection package. Twitter Forecast tsoutliers methods implemented Business Science's anomalize package.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_anomaly_diagnostics.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Automatic group-wise Anomaly Detection by STL Decomposition — tk_anomaly_diagnostics","text":"CLEVELAND, R. B., CLEVELAND, W. S., MCRAE, J. E., TERPENNING, . STL: Seasonal-Trend Decomposition Procedure Based Loess. Journal Official Statistics, Vol. 6, . 1 (1990), pp. 3-73. Owen S. Vallis, Jordan Hochenbaum Arun Kejariwal (2014). Novel Technique Long-Term Anomaly Detection Cloud. Twitter Inc.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_anomaly_diagnostics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Automatic group-wise Anomaly Detection by STL Decomposition — tk_anomaly_diagnostics","text":"","code":"library(dplyr)  walmart_sales_weekly %>%     filter(id %in% c(\"1_1\", \"1_3\")) %>%     group_by(id) %>%     tk_anomaly_diagnostics(Date, Weekly_Sales) #> frequency = 13 observations per 1 quarter #> trend = 52 observations per 1 year #> frequency = 13 observations per 1 quarter #> trend = 52 observations per 1 year #> # A tibble: 286 × 12 #> # Groups:   id [2] #>    id    Date       observed season  trend remainder seasadj remainder_l1 #>    <fct> <date>        <dbl>  <dbl>  <dbl>     <dbl>   <dbl>        <dbl> #>  1 1_1   2010-02-05   24924.   874. 19967.     4083.  24050.      -15981. #>  2 1_1   2010-02-12   46039.  -698. 19835.    26902.  46737.      -15981. #>  3 1_1   2010-02-19   41596. -1216. 19703.    23108.  42812.      -15981. #>  4 1_1   2010-02-26   19404.  -821. 19571.      653.  20224.      -15981. #>  5 1_1   2010-03-05   21828.   324. 19439.     2064.  21504.      -15981. #>  6 1_1   2010-03-12   21043.   471. 19307.     1265.  20572.      -15981. #>  7 1_1   2010-03-19   22137.   920. 19175.     2041.  21217.      -15981. #>  8 1_1   2010-03-26   26229.   752. 19069.     6409.  25478.      -15981. #>  9 1_1   2010-04-02   57258.   503. 18962.    37794.  56755.      -15981. #> 10 1_1   2010-04-09   42961.  1132. 18855.    22974.  41829.      -15981. #> # ℹ 276 more rows #> # ℹ 4 more variables: remainder_l2 <dbl>, anomaly <chr>, recomposed_l1 <dbl>, #> #   recomposed_l2 <dbl>"},{"path":"https://business-science.github.io/timetk/reference/tk_augment_differences.html","id":null,"dir":"Reference","previous_headings":"","what":"Add many differenced columns to the data — tk_augment_differences","title":"Add many differenced columns to the data — tk_augment_differences","text":"handy function adding multiple lagged difference values data frame. Works dplyr groups .","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_differences.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add many differenced columns to the data — tk_augment_differences","text":"","code":"tk_augment_differences(   .data,   .value,   .lags = 1,   .differences = 1,   .log = FALSE,   .names = \"auto\" )"},{"path":"https://business-science.github.io/timetk/reference/tk_augment_differences.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add many differenced columns to the data — tk_augment_differences","text":".data tibble. .value One column(s) transformation applied. Usage tidyselect functions (e.g. contains()) can used select multiple columns. .lags One lags difference(s) .differences number differences apply. .log TRUE, applies log-differences. .names vector names new columns. Must length number output columns. Use \"auto\" automatically rename columns.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_differences.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add many differenced columns to the data — tk_augment_differences","text":"Returns tibble object describing timeseries.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_differences.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add many differenced columns to the data — tk_augment_differences","text":"Benefits scalable function : Designed work grouped data using dplyr::group_by() Add multiple differences adding sequence differences using .lags argument (e.g. lags = 1:20)","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_augment_differences.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add many differenced columns to the data — tk_augment_differences","text":"","code":"library(dplyr)  m4_monthly %>%     group_by(id) %>%     tk_augment_differences(value, .lags = 1:20) #> # A tibble: 1,574 × 23 #> # Groups:   id [4] #>    id    date       value value_lag1_diff1 value_lag2_diff1 value_lag3_diff1 #>    <fct> <date>     <dbl>            <dbl>            <dbl>            <dbl> #>  1 M1    1976-06-01  8000               NA               NA               NA #>  2 M1    1976-07-01  8350              350               NA               NA #>  3 M1    1976-08-01  8570              220              570               NA #>  4 M1    1976-09-01  7700             -870             -650             -300 #>  5 M1    1976-10-01  7080             -620            -1490            -1270 #>  6 M1    1976-11-01  6520             -560            -1180            -2050 #>  7 M1    1976-12-01  6070             -450            -1010            -1630 #>  8 M1    1977-01-01  6650              580              130             -430 #>  9 M1    1977-02-01  6830              180              760              310 #> 10 M1    1977-03-01  5710            -1120             -940             -360 #> # ℹ 1,564 more rows #> # ℹ 17 more variables: value_lag4_diff1 <dbl>, value_lag5_diff1 <dbl>, #> #   value_lag6_diff1 <dbl>, value_lag7_diff1 <dbl>, value_lag8_diff1 <dbl>, #> #   value_lag9_diff1 <dbl>, value_lag10_diff1 <dbl>, value_lag11_diff1 <dbl>, #> #   value_lag12_diff1 <dbl>, value_lag13_diff1 <dbl>, value_lag14_diff1 <dbl>, #> #   value_lag15_diff1 <dbl>, value_lag16_diff1 <dbl>, value_lag17_diff1 <dbl>, #> #   value_lag18_diff1 <dbl>, value_lag19_diff1 <dbl>, value_lag20_diff1 <dbl>"},{"path":"https://business-science.github.io/timetk/reference/tk_augment_fourier.html","id":null,"dir":"Reference","previous_headings":"","what":"Add many fourier series to the data — tk_augment_fourier","title":"Add many fourier series to the data — tk_augment_fourier","text":"handy function adding multiple fourier series data frame. Works dplyr groups .","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_fourier.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add many fourier series to the data — tk_augment_fourier","text":"","code":"tk_augment_fourier(.data, .date_var, .periods, .K = 1, .names = \"auto\")"},{"path":"https://business-science.github.io/timetk/reference/tk_augment_fourier.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add many fourier series to the data — tk_augment_fourier","text":".data tibble. .date_var date date-time column used calculate fourier series .periods One periods fourier series .K maximum number fourier orders. .names vector names new columns. Must length number output columns. Use \"auto\" automatically rename columns.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_fourier.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add many fourier series to the data — tk_augment_fourier","text":"Returns tibble object describing timeseries.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_fourier.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add many fourier series to the data — tk_augment_fourier","text":"Benefits scalable function : Designed work grouped data using dplyr::group_by() Add multiple differences adding sequence differences using .periods argument (e.g. lags = 1:20)","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_augment_fourier.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add many fourier series to the data — tk_augment_fourier","text":"","code":"library(dplyr)  m4_monthly %>%     group_by(id) %>%     tk_augment_fourier(date, .periods = c(6, 12), .K = 2) #> # A tibble: 1,574 × 11 #> # Groups:   id [4] #>    id    date       value date_sin6_K1 date_cos6_K1 date_sin6_K2 date_cos6_K2 #>    <fct> <date>     <dbl>        <dbl>        <dbl>        <dbl>        <dbl> #>  1 M1    1976-06-01  8000       -0.571      -0.821        0.938         0.347 #>  2 M1    1976-07-01  8350       -0.999       0.0506      -0.101        -0.995 #>  3 M1    1976-08-01  8570       -0.455       0.890       -0.811         0.585 #>  4 M1    1976-09-01  7700        0.543       0.840        0.912         0.410 #>  5 M1    1976-10-01  7080        1.000      -0.0169      -0.0338       -0.999 #>  6 M1    1976-11-01  6520        0.485      -0.874       -0.849         0.529 #>  7 M1    1976-12-01  6070       -0.485      -0.874        0.849         0.529 #>  8 M1    1977-01-01  6650       -1.000      -0.0169       0.0338       -0.999 #>  9 M1    1977-02-01  6830       -0.515       0.857       -0.882         0.470 #> 10 M1    1977-03-01  5710        0.394       0.919        0.725         0.689 #> # ℹ 1,564 more rows #> # ℹ 4 more variables: date_sin12_K1 <dbl>, date_cos12_K1 <dbl>, #> #   date_sin12_K2 <dbl>, date_cos12_K2 <dbl>"},{"path":"https://business-science.github.io/timetk/reference/tk_augment_holiday.html","id":null,"dir":"Reference","previous_headings":"","what":"Add many holiday features to the data — tk_augment_holiday","title":"Add many holiday features to the data — tk_augment_holiday","text":"Quickly add \"holiday signature\" - sets holiday features correspond calendar dates. Works dplyr groups .","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_holiday.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add many holiday features to the data — tk_augment_holiday","text":"","code":"tk_augment_holiday_signature(   .data,   .date_var = NULL,   .holiday_pattern = \".\",   .locale_set = c(\"all\", \"none\", \"World\", \"US\", \"CA\", \"GB\", \"FR\", \"IT\", \"JP\", \"CH\", \"DE\"),   .exchange_set = c(\"all\", \"none\", \"NYSE\", \"LONDON\", \"NERC\", \"TSX\", \"ZURICH\") )"},{"path":"https://business-science.github.io/timetk/reference/tk_augment_holiday.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add many holiday features to the data — tk_augment_holiday","text":".data time-based tibble time-series object. .date_var column containing either date date-time values. NULL, time-based column interpret object (tibble). .holiday_pattern regular expression pattern search \"Holiday Set\". .locale_set Return binary holidays based locale. One : \"\", \"none\", \"World\", \"US\", \"CA\", \"GB\", \"FR\", \"\", \"JP\", \"CH\", \"DE\". .exchange_set Return binary holidays based Stock Exchange Calendars. One : \"\", \"none\", \"NYSE\", \"LONDON\", \"NERC\", \"TSX\", \"ZURICH\".","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_holiday.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add many holiday features to the data — tk_augment_holiday","text":"Returns tibble object describing holiday timeseries.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_holiday.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add many holiday features to the data — tk_augment_holiday","text":"tk_augment_holiday_signature adds holiday signature features. See tk_get_holiday_signature() (powers augment function) full description examples use. 1. Individual Holidays single holiday features can filtered using pattern. helps identifying holidays important machine learning model. can useful example e-commerce initiatives (e.g. sales Christmas Thanskgiving). 2. Locale-Based Summary Sets Locale-based holdiay sets useful e-commerce initiatives (e.g. sales Christmas Thanskgiving). Filter locale identify holidays locale. 3. Stock Exchange Calendar Summary Sets Exchange-based holdiay sets useful identifying non-working days. Filter index identify holidays commonly non-working.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_augment_holiday.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add many holiday features to the data — tk_augment_holiday","text":"","code":"library(dplyr)  dates_in_2017_tbl <- tibble(index = tk_make_timeseries(\"2017-01-01\", \"2017-12-31\", by = \"day\"))  # Non-working days in US due to Holidays using NYSE stock exchange calendar dates_in_2017_tbl %>%     tk_augment_holiday_signature(         index,         .holiday_pattern = \"^$\",   # Returns nothing on purpose         .locale_set      = \"none\",         .exchange_set    = \"NYSE\") #> # A tibble: 365 × 2 #>    index      exch_NYSE #>    <date>         <dbl> #>  1 2017-01-01         0 #>  2 2017-01-02         1 #>  3 2017-01-03         0 #>  4 2017-01-04         0 #>  5 2017-01-05         0 #>  6 2017-01-06         0 #>  7 2017-01-07         0 #>  8 2017-01-08         0 #>  9 2017-01-09         0 #> 10 2017-01-10         0 #> # ℹ 355 more rows  # All holidays in US dates_in_2017_tbl %>%     tk_augment_holiday_signature(         index,         .holiday_pattern = \"US_\",         .locale_set      = \"US\",         .exchange_set    = \"none\") #> # A tibble: 365 × 19 #>    index      locale_US US_NewYearsDay US_MLKingsBirthday US_InaugurationDay #>    <date>         <dbl>          <dbl>              <dbl>              <dbl> #>  1 2017-01-01         1              1                  0                  0 #>  2 2017-01-02         0              0                  0                  0 #>  3 2017-01-03         0              0                  0                  0 #>  4 2017-01-04         0              0                  0                  0 #>  5 2017-01-05         0              0                  0                  0 #>  6 2017-01-06         0              0                  0                  0 #>  7 2017-01-07         0              0                  0                  0 #>  8 2017-01-08         0              0                  0                  0 #>  9 2017-01-09         0              0                  0                  0 #> 10 2017-01-10         0              0                  0                  0 #> # ℹ 355 more rows #> # ℹ 14 more variables: US_LincolnsBirthday <dbl>, US_PresidentsDay <dbl>, #> #   US_WashingtonsBirthday <dbl>, US_CPulaskisBirthday <dbl>, #> #   US_GoodFriday <dbl>, US_MemorialDay <dbl>, US_DecorationMemorialDay <dbl>, #> #   US_IndependenceDay <dbl>, US_LaborDay <dbl>, US_ColumbusDay <dbl>, #> #   US_ElectionDay <dbl>, US_VeteransDay <dbl>, US_ThanksgivingDay <dbl>, #> #   US_ChristmasDay <dbl>  # All holidays for World and Italy-specific Holidays # - Note that Italy celebrates specific holidays in addition to many World Holidays dates_in_2017_tbl %>%     tk_augment_holiday_signature(         index,         .holiday_pattern = \"(World)|(IT_)\",         .locale_set      = c(\"World\", \"IT\"),         .exchange_set    = \"none\") #> # A tibble: 365 × 46 #>    index      locale_World locale_IT World_NewYearsDay World_SolemnityOfMary #>    <date>            <dbl>     <dbl>             <dbl>                 <dbl> #>  1 2017-01-01            1         0                 1                     1 #>  2 2017-01-02            0         0                 0                     0 #>  3 2017-01-03            0         0                 0                     0 #>  4 2017-01-04            0         0                 0                     0 #>  5 2017-01-05            0         0                 0                     0 #>  6 2017-01-06            1         1                 0                     0 #>  7 2017-01-07            0         0                 0                     0 #>  8 2017-01-08            0         0                 0                     0 #>  9 2017-01-09            0         0                 0                     0 #> 10 2017-01-10            0         0                 0                     0 #> # ℹ 355 more rows #> # ℹ 41 more variables: World_Epiphany <dbl>, IT_Epiphany <dbl>, #> #   World_PresentationOfLord <dbl>, World_Septuagesima <dbl>, #> #   World_Quinquagesima <dbl>, World_AshWednesday <dbl>, #> #   World_InternationalWomensDay <dbl>, World_Annunciation <dbl>, #> #   World_PalmSunday <dbl>, World_GoodFriday <dbl>, World_Easter <dbl>, #> #   World_EasterSunday <dbl>, World_EasterMonday <dbl>, …"},{"path":"https://business-science.github.io/timetk/reference/tk_augment_lags.html","id":null,"dir":"Reference","previous_headings":"","what":"Add many lags to the data — tk_augment_lags","title":"Add many lags to the data — tk_augment_lags","text":"handy function adding multiple lagged columns data frame. Works dplyr groups .","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_lags.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add many lags to the data — tk_augment_lags","text":"","code":"tk_augment_lags(.data, .value, .lags = 1, .names = \"auto\")  tk_augment_leads(.data, .value, .lags = -1, .names = \"auto\")"},{"path":"https://business-science.github.io/timetk/reference/tk_augment_lags.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add many lags to the data — tk_augment_lags","text":".data tibble. .value One column(s) transformation applied. Usage tidyselect functions (e.g. contains()) can used select multiple columns. .lags One lags difference(s) .names vector names new columns. Must length .lags.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_lags.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add many lags to the data — tk_augment_lags","text":"Returns tibble object describing timeseries.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_lags.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add many lags to the data — tk_augment_lags","text":"Lags vs Leads negative lag considered lead. tk_augment_leads() function identical tk_augment_lags() exception automatic naming convetion (.names = 'auto') convert column names negative lags leads. Benefits scalable function : Designed work grouped data using dplyr::group_by() Add multiple lags adding sequence lags using .lags argument (e.g. .lags = 1:20)","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_augment_lags.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add many lags to the data — tk_augment_lags","text":"","code":"library(dplyr)  # Lags m4_monthly %>%     group_by(id) %>%     tk_augment_lags(contains(\"value\"), .lags = 1:20) #> # A tibble: 1,574 × 23 #> # Groups:   id [4] #>    id    date       value value_lag1 value_lag2 value_lag3 value_lag4 value_lag5 #>    <fct> <date>     <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl> #>  1 M1    1976-06-01  8000         NA         NA         NA         NA         NA #>  2 M1    1976-07-01  8350       8000         NA         NA         NA         NA #>  3 M1    1976-08-01  8570       8350       8000         NA         NA         NA #>  4 M1    1976-09-01  7700       8570       8350       8000         NA         NA #>  5 M1    1976-10-01  7080       7700       8570       8350       8000         NA #>  6 M1    1976-11-01  6520       7080       7700       8570       8350       8000 #>  7 M1    1976-12-01  6070       6520       7080       7700       8570       8350 #>  8 M1    1977-01-01  6650       6070       6520       7080       7700       8570 #>  9 M1    1977-02-01  6830       6650       6070       6520       7080       7700 #> 10 M1    1977-03-01  5710       6830       6650       6070       6520       7080 #> # ℹ 1,564 more rows #> # ℹ 15 more variables: value_lag6 <dbl>, value_lag7 <dbl>, value_lag8 <dbl>, #> #   value_lag9 <dbl>, value_lag10 <dbl>, value_lag11 <dbl>, value_lag12 <dbl>, #> #   value_lag13 <dbl>, value_lag14 <dbl>, value_lag15 <dbl>, value_lag16 <dbl>, #> #   value_lag17 <dbl>, value_lag18 <dbl>, value_lag19 <dbl>, value_lag20 <dbl>  # Leads m4_monthly %>%     group_by(id) %>%     tk_augment_leads(value, .lags = 1:-20) #> # A tibble: 1,574 × 25 #> # Groups:   id [4] #>    id    date       value value_lag1 value_lag0 value_lead1 value_lead2 #>    <fct> <date>     <dbl>      <dbl>      <dbl>       <dbl>       <dbl> #>  1 M1    1976-06-01  8000         NA       8000        8350        8570 #>  2 M1    1976-07-01  8350       8000       8350        8570        7700 #>  3 M1    1976-08-01  8570       8350       8570        7700        7080 #>  4 M1    1976-09-01  7700       8570       7700        7080        6520 #>  5 M1    1976-10-01  7080       7700       7080        6520        6070 #>  6 M1    1976-11-01  6520       7080       6520        6070        6650 #>  7 M1    1976-12-01  6070       6520       6070        6650        6830 #>  8 M1    1977-01-01  6650       6070       6650        6830        5710 #>  9 M1    1977-02-01  6830       6650       6830        5710        5260 #> 10 M1    1977-03-01  5710       6830       5710        5260        5470 #> # ℹ 1,564 more rows #> # ℹ 18 more variables: value_lead3 <dbl>, value_lead4 <dbl>, value_lead5 <dbl>, #> #   value_lead6 <dbl>, value_lead7 <dbl>, value_lead8 <dbl>, value_lead9 <dbl>, #> #   value_lead10 <dbl>, value_lead11 <dbl>, value_lead12 <dbl>, #> #   value_lead13 <dbl>, value_lead14 <dbl>, value_lead15 <dbl>, #> #   value_lead16 <dbl>, value_lead17 <dbl>, value_lead18 <dbl>, #> #   value_lead19 <dbl>, value_lead20 <dbl>"},{"path":"https://business-science.github.io/timetk/reference/tk_augment_slidify.html","id":null,"dir":"Reference","previous_headings":"","what":"Add many rolling window calculations to the data — tk_augment_slidify","title":"Add many rolling window calculations to the data — tk_augment_slidify","text":"Quickly use function rolling function apply multiple .periods. Works dplyr groups .","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_slidify.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add many rolling window calculations to the data — tk_augment_slidify","text":"","code":"tk_augment_slidify(   .data,   .value,   .period,   .f,   ...,   .align = c(\"center\", \"left\", \"right\"),   .partial = FALSE,   .names = \"auto\" )"},{"path":"https://business-science.github.io/timetk/reference/tk_augment_slidify.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add many rolling window calculations to the data — tk_augment_slidify","text":".data tibble. .value One column(s) transformation applied. Usage tidyselect functions (e.g. contains()) can used select multiple columns. .period One periods rolling window(s) .f summary [function / formula], ... Optional arguments summary function .align Rolling functions generate .period - 1 fewer values incoming vector. Thus, vector needs aligned. Select one \"center\", \"left\", \"right\". .partial .partial moving window allowed return partial (incomplete) windows instead NA values. Set FALSE default, can switched TRUE remove NA's. .names vector names new columns. Must length .period. Default \"auto\".","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_slidify.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add many rolling window calculations to the data — tk_augment_slidify","text":"Returns tibble object describing timeseries.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_slidify.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add many rolling window calculations to the data — tk_augment_slidify","text":"tk_augment_slidify() scales slidify_vec() function multiple time series .periods. See slidify_vec() examples usage core function arguments.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_augment_slidify.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add many rolling window calculations to the data — tk_augment_slidify","text":"","code":"library(dplyr)  # Single Column | Multiple Rolling Windows FANG %>%     select(symbol, date, adjusted) %>%     group_by(symbol) %>%     tk_augment_slidify(         .value   = contains(\"adjust\"),         # Multiple rolling windows         .period  = c(10, 30, 60, 90),         .f       = mean,         .partial = TRUE,         .names   = stringr::str_c(\"MA_\", c(10, 30, 60, 90))     ) %>%     ungroup() #> # A tibble: 4,032 × 7 #>    symbol date       adjusted MA_10 MA_30 MA_60 MA_90 #>    <chr>  <date>        <dbl> <dbl> <dbl> <dbl> <dbl> #>  1 FB     2013-01-02     28    28.9  30.0  29.7  29.1 #>  2 FB     2013-01-03     27.8  29.3  30.1  29.7  29.1 #>  3 FB     2013-01-04     28.8  29.6  30.2  29.7  29.0 #>  4 FB     2013-01-07     29.4  29.7  30.2  29.6  29.0 #>  5 FB     2013-01-08     29.1  29.8  30.3  29.6  29.0 #>  6 FB     2013-01-09     30.6  30.0  30.3  29.5  28.9 #>  7 FB     2013-01-10     31.3  30.2  30.3  29.4  28.9 #>  8 FB     2013-01-11     31.7  30.3  30.2  29.4  28.8 #>  9 FB     2013-01-14     31.0  30.4  30.1  29.3  28.8 #> 10 FB     2013-01-15     30.1  30.6  30.1  29.3  28.7 #> # ℹ 4,022 more rows  # Multiple Columns | Multiple Rolling Windows FANG %>%     select(symbol, date, adjusted, volume) %>%     group_by(symbol) %>%     tk_augment_slidify(         .value  = c(adjusted, volume),         .period  = c(10, 30, 60, 90),         .f       = mean,         .partial = TRUE     ) %>%     ungroup() #> # A tibble: 4,032 × 12 #>    symbol date       adjusted    volume adjusted_roll_10 volume_roll_10 #>    <chr>  <date>        <dbl>     <dbl>            <dbl>          <dbl> #>  1 FB     2013-01-02     28    69846400             28.9      73357200  #>  2 FB     2013-01-03     27.8  63140600             29.3      76494229. #>  3 FB     2013-01-04     28.8  72715400             29.6      78132200  #>  4 FB     2013-01-07     29.4  83781800             29.7      80438933. #>  5 FB     2013-01-08     29.1  45871300             29.8      89719300  #>  6 FB     2013-01-09     30.6 104787700             30.0      90267930  #>  7 FB     2013-01-10     31.3  95316400             30.2      87979540  #>  8 FB     2013-01-11     31.7  89598000             30.3      85671150  #>  9 FB     2013-01-14     31.0  98892800             30.4      82817300  #> 10 FB     2013-01-15     30.1 173242600             30.6      83120150  #> # ℹ 4,022 more rows #> # ℹ 6 more variables: adjusted_roll_30 <dbl>, volume_roll_30 <dbl>, #> #   adjusted_roll_60 <dbl>, volume_roll_60 <dbl>, adjusted_roll_90 <dbl>, #> #   volume_roll_90 <dbl>"},{"path":"https://business-science.github.io/timetk/reference/tk_augment_timeseries.html","id":null,"dir":"Reference","previous_headings":"","what":"Add many time series features to the data — tk_augment_timeseries","title":"Add many time series features to the data — tk_augment_timeseries","text":"Add many time series features data","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_timeseries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add many time series features to the data — tk_augment_timeseries","text":"","code":"tk_augment_timeseries_signature(.data, .date_var = NULL)"},{"path":"https://business-science.github.io/timetk/reference/tk_augment_timeseries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add many time series features to the data — tk_augment_timeseries","text":".data time-based tibble time-series object. .date_var tibbles, column containing either date date-time values. NULL, time-based column interpret object (tibble, xts, zoo, etc).","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_timeseries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add many time series features to the data — tk_augment_timeseries","text":"Returns tibble object describing timeseries.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_augment_timeseries.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add many time series features to the data — tk_augment_timeseries","text":"tk_augment_timeseries_signature() adds 25+ time series features including: Trend Seconds Granularity: index.num Yearly Seasonality: Year, Month, Quarter Weekly Seasonality: Week Month, Day Month, Day Week, Daily Seasonality: Hour, Minute, Second Weekly Cyclic Patterns: 2 weeks, 3 weeks, 4 weeks","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_augment_timeseries.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add many time series features to the data — tk_augment_timeseries","text":"","code":"library(dplyr)  m4_daily %>%     group_by(id) %>%     tk_augment_timeseries_signature(date) #> # A tibble: 9,743 × 31 #> # Groups:   id [4] #>    id    date       value  index.num  diff  year year.iso  half quarter month #>    <fct> <date>     <dbl>      <dbl> <dbl> <int>    <int> <int>   <int> <int> #>  1 D10   2014-07-03 2076. 1404345600    NA  2014     2014     2       3     7 #>  2 D10   2014-07-04 2073. 1404432000 86400  2014     2014     2       3     7 #>  3 D10   2014-07-05 2049. 1404518400 86400  2014     2014     2       3     7 #>  4 D10   2014-07-06 2049. 1404604800 86400  2014     2014     2       3     7 #>  5 D10   2014-07-07 2006. 1404691200 86400  2014     2014     2       3     7 #>  6 D10   2014-07-08 2018. 1404777600 86400  2014     2014     2       3     7 #>  7 D10   2014-07-09 2019. 1404864000 86400  2014     2014     2       3     7 #>  8 D10   2014-07-10 2007. 1404950400 86400  2014     2014     2       3     7 #>  9 D10   2014-07-11 2010  1405036800 86400  2014     2014     2       3     7 #> 10 D10   2014-07-12 2002. 1405123200 86400  2014     2014     2       3     7 #> # ℹ 9,733 more rows #> # ℹ 21 more variables: month.xts <int>, month.lbl <ord>, day <int>, hour <int>, #> #   minute <int>, second <int>, hour12 <int>, am.pm <int>, wday <int>, #> #   wday.xts <int>, wday.lbl <ord>, mday <int>, qday <int>, yday <int>, #> #   mweek <int>, week <int>, week.iso <int>, week2 <int>, week3 <int>, #> #   week4 <int>, mday7 <int>"},{"path":"https://business-science.github.io/timetk/reference/tk_get_frequency.html","id":null,"dir":"Reference","previous_headings":"","what":"Automatic frequency and trend calculation from a time series index — tk_get_frequency","title":"Automatic frequency and trend calculation from a time series index — tk_get_frequency","text":"Automatic frequency trend calculation time series index","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_get_frequency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Automatic frequency and trend calculation from a time series index — tk_get_frequency","text":"","code":"tk_get_frequency(idx, period = \"auto\", message = TRUE)  tk_get_trend(idx, period = \"auto\", message = TRUE)"},{"path":"https://business-science.github.io/timetk/reference/tk_get_frequency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Automatic frequency and trend calculation from a time series index — tk_get_frequency","text":"idx date datetime index. period Either \"auto\", time-based definition (e.g. \"2 weeks\"), numeric number observations per frequency (e.g. 10). message boolean. message = TRUE, frequency trend output message along units scale data.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_get_frequency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Automatic frequency and trend calculation from a time series index — tk_get_frequency","text":"Returns scalar numeric value indicating number observations frequency trend span.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_get_frequency.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Automatic frequency and trend calculation from a time series index — tk_get_frequency","text":"frequency loosely defined number observations comprise cycle data set. trend loosely defined time span can aggregated across visualize central tendency data. often easiest think frequency trend terms time-based units data already . tk_get_frequency() time_trend() enable: using time-based periods define frequency trend. Frequency: example, weekly cycle often 5-days (working days) 7-days (calendar days). Rather specify frequency 5 7, user can specify period = \"1 week\", tk_get_frequency() detect scale time series return 5 7 based actual data. period argument three basic options returning frequency. Options include: \"auto\": target frequency determined using pre-defined template (see template ). time-based duration: (e.g. \"1 week\" \"2 quarters\" per cycle) numeric number observations: (e.g. 5 5 observations per cycle) period = \"auto\", tk_time_scale_template() used calculate frequency. Trend: example, trend daily data often best aggregated evaluating moving average quarter month span. Rather specify number days quarter month, user can specify \"1 quarter\" \"1 month\", time_trend() function return correct number observations per trend cycle. addition, option, period = \"auto\", auto-detect appropriate trend span depending data. template used define appropriate trend span. Time Scale Template tk_time_scale_template() Look-Table used trend period find appropriate time scale. contains three features: time_scale, frequency, trend. algorithm inspect scale time series select best frequency trend matches scale number observations per target frequency. frequency chosen best match. predefined template stored function tk_time_scale_template(). can modify template set_tk_time_scale_template().","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_get_frequency.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Automatic frequency and trend calculation from a time series index — tk_get_frequency","text":"","code":"library(dplyr)  idx_FB <- FANG %>%     filter(symbol == \"FB\") %>%     pull(date)  # Automated Frequency Calculation tk_get_frequency(idx_FB, period = \"auto\") #> frequency = 5 observations per 1 week #> [1] 5  # Automated Trend Calculation tk_get_trend(idx_FB, period = \"auto\") #> trend = 64 observations per 3 months #> [1] 64  # Manually Override Trend tk_get_trend(idx_FB, period = \"1 year\") #> trend = 252 observations per 1 year #> [1] 252"},{"path":"https://business-science.github.io/timetk/reference/tk_get_holiday.html","id":null,"dir":"Reference","previous_headings":"","what":"Get holiday features from a time-series index — tk_get_holiday","title":"Get holiday features from a time-series index — tk_get_holiday","text":"Get holiday features time-series index","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_get_holiday.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get holiday features from a time-series index — tk_get_holiday","text":"","code":"tk_get_holiday_signature(   idx,   holiday_pattern = \".\",   locale_set = c(\"all\", \"none\", \"World\", \"US\", \"CA\", \"GB\", \"FR\", \"IT\", \"JP\", \"CH\", \"DE\"),   exchange_set = c(\"all\", \"none\", \"NYSE\", \"LONDON\", \"NERC\", \"TSX\", \"ZURICH\") )  tk_get_holidays_by_year(years = year(today()))"},{"path":"https://business-science.github.io/timetk/reference/tk_get_holiday.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get holiday features from a time-series index — tk_get_holiday","text":"idx time-series index vector dates datetimes. holiday_pattern regular expression pattern search \"Holiday Set\". locale_set Return binary holidays based locale. One : \"\", \"none\", \"World\", \"US\", \"CA\", \"GB\", \"FR\", \"\", \"JP\", \"CH\", \"DE\". exchange_set Return binary holidays based Stock Exchange Calendars. One : \"\", \"none\", \"NYSE\", \"LONDON\", \"NERC\", \"TSX\", \"ZURICH\". years One years collect holidays .","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_get_holiday.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get holiday features from a time-series index — tk_get_holiday","text":"Returns tibble object describing timeseries holidays.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_get_holiday.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get holiday features from a time-series index — tk_get_holiday","text":"Feature engineering holidays can help identify critical patterns machine learning algorithms. tk_get_holiday_signature() helps providing feature sets 3 types features: 1. Individual Holidays single holiday features can filtered using pattern. helps identifying holidays important machine learning model. can useful example e-commerce initiatives (e.g. sales Christmas Thanskgiving). 2. Locale-Based Summary Sets Locale-based holdiay sets useful e-commerce initiatives (e.g. sales Christmas Thanskgiving). Filter locale identify holidays locale. 3. Stock Exchange Calendar Summary Sets Exchange-based holdiay sets useful identifying non-working days. Filter index identify holidays commonly non-working.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_get_holiday.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get holiday features from a time-series index — tk_get_holiday","text":"","code":"library(dplyr) library(stringr) #>  #> Attaching package: ‘stringr’ #> The following object is masked from ‘package:recipes’: #>  #>     fixed  # Works with time-based tibbles idx <- tk_make_timeseries(\"2017-01-01\", \"2017-12-31\", by = \"day\")  # --- BASIC USAGE ----  tk_get_holiday_signature(idx) #> # A tibble: 365 × 133 #>    index      exch_NYSE exch_LONDON exch_NERC exch_TSX exch_ZURICH locale_JP #>    <date>         <dbl>       <dbl>     <dbl>    <dbl>       <dbl>     <dbl> #>  1 2017-01-01         0           0         0        0           0         1 #>  2 2017-01-02         1           1         1        1           1         1 #>  3 2017-01-03         0           0         0        0           0         1 #>  4 2017-01-04         0           0         0        0           0         0 #>  5 2017-01-05         0           0         0        0           0         0 #>  6 2017-01-06         0           0         0        0           0         0 #>  7 2017-01-07         0           0         0        0           0         0 #>  8 2017-01-08         0           0         0        0           0         0 #>  9 2017-01-09         0           0         0        0           0         1 #> 10 2017-01-10         0           0         0        0           0         0 #> # ℹ 355 more rows #> # ℹ 126 more variables: locale_US <dbl>, locale_World <dbl>, locale_CH <dbl>, #> #   locale_IT <dbl>, locale_CA <dbl>, locale_GB <dbl>, locale_FR <dbl>, #> #   locale_DE <dbl>, JP_Gantan <dbl>, JP_NewYearsDay <dbl>, #> #   World_NewYearsDay <dbl>, World_SolemnityOfMary <dbl>, US_NewYearsDay <dbl>, #> #   CH_BerchtoldsDay <dbl>, JP_BankHolidayJan2 <dbl>, JP_BankHolidayJan3 <dbl>, #> #   World_Epiphany <dbl>, IT_Epiphany <dbl>, JP_ComingOfAgeDay <dbl>, …  # ---- FILTERING WITH PATTERNS & SETS ----  # List available holidays - see patterns tk_get_holidays_by_year(2020) %>%     filter(holiday_name %>% str_detect(\"US_\")) #> # A tibble: 17 × 3 #>    date       locale holiday_name             #>    <date>     <chr>  <chr>                    #>  1 2020-01-01 US     US_NewYearsDay           #>  2 2020-01-20 US     US_InaugurationDay       #>  3 2020-01-20 US     US_MLKingsBirthday       #>  4 2020-02-12 US     US_LincolnsBirthday      #>  5 2020-02-17 US     US_PresidentsDay         #>  6 2020-02-22 US     US_WashingtonsBirthday   #>  7 2020-03-02 US     US_CPulaskisBirthday     #>  8 2020-04-10 US     US_GoodFriday            #>  9 2020-05-25 US     US_MemorialDay           #> 10 2020-05-30 US     US_DecorationMemorialDay #> 11 2020-07-04 US     US_IndependenceDay       #> 12 2020-09-07 US     US_LaborDay              #> 13 2020-10-12 US     US_ColumbusDay           #> 14 2020-11-03 US     US_ElectionDay           #> 15 2020-11-11 US     US_VeteransDay           #> 16 2020-11-26 US     US_ThanksgivingDay       #> 17 2020-12-25 US     US_ChristmasDay           # Filter using holiday patterns # - Get New Years, Christmas and Thanksgiving Features in US tk_get_holiday_signature(     idx,     holiday_pattern = \"(US_NewYears)|(US_Christmas)|(US_Thanks)\",     locale_set      = \"none\",     exchange_set    = \"none\") #> # A tibble: 365 × 4 #>    index      US_NewYearsDay US_ThanksgivingDay US_ChristmasDay #>    <date>              <dbl>              <dbl>           <dbl> #>  1 2017-01-01              1                  0               0 #>  2 2017-01-02              0                  0               0 #>  3 2017-01-03              0                  0               0 #>  4 2017-01-04              0                  0               0 #>  5 2017-01-05              0                  0               0 #>  6 2017-01-06              0                  0               0 #>  7 2017-01-07              0                  0               0 #>  8 2017-01-08              0                  0               0 #>  9 2017-01-09              0                  0               0 #> 10 2017-01-10              0                  0               0 #> # ℹ 355 more rows  # ---- APPLYING FILTERS ----  # Filter with locale sets - Signals all holidays in a locale tk_get_holiday_signature(     idx,     holiday_pattern = \"$^\", # Matches nothing on purpose     locale_set      = \"US\",     exchange_set    = \"none\") #> # A tibble: 365 × 2 #>    index      locale_US #>    <date>         <dbl> #>  1 2017-01-01         1 #>  2 2017-01-02         0 #>  3 2017-01-03         0 #>  4 2017-01-04         0 #>  5 2017-01-05         0 #>  6 2017-01-06         0 #>  7 2017-01-07         0 #>  8 2017-01-08         0 #>  9 2017-01-09         0 #> 10 2017-01-10         0 #> # ℹ 355 more rows  # Filter with exchange sets - Signals Common Non-Business Days tk_get_holiday_signature(     idx,     holiday_pattern = \"$^\", # Matches nothing on purpose     locale_set      = \"none\",     exchange_set    = \"NYSE\") #> # A tibble: 365 × 2 #>    index      exch_NYSE #>    <date>         <dbl> #>  1 2017-01-01         0 #>  2 2017-01-02         1 #>  3 2017-01-03         0 #>  4 2017-01-04         0 #>  5 2017-01-05         0 #>  6 2017-01-06         0 #>  7 2017-01-07         0 #>  8 2017-01-08         0 #>  9 2017-01-09         0 #> 10 2017-01-10         0 #> # ℹ 355 more rows"},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries.html","id":null,"dir":"Reference","previous_headings":"","what":"Get date features from a time-series index — tk_get_timeseries","title":"Get date features from a time-series index — tk_get_timeseries","text":"Get date features time-series index","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get date features from a time-series index — tk_get_timeseries","text":"","code":"tk_get_timeseries_signature(idx)  tk_get_timeseries_summary(idx)"},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get date features from a time-series index — tk_get_timeseries","text":"idx time-series index vector dates datetimes.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get date features from a time-series index — tk_get_timeseries","text":"Returns tibble object describing timeseries.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get date features from a time-series index — tk_get_timeseries","text":"tk_get_timeseries_signature decomposes timeseries commonly needed features numeric value, differences, year, month, day, day week, day month, day year, hour, minute, second. tk_get_timeseries_summary returns summary returns start, end, units, scale, \"summary\" timeseries differences seconds including minimum, 1st quartile, median, mean, 3rd quartile, maximum frequency. timeseries differences give user better picture index frequency user can understand level regularity irregularity. perfectly regular time series equal values seconds metric. However, often case. Important Note: functions work time-based indexes datetime, date, yearmon, yearqtr values. Regularized dates decomposed.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get date features from a time-series index — tk_get_timeseries","text":"","code":"library(dplyr) library(lubridate) library(zoo) #>  #> Attaching package: ‘zoo’ #> The following objects are masked from ‘package:base’: #>  #>     as.Date, as.Date.numeric  # Works with time-based tibbles FB_tbl <- FANG %>% dplyr::filter(symbol == \"FB\") FB_idx <- tk_index(FB_tbl)  tk_get_timeseries_signature(FB_idx) #> # A tibble: 1,008 × 29 #>    index       index.num   diff  year year.iso  half quarter month month.xts #>    <date>          <dbl>  <dbl> <int>    <int> <int>   <int> <int>     <int> #>  1 2013-01-02 1357084800     NA  2013     2013     1       1     1         0 #>  2 2013-01-03 1357171200  86400  2013     2013     1       1     1         0 #>  3 2013-01-04 1357257600  86400  2013     2013     1       1     1         0 #>  4 2013-01-07 1357516800 259200  2013     2013     1       1     1         0 #>  5 2013-01-08 1357603200  86400  2013     2013     1       1     1         0 #>  6 2013-01-09 1357689600  86400  2013     2013     1       1     1         0 #>  7 2013-01-10 1357776000  86400  2013     2013     1       1     1         0 #>  8 2013-01-11 1357862400  86400  2013     2013     1       1     1         0 #>  9 2013-01-14 1358121600 259200  2013     2013     1       1     1         0 #> 10 2013-01-15 1358208000  86400  2013     2013     1       1     1         0 #> # ℹ 998 more rows #> # ℹ 20 more variables: month.lbl <ord>, day <int>, hour <int>, minute <int>, #> #   second <int>, hour12 <int>, am.pm <int>, wday <int>, wday.xts <int>, #> #   wday.lbl <ord>, mday <int>, qday <int>, yday <int>, mweek <int>, #> #   week <int>, week.iso <int>, week2 <int>, week3 <int>, week4 <int>, #> #   mday7 <int> tk_get_timeseries_summary(FB_idx) #> # A tibble: 1 × 12 #>   n.obs start      end        units scale tzone diff.minimum diff.q1 diff.median #>   <int> <date>     <date>     <chr> <chr> <chr>        <dbl>   <dbl>       <dbl> #> 1  1008 2013-01-02 2016-12-30 days  day   UTC          86400   86400       86400 #> # ℹ 3 more variables: diff.mean <dbl>, diff.q3 <dbl>, diff.maximum <dbl>   # Works with dates in any periodicity idx_weekly <- seq.Date(from = lubridate::ymd(\"2016-01-01\"), by = 'week', length.out = 6)  tk_get_timeseries_signature(idx_weekly) #> # A tibble: 6 × 29 #>   index       index.num   diff  year year.iso  half quarter month month.xts #>   <date>          <dbl>  <dbl> <int>    <int> <int>   <int> <int>     <int> #> 1 2016-01-01 1451606400     NA  2016     2015     1       1     1         0 #> 2 2016-01-08 1452211200 604800  2016     2016     1       1     1         0 #> 3 2016-01-15 1452816000 604800  2016     2016     1       1     1         0 #> 4 2016-01-22 1453420800 604800  2016     2016     1       1     1         0 #> 5 2016-01-29 1454025600 604800  2016     2016     1       1     1         0 #> 6 2016-02-05 1454630400 604800  2016     2016     1       1     2         1 #> # ℹ 20 more variables: month.lbl <ord>, day <int>, hour <int>, minute <int>, #> #   second <int>, hour12 <int>, am.pm <int>, wday <int>, wday.xts <int>, #> #   wday.lbl <ord>, mday <int>, qday <int>, yday <int>, mweek <int>, #> #   week <int>, week.iso <int>, week2 <int>, week3 <int>, week4 <int>, #> #   mday7 <int> tk_get_timeseries_summary(idx_weekly) #> # A tibble: 1 × 12 #>   n.obs start      end        units scale tzone diff.minimum diff.q1 diff.median #>   <int> <date>     <date>     <chr> <chr> <chr>        <dbl>   <dbl>       <dbl> #> 1     6 2016-01-01 2016-02-05 days  week  UTC         604800  604800      604800 #> # ℹ 3 more variables: diff.mean <dbl>, diff.q3 <dbl>, diff.maximum <dbl>   # Works with zoo yearmon and yearqtr classes idx_yearmon <- seq.Date(from       = lubridate::ymd(\"2016-01-01\"),                         by         = \"month\",                         length.out = 12) %>%     zoo::as.yearmon()  tk_get_timeseries_signature(idx_yearmon) #> # A tibble: 12 × 29 #>    index      index.num    diff  year year.iso  half quarter month month.xts #>    <yearmon>      <dbl>   <dbl> <int>    <int> <int>   <int> <int>     <int> #>  1 Jan 2016  1451606400      NA  2016     2015     1       1     1         0 #>  2 Feb 2016  1454284800 2678400  2016     2016     1       1     2         1 #>  3 Mar 2016  1456790400 2505600  2016     2016     1       1     3         2 #>  4 Apr 2016  1459468800 2678400  2016     2016     1       2     4         3 #>  5 May 2016  1462060800 2592000  2016     2016     1       2     5         4 #>  6 Jun 2016  1464739200 2678400  2016     2016     1       2     6         5 #>  7 Jul 2016  1467331200 2592000  2016     2016     2       3     7         6 #>  8 Aug 2016  1470009600 2678400  2016     2016     2       3     8         7 #>  9 Sep 2016  1472688000 2678400  2016     2016     2       3     9         8 #> 10 Oct 2016  1475280000 2592000  2016     2016     2       4    10         9 #> 11 Nov 2016  1477958400 2678400  2016     2016     2       4    11        10 #> 12 Dec 2016  1480550400 2592000  2016     2016     2       4    12        11 #> # ℹ 20 more variables: month.lbl <ord>, day <int>, hour <int>, minute <int>, #> #   second <int>, hour12 <int>, am.pm <int>, wday <int>, wday.xts <int>, #> #   wday.lbl <ord>, mday <int>, qday <int>, yday <int>, mweek <int>, #> #   week <int>, week.iso <int>, week2 <int>, week3 <int>, week4 <int>, #> #   mday7 <int> tk_get_timeseries_summary(idx_yearmon) #> # A tibble: 1 × 12 #>   n.obs start end   units scale tzone diff.minimum diff.q1 diff.median diff.mean #>   <int> <yea> <yea> <chr> <chr> <chr>        <dbl>   <dbl>       <dbl>     <dbl> #> 1    12 Jan … Dec … days  month UTC        2505600 2592000     2678400  2631273. #> # ℹ 2 more variables: diff.q3 <dbl>, diff.maximum <dbl>"},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries_unit_frequency.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the timeseries unit frequency for the primary time scales — tk_get_timeseries_unit_frequency","title":"Get the timeseries unit frequency for the primary time scales — tk_get_timeseries_unit_frequency","text":"Get timeseries unit frequency primary time scales","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries_unit_frequency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the timeseries unit frequency for the primary time scales — tk_get_timeseries_unit_frequency","text":"","code":"tk_get_timeseries_unit_frequency()"},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries_unit_frequency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the timeseries unit frequency for the primary time scales — tk_get_timeseries_unit_frequency","text":"tk_get_timeseries_unit_frequency returns tibble containing timeseries frequencies seconds primary time scales including \"sec\", \"min\", \"hour\", \"day\", \"week\", \"month\", \"quarter\", \"year\".","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries_unit_frequency.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the timeseries unit frequency for the primary time scales — tk_get_timeseries_unit_frequency","text":"","code":"tk_get_timeseries_unit_frequency() #> # A tibble: 1 × 8 #>     sec   min  hour   day   week   month quarter     year #>   <dbl> <dbl> <dbl> <dbl>  <dbl>   <dbl>   <dbl>    <dbl> #> 1     0    60  3600 86400 604800 2678400 7948800 31795200"},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries_variables.html","id":null,"dir":"Reference","previous_headings":"","what":"Get date or datetime variables (column names) — tk_get_timeseries_variables","title":"Get date or datetime variables (column names) — tk_get_timeseries_variables","text":"Get date datetime variables (column names)","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries_variables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get date or datetime variables (column names) — tk_get_timeseries_variables","text":"","code":"tk_get_timeseries_variables(data)"},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries_variables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get date or datetime variables (column names) — tk_get_timeseries_variables","text":"data object class data.frame","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries_variables.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get date or datetime variables (column names) — tk_get_timeseries_variables","text":"tk_get_timeseries_variables returns vector containing column names date-like classes.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries_variables.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get date or datetime variables (column names) — tk_get_timeseries_variables","text":"tk_get_timeseries_variables returns column names date datetime variables data frame. Classes meet criteria return include inherit POSIXt, Date, zoo::yearmon, zoo::yearqtr. Function adapted padr:::get_date_variables(). See padr helpers.R","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_get_timeseries_variables.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get date or datetime variables (column names) — tk_get_timeseries_variables","text":"","code":"library(dplyr)  FANG %>%     tk_get_timeseries_variables() #> [1] \"date\""},{"path":"https://business-science.github.io/timetk/reference/tk_index.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract an index of date or datetime from time series objects, models, forecasts — tk_index","title":"Extract an index of date or datetime from time series objects, models, forecasts — tk_index","text":"Extract index date datetime time series objects, models, forecasts","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_index.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract an index of date or datetime from time series objects, models, forecasts — tk_index","text":"","code":"tk_index(data, timetk_idx = FALSE, silent = FALSE)  has_timetk_idx(data)"},{"path":"https://business-science.github.io/timetk/reference/tk_index.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract an index of date or datetime from time series objects, models, forecasts — tk_index","text":"data time-based tibble, time-series object, time-series model, forecast object. timetk_idx timetk_idx TRUE timetk time-based index attribute attempted returned. FALSE default index returned. See discussion details. silent Used toggle printing messages warnings.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_index.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract an index of date or datetime from time series objects, models, forecasts — tk_index","text":"Returns vector date date times","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_index.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract an index of date or datetime from time series objects, models, forecasts — tk_index","text":"tk_index() used extract date datetime index various time series objects, models forecasts. method can used tbl, xts, zoo, zooreg, ts objects. method can additionally used forecast objects number objects generated modeling functions Arima, ets, HoltWinters classes get index underlying data. boolean timetk_idx argument applicable regularized time series objects ts zooreg classes regularized index potentially \"timetk index\" (time-based attribute). set FALSE regularized index returned. set TRUE time-based timetk index returned present. has_timetk_idx() used determine object \"timetk index\" attribute can thus benefit tk_index(timetk_idx = TRUE). TRUE indicates \"timetk index\" attribute present. FALSE indicates \"timetk index\" attribute present. FALSE, tk_index() function return default index data type. Important Note: gain benefit timetk_idx time series must timetk index. Use has_timetk_idx determine object timetk index. particularly important ts objects, default contain time-based index therefore must coerced time-based objects tbl, xts, zoo using tk_ts() function order get \"timetk index\" attribute. Refer tk_ts() creating persistent date / datetime index coercion ts.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_index.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract an index of date or datetime from time series objects, models, forecasts — tk_index","text":"","code":"# Create time-based tibble data_tbl <- tibble::tibble(     date = seq.Date(from = as.Date(\"2000-01-01\"), by = 1, length.out = 5),     x    = rnorm(5) * 10,     y    = 5:1 ) tk_index(data_tbl) # Returns time-based index vector #> [1] \"2000-01-01\" \"2000-01-02\" \"2000-01-03\" \"2000-01-04\" \"2000-01-05\"  # Coerce to ts using tk_ts(): Preserves time-basis data_ts <- tk_ts(data_tbl) #> Warning: Non-numeric columns being dropped: date tk_index(data_ts, timetk_idx = FALSE) # Returns regularized index #> [1] 1 2 3 4 5 tk_index(data_ts, timetk_idx = TRUE)  # Returns original time-based index vector #> Warning: 'tzone' attributes are inconsistent #> [1] \"2000-01-01\" \"2000-01-02\" \"2000-01-03\" \"2000-01-04\" \"2000-01-05\"  # Coercing back to tbl tk_tbl(data_ts, timetk_idx = FALSE) # Returns regularized tbl #> Warning: Warning: No index to preserve. Object otherwise converted to tibble successfully. #> # A tibble: 5 × 2 #>        x     y #>    <dbl> <dbl> #> 1 -14.0      5 #> 2   2.59     4 #> 3  -4.42     3 #> 4   5.69     2 #> 5  21.3      1 tk_tbl(data_ts, timetk_idx = TRUE)  # Returns time-based tbl #> Warning: 'tzone' attributes are inconsistent #> # A tibble: 5 × 3 #>   index           x     y #>   <date>      <dbl> <dbl> #> 1 2000-01-01 -14.0      5 #> 2 2000-01-02   2.59     4 #> 3 2000-01-03  -4.42     3 #> 4 2000-01-04   5.69     2 #> 5 2000-01-05  21.3      1"},{"path":"https://business-science.github.io/timetk/reference/tk_make_future_timeseries.html","id":null,"dir":"Reference","previous_headings":"","what":"Make future time series from existing — tk_make_future_timeseries","title":"Make future time series from existing — tk_make_future_timeseries","text":"Make future time series existing","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_make_future_timeseries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make future time series from existing — tk_make_future_timeseries","text":"","code":"tk_make_future_timeseries(   idx,   length_out,   inspect_weekdays = FALSE,   inspect_months = FALSE,   skip_values = NULL,   insert_values = NULL,   n_future = NULL )"},{"path":"https://business-science.github.io/timetk/reference/tk_make_future_timeseries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make future time series from existing — tk_make_future_timeseries","text":"idx vector dates length_out Number future observations. Can numeric number phrase like \"1 year\". inspect_weekdays Uses logistic regression algorithm inspect whether certain weekdays (e.g. weekends) excluded future dates. Default FALSE. inspect_months Uses logistic regression algorithm inspect whether certain days months (e.g. last two weeks year seasonal days) excluded future dates. Default FALSE. skip_values vector class idx timeseries values skip. insert_values vector class idx timeseries values insert. n_future (DEPRECATED) Number future observations. Can numeric number phrase like \"1 year\".","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_make_future_timeseries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make future time series from existing — tk_make_future_timeseries","text":"vector containing future index class incoming index idx","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_make_future_timeseries.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make future time series from existing — tk_make_future_timeseries","text":"Future Sequences tk_make_future_timeseries returns time series based input index frequency attributes. Specifying Length Future Observations argument length_out determines many future index observations compute. can specified : numeric value - number future observations return. number observations returned always equal value user inputs. end date can vary based number timestamps chosen. time-based phrase - duration future include (e.g. \"6 months\" \"30 minutes\"). duration defines end date observations. end date change timestamps fall within end date returned (e.g. quarterly time series return 4 quarters length_out = \"1 year\"). number observations vary fit within end date. Weekday Month Inspection inspect_weekdays inspect_months arguments apply \"daily\" (scale = \"day\") data (refer tk_get_timeseries_summary() get index scale). inspect_weekdays argument useful determining missing days week occur weekly frequency every week, every week, . recommended least 60 days use option. inspect_months argument useful determining missing days month, quarter year; however, algorithm can inadvertently select incorrect dates pattern erratic. Skipping / Inserting Values skip_values insert_values arguments can used remove add values series future times. values must format idx class. skip_values argument useful passing holidays special index values excluded future time series. insert_values argument useful adding values back algorithm may excluded.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_make_future_timeseries.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make future time series from existing — tk_make_future_timeseries","text":"","code":"library(dplyr)  # Basic example - By 3 seconds idx <- tk_make_timeseries(\"2016-01-01 00:00:00\", by = \"3 sec\", length_out = 3) idx #> [1] \"2016-01-01 00:00:00 UTC\" \"2016-01-01 00:00:03 UTC\" #> [3] \"2016-01-01 00:00:06 UTC\"  # Make next three timestamps in series idx %>% tk_make_future_timeseries(length_out = 3) #> [1] \"2016-01-01 00:00:09 UTC\" \"2016-01-01 00:00:12 UTC\" #> [3] \"2016-01-01 00:00:15 UTC\"  # Make next 6 seconds of timestamps from the next timestamp idx %>% tk_make_future_timeseries(length_out = \"6 sec\") #> [1] \"2016-01-01 00:00:09 UTC\" \"2016-01-01 00:00:12 UTC\"   # Basic Example - By 1 Month idx <- tk_make_timeseries(\"2016-01-01\", by = \"1 month\",                           length_out = \"12 months\") idx #>  [1] \"2016-01-01\" \"2016-02-01\" \"2016-03-01\" \"2016-04-01\" \"2016-05-01\" #>  [6] \"2016-06-01\" \"2016-07-01\" \"2016-08-01\" \"2016-09-01\" \"2016-10-01\" #> [11] \"2016-11-01\" \"2016-12-01\" \"2017-01-01\"  # Make 12 months of timestamps from the next timestamp idx %>% tk_make_future_timeseries(length_out = \"12 months\") #>  [1] \"2017-02-01\" \"2017-03-01\" \"2017-04-01\" \"2017-05-01\" \"2017-06-01\" #>  [6] \"2017-07-01\" \"2017-08-01\" \"2017-09-01\" \"2017-10-01\" \"2017-11-01\" #> [11] \"2017-12-01\" \"2018-01-01\"    # --- APPLICATION --- # - Combine holiday sequences with future sequences  # Create index of days that FB stock will be traded in 2017 based on 2016 + holidays FB_tbl <- FANG %>% dplyr::filter(symbol == \"FB\")  holidays <- tk_make_holiday_sequence(     start_date = \"2017-01-01\",     end_date   = \"2017-12-31\",     calendar   = \"NYSE\")  # Remove holidays with skip_values, and remove weekends with inspect_weekdays = TRUE FB_tbl %>%     tk_index() %>%     tk_make_future_timeseries(length_out       = \"1 year\",                               inspect_weekdays = TRUE,                               skip_values      = holidays) #>   [1] \"2017-01-03\" \"2017-01-04\" \"2017-01-05\" \"2017-01-06\" \"2017-01-09\" #>   [6] \"2017-01-10\" \"2017-01-11\" \"2017-01-12\" \"2017-01-13\" \"2017-01-17\" #>  [11] \"2017-01-18\" \"2017-01-19\" \"2017-01-20\" \"2017-01-23\" \"2017-01-24\" #>  [16] \"2017-01-25\" \"2017-01-26\" \"2017-01-27\" \"2017-01-30\" \"2017-01-31\" #>  [21] \"2017-02-01\" \"2017-02-02\" \"2017-02-03\" \"2017-02-06\" \"2017-02-07\" #>  [26] \"2017-02-08\" \"2017-02-09\" \"2017-02-10\" \"2017-02-13\" \"2017-02-14\" #>  [31] \"2017-02-15\" \"2017-02-16\" \"2017-02-17\" \"2017-02-21\" \"2017-02-22\" #>  [36] \"2017-02-23\" \"2017-02-24\" \"2017-02-27\" \"2017-02-28\" \"2017-03-01\" #>  [41] \"2017-03-02\" \"2017-03-03\" \"2017-03-06\" \"2017-03-07\" \"2017-03-08\" #>  [46] \"2017-03-09\" \"2017-03-10\" \"2017-03-13\" \"2017-03-14\" \"2017-03-15\" #>  [51] \"2017-03-16\" \"2017-03-17\" \"2017-03-20\" \"2017-03-21\" \"2017-03-22\" #>  [56] \"2017-03-23\" \"2017-03-24\" \"2017-03-27\" \"2017-03-28\" \"2017-03-29\" #>  [61] \"2017-03-30\" \"2017-03-31\" \"2017-04-03\" \"2017-04-04\" \"2017-04-05\" #>  [66] \"2017-04-06\" \"2017-04-07\" \"2017-04-10\" \"2017-04-11\" \"2017-04-12\" #>  [71] \"2017-04-13\" \"2017-04-17\" \"2017-04-18\" \"2017-04-19\" \"2017-04-20\" #>  [76] \"2017-04-21\" \"2017-04-24\" \"2017-04-25\" \"2017-04-26\" \"2017-04-27\" #>  [81] \"2017-04-28\" \"2017-05-01\" \"2017-05-02\" \"2017-05-03\" \"2017-05-04\" #>  [86] \"2017-05-05\" \"2017-05-08\" \"2017-05-09\" \"2017-05-10\" \"2017-05-11\" #>  [91] \"2017-05-12\" \"2017-05-15\" \"2017-05-16\" \"2017-05-17\" \"2017-05-18\" #>  [96] \"2017-05-19\" \"2017-05-22\" \"2017-05-23\" \"2017-05-24\" \"2017-05-25\" #> [101] \"2017-05-26\" \"2017-05-30\" \"2017-05-31\" \"2017-06-01\" \"2017-06-02\" #> [106] \"2017-06-05\" \"2017-06-06\" \"2017-06-07\" \"2017-06-08\" \"2017-06-09\" #> [111] \"2017-06-12\" \"2017-06-13\" \"2017-06-14\" \"2017-06-15\" \"2017-06-16\" #> [116] \"2017-06-19\" \"2017-06-20\" \"2017-06-21\" \"2017-06-22\" \"2017-06-23\" #> [121] \"2017-06-26\" \"2017-06-27\" \"2017-06-28\" \"2017-06-29\" \"2017-06-30\" #> [126] \"2017-07-03\" \"2017-07-05\" \"2017-07-06\" \"2017-07-07\" \"2017-07-10\" #> [131] \"2017-07-11\" \"2017-07-12\" \"2017-07-13\" \"2017-07-14\" \"2017-07-17\" #> [136] \"2017-07-18\" \"2017-07-19\" \"2017-07-20\" \"2017-07-21\" \"2017-07-24\" #> [141] \"2017-07-25\" \"2017-07-26\" \"2017-07-27\" \"2017-07-28\" \"2017-07-31\" #> [146] \"2017-08-01\" \"2017-08-02\" \"2017-08-03\" \"2017-08-04\" \"2017-08-07\" #> [151] \"2017-08-08\" \"2017-08-09\" \"2017-08-10\" \"2017-08-11\" \"2017-08-14\" #> [156] \"2017-08-15\" \"2017-08-16\" \"2017-08-17\" \"2017-08-18\" \"2017-08-21\" #> [161] \"2017-08-22\" \"2017-08-23\" \"2017-08-24\" \"2017-08-25\" \"2017-08-28\" #> [166] \"2017-08-29\" \"2017-08-30\" \"2017-08-31\" \"2017-09-01\" \"2017-09-05\" #> [171] \"2017-09-06\" \"2017-09-07\" \"2017-09-08\" \"2017-09-11\" \"2017-09-12\" #> [176] \"2017-09-13\" \"2017-09-14\" \"2017-09-15\" \"2017-09-18\" \"2017-09-19\" #> [181] \"2017-09-20\" \"2017-09-21\" \"2017-09-22\" \"2017-09-25\" \"2017-09-26\" #> [186] \"2017-09-27\" \"2017-09-28\" \"2017-09-29\" \"2017-10-02\" \"2017-10-03\" #> [191] \"2017-10-04\" \"2017-10-05\" \"2017-10-06\" \"2017-10-09\" \"2017-10-10\" #> [196] \"2017-10-11\" \"2017-10-12\" \"2017-10-13\" \"2017-10-16\" \"2017-10-17\" #> [201] \"2017-10-18\" \"2017-10-19\" \"2017-10-20\" \"2017-10-23\" \"2017-10-24\" #> [206] \"2017-10-25\" \"2017-10-26\" \"2017-10-27\" \"2017-10-30\" \"2017-10-31\" #> [211] \"2017-11-01\" \"2017-11-02\" \"2017-11-03\" \"2017-11-06\" \"2017-11-07\" #> [216] \"2017-11-08\" \"2017-11-09\" \"2017-11-10\" \"2017-11-13\" \"2017-11-14\" #> [221] \"2017-11-15\" \"2017-11-16\" \"2017-11-17\" \"2017-11-20\" \"2017-11-21\" #> [226] \"2017-11-22\" \"2017-11-24\" \"2017-11-27\" \"2017-11-28\" \"2017-11-29\" #> [231] \"2017-11-30\" \"2017-12-01\" \"2017-12-04\" \"2017-12-05\" \"2017-12-06\" #> [236] \"2017-12-07\" \"2017-12-08\" \"2017-12-11\" \"2017-12-12\" \"2017-12-13\" #> [241] \"2017-12-14\" \"2017-12-15\" \"2017-12-18\" \"2017-12-19\" \"2017-12-20\" #> [246] \"2017-12-21\" \"2017-12-22\" \"2017-12-26\" \"2017-12-27\" \"2017-12-28\" #> [251] \"2017-12-29\""},{"path":"https://business-science.github.io/timetk/reference/tk_make_holiday_sequence.html","id":null,"dir":"Reference","previous_headings":"","what":"Make daily Holiday and Weekend date sequences — tk_make_holiday_sequence","title":"Make daily Holiday and Weekend date sequences — tk_make_holiday_sequence","text":"Make daily Holiday Weekend date sequences","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_make_holiday_sequence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make daily Holiday and Weekend date sequences — tk_make_holiday_sequence","text":"","code":"tk_make_holiday_sequence(   start_date,   end_date,   calendar = c(\"NYSE\", \"LONDON\", \"NERC\", \"TSX\", \"ZURICH\"),   skip_values = NULL,   insert_values = NULL )  tk_make_weekend_sequence(start_date, end_date)  tk_make_weekday_sequence(   start_date,   end_date,   remove_weekends = TRUE,   remove_holidays = FALSE,   calendar = c(\"NYSE\", \"LONDON\", \"NERC\", \"TSX\", \"ZURICH\"),   skip_values = NULL,   insert_values = NULL )"},{"path":"https://business-science.github.io/timetk/reference/tk_make_holiday_sequence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make daily Holiday and Weekend date sequences — tk_make_holiday_sequence","text":"start_date Used define starting date date sequence generation. Provide \"YYYY-MM-DD\" format. end_date Used define ending date date sequence generation. Provide \"YYYY-MM-DD\" format. calendar calendar used Date Sequence calculations Holidays timeDate package. Acceptable values : \"NYSE\", \"LONDON\", \"NERC\", \"TSX\", \"ZURICH\". skip_values daily date sequence skip insert_values daily date sequence insert remove_weekends logical value indicating whether remove weekends (Saturday Sunday) date sequence remove_holidays logical value indicating whether remove common holidays date sequence","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_make_holiday_sequence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make daily Holiday and Weekend date sequences — tk_make_holiday_sequence","text":"vector containing future dates","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_make_holiday_sequence.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make daily Holiday and Weekend date sequences — tk_make_holiday_sequence","text":"Start End Date Specification Accept shorthand notation (.e. tk_make_timeseries() specifications apply) available Daily Periods. Holiday Sequences tk_make_holiday_sequence() wrapper various holiday calendars timeDate package, making easy generate holiday sequences common business calendars: New York Stock Exchange: calendar = \"NYSE\" Londo Stock Exchange: \"LONDON\" North American Reliability Council: \"NERC\" Toronto Stock Exchange: \"TSX\" Zurich Stock Exchange: \"ZURICH\" Weekend Weekday Sequences simply populate","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_make_holiday_sequence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make daily Holiday and Weekend date sequences — tk_make_holiday_sequence","text":"","code":"library(dplyr)  # Set max.print to 50 options_old <- options()$max.print options(max.print = 50)   # ---- HOLIDAYS & WEEKENDS ----  # Business Holiday Sequence tk_make_holiday_sequence(\"2017-01-01\", \"2017-12-31\", calendar = \"NYSE\") #> [1] \"2017-01-02\" \"2017-01-16\" \"2017-02-20\" \"2017-04-14\" \"2017-05-29\" #> [6] \"2017-07-04\" \"2017-09-04\" \"2017-11-23\" \"2017-12-25\"  tk_make_holiday_sequence(\"2017\", calendar = \"NYSE\") # Same thing as above (just shorter) #> [1] \"2017-01-02\" \"2017-01-16\" \"2017-02-20\" \"2017-04-14\" \"2017-05-29\" #> [6] \"2017-07-04\" \"2017-09-04\" \"2017-11-23\" \"2017-12-25\"  # Weekday Sequence tk_make_weekday_sequence(\"2017\", \"2018\", remove_holidays = TRUE) #>  [1] \"2017-01-03\" \"2017-01-04\" \"2017-01-05\" \"2017-01-06\" \"2017-01-09\" #>  [6] \"2017-01-10\" \"2017-01-11\" \"2017-01-12\" \"2017-01-13\" \"2017-01-17\" #> [11] \"2017-01-18\" \"2017-01-19\" \"2017-01-20\" \"2017-01-23\" \"2017-01-24\" #> [16] \"2017-01-25\" \"2017-01-26\" \"2017-01-27\" \"2017-01-30\" \"2017-01-31\" #> [21] \"2017-02-01\" \"2017-02-02\" \"2017-02-03\" \"2017-02-06\" \"2017-02-07\" #> [26] \"2017-02-08\" \"2017-02-09\" \"2017-02-10\" \"2017-02-13\" \"2017-02-14\" #> [31] \"2017-02-15\" \"2017-02-16\" \"2017-02-17\" \"2017-02-21\" \"2017-02-22\" #> [36] \"2017-02-23\" \"2017-02-24\" \"2017-02-27\" \"2017-02-28\" \"2017-03-01\" #> [41] \"2017-03-02\" \"2017-03-03\" \"2017-03-06\" \"2017-03-07\" \"2017-03-08\" #> [46] \"2017-03-09\" \"2017-03-10\" \"2017-03-13\" \"2017-03-14\" \"2017-03-15\" #>  [ reached 'max' / getOption(\"max.print\") -- omitted 453 entries ]  # Weekday Sequence + Removing Business Holidays tk_make_weekday_sequence(\"2017\", \"2018\", remove_holidays = TRUE) #>  [1] \"2017-01-03\" \"2017-01-04\" \"2017-01-05\" \"2017-01-06\" \"2017-01-09\" #>  [6] \"2017-01-10\" \"2017-01-11\" \"2017-01-12\" \"2017-01-13\" \"2017-01-17\" #> [11] \"2017-01-18\" \"2017-01-19\" \"2017-01-20\" \"2017-01-23\" \"2017-01-24\" #> [16] \"2017-01-25\" \"2017-01-26\" \"2017-01-27\" \"2017-01-30\" \"2017-01-31\" #> [21] \"2017-02-01\" \"2017-02-02\" \"2017-02-03\" \"2017-02-06\" \"2017-02-07\" #> [26] \"2017-02-08\" \"2017-02-09\" \"2017-02-10\" \"2017-02-13\" \"2017-02-14\" #> [31] \"2017-02-15\" \"2017-02-16\" \"2017-02-17\" \"2017-02-21\" \"2017-02-22\" #> [36] \"2017-02-23\" \"2017-02-24\" \"2017-02-27\" \"2017-02-28\" \"2017-03-01\" #> [41] \"2017-03-02\" \"2017-03-03\" \"2017-03-06\" \"2017-03-07\" \"2017-03-08\" #> [46] \"2017-03-09\" \"2017-03-10\" \"2017-03-13\" \"2017-03-14\" \"2017-03-15\" #>  [ reached 'max' / getOption(\"max.print\") -- omitted 453 entries ]   # ---- COMBINE HOLIDAYS WITH MAKE FUTURE TIMESERIES FROM EXISTING ---- # - A common machine learning application is creating a future time series data set #   from an existing  # Create index of days that FB stock will be traded in 2017 based on 2016 + holidays FB_tbl <- FANG %>% dplyr::filter(symbol == \"FB\")  holidays <- tk_make_holiday_sequence(     start_date = \"2016\",     end_date   = \"2017\",     calendar   = \"NYSE\")  weekends <- tk_make_weekend_sequence(     start_date = \"2016\",     end_date   = \"2017\")  # Remove holidays and weekends with skip_values # We could also remove weekends with inspect_weekdays = TRUE FB_tbl %>%     tk_index() %>%     tk_make_future_timeseries(length_out       = 366,                               skip_values      = c(holidays, weekends)) #> The following `skip_values` were not in the future date sequence: 2016-01-01, 2016-01-18, 2016-02-15, 2016-03-25, 2016-05-30, 2016-07-04, 2016-09-05, 2016-11-24, 2016-12-26, 2016-01-02, 2016-01-03, 2016-01-09, 2016-01-10, 2016-01-16, 2016-01-17, 2016-01-23, 2016-01-24, 2016-01-30, 2016-01-31, 2016-02-06, 2016-02-07, 2016-02-13, 2016-02-14, 2016-02-20, 2016-02-21, 2016-02-27, 2016-02-28, 2016-03-05, 2016-03-06, 2016-03-12, 2016-03-13, 2016-03-19, 2016-03-20, 2016-03-26, 2016-03-27, 2016-04-02, 2016-04-03, 2016-04-09, 2016-04-10, 2016-04-16, 2016-04-17, 2016-04-23, 2016-04-24, 2016-04-30, 2016-05-01, 2016-05-07, 2016-05-08, 2016-05-14, 2016-05-15, 2016-05-21, 2016-05-22, 2016-05-28, 2016-05-29, 2016-06-04, 2016-06-05, 2016-06-11, 2016-06-12, 2016-06-18, 2016-06-19, 2016-06-25, 2016-06-26, 2016-07-02, 2016-07-03, 2016-07-09, 2016-07-10, 2016-07-16, 2016-07-17, 2016-07-23, 2016-07-24, 2016-07-30, 2016-07-31, 2016-08-06, 2016-08-07, 2016-08-13, 2016-08-14, 2016-08-20, 2016-08-21, 2016-08-27, 2016-08-28, 2016-09-03, 2016-09-04, 2016-09-10, 2016-09-11, 2016-09-17, 2016-09-18, 2016-09-24, 2016-09-25, 2016-10-01, 2016-10-02, 2016-10-08, 2016-10-09, 2016-10-15, 2016-10-16, 2016-10-22, 2016-10-23, 2016-10-29, 2016-10-30, 2016-11-05, 2016-11-06, 2016-11-12, 2016-11-13, 2016-11-19, 2016-11-20, 2016-11-26, 2016-11-27, 2016-12-03, 2016-12-04, 2016-12-10, 2016-12-11, 2016-12-17, 2016-12-18, 2016-12-24, 2016-12-25 #>  [1] \"2017-01-03\" \"2017-01-04\" \"2017-01-05\" \"2017-01-06\" \"2017-01-09\" #>  [6] \"2017-01-10\" \"2017-01-11\" \"2017-01-12\" \"2017-01-13\" \"2017-01-17\" #> [11] \"2017-01-18\" \"2017-01-19\" \"2017-01-20\" \"2017-01-23\" \"2017-01-24\" #> [16] \"2017-01-25\" \"2017-01-26\" \"2017-01-27\" \"2017-01-30\" \"2017-01-31\" #> [21] \"2017-02-01\" \"2017-02-02\" \"2017-02-03\" \"2017-02-06\" \"2017-02-07\" #> [26] \"2017-02-08\" \"2017-02-09\" \"2017-02-10\" \"2017-02-13\" \"2017-02-14\" #> [31] \"2017-02-15\" \"2017-02-16\" \"2017-02-17\" \"2017-02-21\" \"2017-02-22\" #> [36] \"2017-02-23\" \"2017-02-24\" \"2017-02-27\" \"2017-02-28\" \"2017-03-01\" #> [41] \"2017-03-02\" \"2017-03-03\" \"2017-03-06\" \"2017-03-07\" \"2017-03-08\" #> [46] \"2017-03-09\" \"2017-03-10\" \"2017-03-13\" \"2017-03-14\" \"2017-03-15\" #>  [ reached 'max' / getOption(\"max.print\") -- omitted 316 entries ]   options(max.print = options_old)"},{"path":"https://business-science.github.io/timetk/reference/tk_make_timeseries.html","id":null,"dir":"Reference","previous_headings":"","what":"Intelligent date and date-time sequence creation — tk_make_timeseries","title":"Intelligent date and date-time sequence creation — tk_make_timeseries","text":"Improves seq.Date() seq.POSIXt() functions simplifying 1 function tk_make_timeseries(). Intelligently handles character dates logical assumptions based user inputs.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_make_timeseries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Intelligent date and date-time sequence creation — tk_make_timeseries","text":"","code":"tk_make_timeseries(   start_date,   end_date,   by,   length_out = NULL,   include_endpoints = TRUE,   skip_values = NULL,   insert_values = NULL )"},{"path":"https://business-science.github.io/timetk/reference/tk_make_timeseries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Intelligent date and date-time sequence creation — tk_make_timeseries","text":"start_date Used define starting date date sequence generation. Provide \"YYYY-MM-DD\" format. end_date Used define ending date date sequence generation. Provide \"YYYY-MM-DD\" format. character string, containing one \"sec\", \"min\", \"hour\", \"day\", \"week\", \"month\", \"quarter\" \"year\". can create regularly spaced sequences using phrases like = \"10 min\". See Details. length_out Optional length sequence. Can used instead one : start_date, end_date, . Can specified number time-based phrase. include_endpoints Logical. Whether keep last value length_out time-based phrase. Default TRUE (keep last value). skip_values sequence skip insert_values sequence insert","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_make_timeseries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Intelligent date and date-time sequence creation — tk_make_timeseries","text":"vector containing date date-times","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_make_timeseries.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Intelligent date and date-time sequence creation — tk_make_timeseries","text":"tk_make_timeseries() function handles date date-time sequences automatically. Parses date date times character Intelligently guesses sequence desired based arguments provided Handles spacing intelligently length_out missing, guesses either second day sequences Can skip insert values needed. Start End Date Specification Start end dates can specified reduced time-based phrases: start_date = \"2014\": converted \"2014-01-01\" (start period) end_date = \"2014\": converted \"2014-12-31\" (end period) start_date = \"2014-03\": converted \"2014-03-01\" (start period) end_date = \"2014-03\": converted \"2014-03-31\" (end period) similar process can used date-times. : Daily Sequences Make daily sequence tk_make_timeseries(). Examples: Every Day: = \"day\" Every 2-Weeks: = \"2 weeks\" Every 6-months: = \"6 months\" missing, guess = \"day\" : Sub-Daily Sequences Make sub-daily sequence tk_make_timeseries(). Examples: Every minute: = \"min\" Every 30-seconds: = \"30 sec\" Every 2-hours: = \"2 hours missing, guess = \"sec\" start end date date-time specification. Length length_out can specified number observation complex time-based expressions. following examples possible. length_out = 12 Creates 12 evenly spaced observations. length_out = \"12 months\" Adjusts end date falls 12th month. Include Endpoint Sometimes last date desired. example, user specifies length_out = 12 months, user may want last value 12th month 13th. Just toggle, include_endpoint = FALSE obtain behavior. Skip / Insert Timestamps Skips inserts performed sequence generated. means use length_out parameter, length may differ length_out.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_make_timeseries.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Intelligent date and date-time sequence creation — tk_make_timeseries","text":"","code":"library(dplyr)  # Set max.print to 50 options_old <- options()$max.print options(max.print = 50)  # ---- DATE ----  # Start + End, Guesses by = \"day\" tk_make_timeseries(\"2017-01-01\", \"2017-12-31\") #> Using by: day #>  [1] \"2017-01-01\" \"2017-01-02\" \"2017-01-03\" \"2017-01-04\" \"2017-01-05\" #>  [6] \"2017-01-06\" \"2017-01-07\" \"2017-01-08\" \"2017-01-09\" \"2017-01-10\" #> [11] \"2017-01-11\" \"2017-01-12\" \"2017-01-13\" \"2017-01-14\" \"2017-01-15\" #> [16] \"2017-01-16\" \"2017-01-17\" \"2017-01-18\" \"2017-01-19\" \"2017-01-20\" #> [21] \"2017-01-21\" \"2017-01-22\" \"2017-01-23\" \"2017-01-24\" \"2017-01-25\" #> [26] \"2017-01-26\" \"2017-01-27\" \"2017-01-28\" \"2017-01-29\" \"2017-01-30\" #> [31] \"2017-01-31\" \"2017-02-01\" \"2017-02-02\" \"2017-02-03\" \"2017-02-04\" #> [36] \"2017-02-05\" \"2017-02-06\" \"2017-02-07\" \"2017-02-08\" \"2017-02-09\" #> [41] \"2017-02-10\" \"2017-02-11\" \"2017-02-12\" \"2017-02-13\" \"2017-02-14\" #> [46] \"2017-02-15\" \"2017-02-16\" \"2017-02-17\" \"2017-02-18\" \"2017-02-19\" #>  [ reached 'max' / getOption(\"max.print\") -- omitted 315 entries ]  # Just Start tk_make_timeseries(\"2017\") # Same result #> Using by: day #>  [1] \"2017-01-01\" \"2017-01-02\" \"2017-01-03\" \"2017-01-04\" \"2017-01-05\" #>  [6] \"2017-01-06\" \"2017-01-07\" \"2017-01-08\" \"2017-01-09\" \"2017-01-10\" #> [11] \"2017-01-11\" \"2017-01-12\" \"2017-01-13\" \"2017-01-14\" \"2017-01-15\" #> [16] \"2017-01-16\" \"2017-01-17\" \"2017-01-18\" \"2017-01-19\" \"2017-01-20\" #> [21] \"2017-01-21\" \"2017-01-22\" \"2017-01-23\" \"2017-01-24\" \"2017-01-25\" #> [26] \"2017-01-26\" \"2017-01-27\" \"2017-01-28\" \"2017-01-29\" \"2017-01-30\" #> [31] \"2017-01-31\" \"2017-02-01\" \"2017-02-02\" \"2017-02-03\" \"2017-02-04\" #> [36] \"2017-02-05\" \"2017-02-06\" \"2017-02-07\" \"2017-02-08\" \"2017-02-09\" #> [41] \"2017-02-10\" \"2017-02-11\" \"2017-02-12\" \"2017-02-13\" \"2017-02-14\" #> [46] \"2017-02-15\" \"2017-02-16\" \"2017-02-17\" \"2017-02-18\" \"2017-02-19\" #>  [ reached 'max' / getOption(\"max.print\") -- omitted 315 entries ]  # Only dates in February, 2017 tk_make_timeseries(\"2017-02\") #> Using by: day #>  [1] \"2017-02-01\" \"2017-02-02\" \"2017-02-03\" \"2017-02-04\" \"2017-02-05\" #>  [6] \"2017-02-06\" \"2017-02-07\" \"2017-02-08\" \"2017-02-09\" \"2017-02-10\" #> [11] \"2017-02-11\" \"2017-02-12\" \"2017-02-13\" \"2017-02-14\" \"2017-02-15\" #> [16] \"2017-02-16\" \"2017-02-17\" \"2017-02-18\" \"2017-02-19\" \"2017-02-20\" #> [21] \"2017-02-21\" \"2017-02-22\" \"2017-02-23\" \"2017-02-24\" \"2017-02-25\" #> [26] \"2017-02-26\" \"2017-02-27\" \"2017-02-28\"  # Start + Length Out, Guesses by = \"day\" tk_make_timeseries(\"2012\", length_out = 6) # Guesses by = \"day\" #> Using by: day #> [1] \"2012-01-01\" \"2012-01-02\" \"2012-01-03\" \"2012-01-04\" \"2012-01-05\" #> [6] \"2012-01-06\"  # Start + By + Length Out, Spacing 6 observations by monthly interval tk_make_timeseries(\"2012\", by = \"1 month\", length_out = 6) #> [1] \"2012-01-01\" \"2012-02-01\" \"2012-03-01\" \"2012-04-01\" \"2012-05-01\" #> [6] \"2012-06-01\"  # Start + By + Length Out, Phrase \"1 year 6 months\" tk_make_timeseries(\"2012\", by = \"1 month\",                    length_out = \"1 year 6 months\", include_endpoints = FALSE) #>  [1] \"2012-01-01\" \"2012-02-01\" \"2012-03-01\" \"2012-04-01\" \"2012-05-01\" #>  [6] \"2012-06-01\" \"2012-07-01\" \"2012-08-01\" \"2012-09-01\" \"2012-10-01\" #> [11] \"2012-11-01\" \"2012-12-01\" \"2013-01-01\" \"2013-02-01\" \"2013-03-01\" #> [16] \"2013-04-01\" \"2013-05-01\" \"2013-06-01\"  # Going in Reverse, End + Length Out tk_make_timeseries(end_date = \"2012-01-01\", by = \"1 month\",                    length_out = \"1 year 6 months\", include_endpoints = FALSE) #>  [1] \"2010-08-01\" \"2010-09-01\" \"2010-10-01\" \"2010-11-01\" \"2010-12-01\" #>  [6] \"2011-01-01\" \"2011-02-01\" \"2011-03-01\" \"2011-04-01\" \"2011-05-01\" #> [11] \"2011-06-01\" \"2011-07-01\" \"2011-08-01\" \"2011-09-01\" \"2011-10-01\" #> [16] \"2011-11-01\" \"2011-12-01\" \"2012-01-01\"  # ---- DATE-TIME ----  # Start + End, Guesses by second tk_make_timeseries(\"2016-01-01 01:01:02\", \"2016-01-01 01:01:04\") #> Using by: sec #> [1] \"2016-01-01 01:01:02 UTC\" \"2016-01-01 01:01:03 UTC\" #> [3] \"2016-01-01 01:01:04 UTC\"  # Date-Time Sequence - By 10 Minutes # - Converts to date-time automatically & applies 10-min interval tk_make_timeseries(\"2017-01-01\", \"2017-01-02\", by = \"10 min\") #>  [1] \"2017-01-01 00:00:00 UTC\" \"2017-01-01 00:10:00 UTC\" #>  [3] \"2017-01-01 00:20:00 UTC\" \"2017-01-01 00:30:00 UTC\" #>  [5] \"2017-01-01 00:40:00 UTC\" \"2017-01-01 00:50:00 UTC\" #>  [7] \"2017-01-01 01:00:00 UTC\" \"2017-01-01 01:10:00 UTC\" #>  [9] \"2017-01-01 01:20:00 UTC\" \"2017-01-01 01:30:00 UTC\" #> [11] \"2017-01-01 01:40:00 UTC\" \"2017-01-01 01:50:00 UTC\" #> [13] \"2017-01-01 02:00:00 UTC\" \"2017-01-01 02:10:00 UTC\" #> [15] \"2017-01-01 02:20:00 UTC\" \"2017-01-01 02:30:00 UTC\" #> [17] \"2017-01-01 02:40:00 UTC\" \"2017-01-01 02:50:00 UTC\" #> [19] \"2017-01-01 03:00:00 UTC\" \"2017-01-01 03:10:00 UTC\" #> [21] \"2017-01-01 03:20:00 UTC\" \"2017-01-01 03:30:00 UTC\" #> [23] \"2017-01-01 03:40:00 UTC\" \"2017-01-01 03:50:00 UTC\" #> [25] \"2017-01-01 04:00:00 UTC\" \"2017-01-01 04:10:00 UTC\" #> [27] \"2017-01-01 04:20:00 UTC\" \"2017-01-01 04:30:00 UTC\" #> [29] \"2017-01-01 04:40:00 UTC\" \"2017-01-01 04:50:00 UTC\" #> [31] \"2017-01-01 05:00:00 UTC\" \"2017-01-01 05:10:00 UTC\" #> [33] \"2017-01-01 05:20:00 UTC\" \"2017-01-01 05:30:00 UTC\" #> [35] \"2017-01-01 05:40:00 UTC\" \"2017-01-01 05:50:00 UTC\" #> [37] \"2017-01-01 06:00:00 UTC\" \"2017-01-01 06:10:00 UTC\" #> [39] \"2017-01-01 06:20:00 UTC\" \"2017-01-01 06:30:00 UTC\" #> [41] \"2017-01-01 06:40:00 UTC\" \"2017-01-01 06:50:00 UTC\" #> [43] \"2017-01-01 07:00:00 UTC\" \"2017-01-01 07:10:00 UTC\" #> [45] \"2017-01-01 07:20:00 UTC\" \"2017-01-01 07:30:00 UTC\" #> [47] \"2017-01-01 07:40:00 UTC\" \"2017-01-01 07:50:00 UTC\" #> [49] \"2017-01-01 08:00:00 UTC\" \"2017-01-01 08:10:00 UTC\" #>  [ reached 'max' / getOption(\"max.print\") -- omitted 95 entries ]   # --- REMOVE / INCLUDE ENDPOINTS ----  # Last value in this case is desired tk_make_timeseries(\"2017-01-01\", by = \"30 min\", length_out = \"6 hours\") #>  [1] \"2017-01-01 00:00:00 UTC\" \"2017-01-01 00:30:00 UTC\" #>  [3] \"2017-01-01 01:00:00 UTC\" \"2017-01-01 01:30:00 UTC\" #>  [5] \"2017-01-01 02:00:00 UTC\" \"2017-01-01 02:30:00 UTC\" #>  [7] \"2017-01-01 03:00:00 UTC\" \"2017-01-01 03:30:00 UTC\" #>  [9] \"2017-01-01 04:00:00 UTC\" \"2017-01-01 04:30:00 UTC\" #> [11] \"2017-01-01 05:00:00 UTC\" \"2017-01-01 05:30:00 UTC\" #> [13] \"2017-01-01 06:00:00 UTC\"  # Last value in monthly case is not wanted tk_make_timeseries(\"2012-01-01\", by = \"1 month\",                    length_out = \"12 months\",                    include_endpoints = FALSE) # Removes unnecessary last value #>  [1] \"2012-01-01\" \"2012-02-01\" \"2012-03-01\" \"2012-04-01\" \"2012-05-01\" #>  [6] \"2012-06-01\" \"2012-07-01\" \"2012-08-01\" \"2012-09-01\" \"2012-10-01\" #> [11] \"2012-11-01\" \"2012-12-01\"   # ---- SKIP & INSERT VALUES ----  tk_make_timeseries(     \"2011-01-01\", length_out = 5,     skip_values   = \"2011-01-05\",     insert_values = \"2011-01-06\" ) #> Using by: day #> [1] \"2011-01-01\" \"2011-01-02\" \"2011-01-03\" \"2011-01-04\" \"2011-01-06\"  options(max.print = options_old)"},{"path":"https://business-science.github.io/timetk/reference/tk_seasonal_diagnostics.html","id":null,"dir":"Reference","previous_headings":"","what":"Group-wise Seasonality Data Preparation — tk_seasonal_diagnostics","title":"Group-wise Seasonality Data Preparation — tk_seasonal_diagnostics","text":"tk_seasonal_diagnostics() preprocessor plot_seasonal_diagnostics(). helps automating feature collection time series seasonality analysis.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_seasonal_diagnostics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group-wise Seasonality Data Preparation — tk_seasonal_diagnostics","text":"","code":"tk_seasonal_diagnostics(.data, .date_var, .value, .feature_set = \"auto\")"},{"path":"https://business-science.github.io/timetk/reference/tk_seasonal_diagnostics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group-wise Seasonality Data Preparation — tk_seasonal_diagnostics","text":".data tibble data.frame time-based column .date_var column containing either date date-time values .value column containing numeric values .feature_set One multiple selections analyze seasonality. Choices include: \"auto\" - Automatically selects features based time stamps length series. \"second\" - Good analyzing seasonality second minute. \"minute\" - Good analyzing seasonality minute hour \"hour\" - Good analyzing seasonality hour day \"wday.lbl\" - Labeled weekdays. Good analyzing seasonality day week. \"week\" - Good analyzing seasonality week year. \"month.lbl\" - Labeled months. Good analyzing seasonality month year. \"quarter\" - Good analyzing seasonality quarter year \"year\" - Good analyzing seasonality multiple years.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_seasonal_diagnostics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Group-wise Seasonality Data Preparation — tk_seasonal_diagnostics","text":"tibble data.frame seasonal features","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_seasonal_diagnostics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Group-wise Seasonality Data Preparation — tk_seasonal_diagnostics","text":"Automatic Feature Selection Internal calculations performed detect sub-range features include useing following logic: minimum feature selected based median difference consecutive timestamps maximum feature selected based 2 full periods. Example: Hourly timestamp data lasts 2 weeks following features: \"hour\", \"wday.lbl\", \"week\". Scalable Grouped Data Frames function respects grouped data.frame tibbles made dplyr::group_by(). grouped data, automatic feature selection returned collection features within sub-groups. means extra features returned even though may meaningless groups. Transformations .value parameter respects transformations (e.g. .value = log(sales)).","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_seasonal_diagnostics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group-wise Seasonality Data Preparation — tk_seasonal_diagnostics","text":"","code":"# \\donttest{ library(dplyr)  # ---- GROUPED EXAMPLES ----  # Hourly Data m4_hourly %>%     group_by(id) %>%     tk_seasonal_diagnostics(date, value) #> # A tibble: 3,060 × 6 #> # Groups:   id [4] #>    id    date                .value hour  wday.lbl  week  #>    <fct> <dttm>               <dbl> <fct> <fct>     <fct> #>  1 H10   2015-07-01 12:00:00    513 12    Wednesday 26    #>  2 H10   2015-07-01 13:00:00    512 13    Wednesday 26    #>  3 H10   2015-07-01 14:00:00    506 14    Wednesday 26    #>  4 H10   2015-07-01 15:00:00    500 15    Wednesday 26    #>  5 H10   2015-07-01 16:00:00    490 16    Wednesday 26    #>  6 H10   2015-07-01 17:00:00    484 17    Wednesday 26    #>  7 H10   2015-07-01 18:00:00    467 18    Wednesday 26    #>  8 H10   2015-07-01 19:00:00    446 19    Wednesday 26    #>  9 H10   2015-07-01 20:00:00    434 20    Wednesday 26    #> 10 H10   2015-07-01 21:00:00    422 21    Wednesday 26    #> # ℹ 3,050 more rows  # Monthly Data m4_monthly %>%     group_by(id) %>%     tk_seasonal_diagnostics(date, value) #> # A tibble: 1,574 × 6 #> # Groups:   id [4] #>    id    date       .value month.lbl quarter year  #>    <fct> <date>      <dbl> <fct>     <fct>   <fct> #>  1 M1    1976-06-01   8000 June      2       1976  #>  2 M1    1976-07-01   8350 July      3       1976  #>  3 M1    1976-08-01   8570 August    3       1976  #>  4 M1    1976-09-01   7700 September 3       1976  #>  5 M1    1976-10-01   7080 October   4       1976  #>  6 M1    1976-11-01   6520 November  4       1976  #>  7 M1    1976-12-01   6070 December  4       1976  #>  8 M1    1977-01-01   6650 January   1       1977  #>  9 M1    1977-02-01   6830 February  1       1977  #> 10 M1    1977-03-01   5710 March     1       1977  #> # ℹ 1,564 more rows  # ---- TRANSFORMATION ----  m4_weekly %>%     group_by(id) %>%     tk_seasonal_diagnostics(date, log(value)) #> # A tibble: 2,295 × 7 #> # Groups:   id [4] #>    id    date       .value week  month.lbl quarter year  #>    <fct> <date>      <dbl> <fct> <fct>     <fct>   <fct> #>  1 W10   1999-01-01   5.51 1     January   1       1999  #>  2 W10   1999-01-08   5.40 2     January   1       1999  #>  3 W10   1999-01-15   6.11 3     January   1       1999  #>  4 W10   1999-01-22   6.11 4     January   1       1999  #>  5 W10   1999-01-29   6.11 5     January   1       1999  #>  6 W10   1999-02-05   6.11 6     February  1       1999  #>  7 W10   1999-02-12   6.11 7     February  1       1999  #>  8 W10   1999-02-19   6.11 8     February  1       1999  #>  9 W10   1999-02-26   6.11 9     February  1       1999  #> 10 W10   1999-03-05   6.11 10    March     1       1999  #> # ℹ 2,285 more rows  # ---- CUSTOM FEATURE SELECTION ----  m4_hourly %>%     group_by(id) %>%     tk_seasonal_diagnostics(date, value, .feature_set = c(\"hour\", \"week\")) #> # A tibble: 3,060 × 5 #> # Groups:   id [4] #>    id    date                .value hour  week  #>    <fct> <dttm>               <dbl> <fct> <fct> #>  1 H10   2015-07-01 12:00:00    513 12    26    #>  2 H10   2015-07-01 13:00:00    512 13    26    #>  3 H10   2015-07-01 14:00:00    506 14    26    #>  4 H10   2015-07-01 15:00:00    500 15    26    #>  5 H10   2015-07-01 16:00:00    490 16    26    #>  6 H10   2015-07-01 17:00:00    484 17    26    #>  7 H10   2015-07-01 18:00:00    467 18    26    #>  8 H10   2015-07-01 19:00:00    446 19    26    #>  9 H10   2015-07-01 20:00:00    434 20    26    #> 10 H10   2015-07-01 21:00:00    422 21    26    #> # ℹ 3,050 more rows  # }"},{"path":"https://business-science.github.io/timetk/reference/tk_stl_diagnostics.html","id":null,"dir":"Reference","previous_headings":"","what":"Group-wise STL Decomposition (Season, Trend, Remainder) — tk_stl_diagnostics","title":"Group-wise STL Decomposition (Season, Trend, Remainder) — tk_stl_diagnostics","text":"tk_stl_diagnostics() preprocessor plot_stl_diagnostics(). helps automating frequency trend selection.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_stl_diagnostics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group-wise STL Decomposition (Season, Trend, Remainder) — tk_stl_diagnostics","text":"","code":"tk_stl_diagnostics(   .data,   .date_var,   .value,   .frequency = \"auto\",   .trend = \"auto\",   .message = TRUE )"},{"path":"https://business-science.github.io/timetk/reference/tk_stl_diagnostics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group-wise STL Decomposition (Season, Trend, Remainder) — tk_stl_diagnostics","text":".data tibble data.frame time-based column .date_var column containing either date date-time values .value column containing numeric values .frequency Controls seasonal adjustment (removal seasonality). Input can either \"auto\", time-based definition (e.g. \"2 weeks\"), numeric number observations per frequency (e.g. 10). Refer tk_get_frequency(). .trend Controls trend component. STL, trend controls sensitivity lowess smoother, used remove remainder. .message boolean. TRUE, output information related automatic frequency trend selection (applicable).","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_stl_diagnostics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Group-wise STL Decomposition (Season, Trend, Remainder) — tk_stl_diagnostics","text":"tibble data.frame Observed, Season, Trend, Remainder, Seasonally-Adjusted features","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_stl_diagnostics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Group-wise STL Decomposition (Season, Trend, Remainder) — tk_stl_diagnostics","text":"tk_stl_diagnostics() function generates Seasonal-Trend-Loess decomposition. function \"tidy\" sense works data frames designed work dplyr groups. STL method: STL method implements time series decomposition using underlying stats::stl(). decomposition separates \"season\" \"trend\" components \"observed\" values leaving \"remainder\". Frequency & Trend Selection user can control two parameters: .frequency .trend. .frequency parameter adjusts \"season\" component removed \"observed\" values. .trend parameter adjusts trend window (t.window parameter stl()) used. user may supply .frequency .trend time-based durations (e.g. \"6 weeks\") numeric values (e.g. 180) \"auto\", automatically selects frequency /trend based scale time series.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_stl_diagnostics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group-wise STL Decomposition (Season, Trend, Remainder) — tk_stl_diagnostics","text":"","code":"library(dplyr)   # ---- GROUPS & TRANSFORMATION ---- m4_daily %>%     group_by(id) %>%     tk_stl_diagnostics(date, box_cox_vec(value)) #> frequency = 7 observations per 1 week #> trend = 92 observations per 3 months #> box_cox_vec(): Using value for lambda: 1.25119350454964 #> frequency = 7 observations per 1 week #> trend = 92 observations per 3 months #> box_cox_vec(): Using value for lambda: 0.0882021886505848 #> frequency = 7 observations per 1 week #> trend = 92 observations per 3 months #> box_cox_vec(): Using value for lambda: 1.99992424816297 #> frequency = 7 observations per 1 week #> trend = 92 observations per 3 months #> box_cox_vec(): Using value for lambda: 0.401716085353735 #> # A tibble: 9,743 × 7 #> # Groups:   id [4] #>    id    date       observed season  trend remainder seasadj #>    <fct> <date>        <dbl>  <dbl>  <dbl>     <dbl>   <dbl> #>  1 D10   2014-07-03   11303.  -8.25 10796.     516.   11311. #>  2 D10   2014-07-04   11284.  -7.93 10790.     502.   11292. #>  3 D10   2014-07-05   11116. -14.8  10784.     347.   11131. #>  4 D10   2014-07-06   11117.  -6.28 10778.     346.   11124. #>  5 D10   2014-07-07   10829.  10.7  10772.      47.0  10819. #>  6 D10   2014-07-08   10905.  16.6  10766.     123.   10889. #>  7 D10   2014-07-09   10915.  10.0  10760.     145.   10905. #>  8 D10   2014-07-10   10836.  -8.25 10754.      90.5  10844. #>  9 D10   2014-07-11   10854.  -7.93 10748.     114.   10862. #> 10 D10   2014-07-12   10796. -14.8  10742.      69.1  10811. #> # ℹ 9,733 more rows  # ---- CUSTOM TREND ---- m4_weekly %>%     group_by(id) %>%     tk_stl_diagnostics(date, box_cox_vec(value), .trend = \"2 quarters\") #> frequency = 13 observations per 1 quarter #> trend = 25 observations per 2 quarters #> box_cox_vec(): Using value for lambda: -0.374719526760349 #> frequency = 13 observations per 1 quarter #> trend = 25 observations per 2 quarters #> box_cox_vec(): Using value for lambda: 0.0597533426736463 #> frequency = 13 observations per 1 quarter #> trend = 26 observations per 2 quarters #> box_cox_vec(): Using value for lambda: -0.937375922566063 #> frequency = 13 observations per 1 quarter #> trend = 26 observations per 2 quarters #> box_cox_vec(): Using value for lambda: -0.195493340612351 #> # A tibble: 2,295 × 7 #> # Groups:   id [4] #>    id    date       observed     season trend  remainder seasadj #>    <fct> <date>        <dbl>      <dbl> <dbl>      <dbl>   <dbl> #>  1 W10   1999-01-01     2.33 -0.000172   2.40 -0.0678       2.33 #>  2 W10   1999-01-08     2.32 -0.000195   2.40 -0.0814       2.32 #>  3 W10   1999-01-15     2.40  0.0000406  2.40  0.000106     2.40 #>  4 W10   1999-01-22     2.40  0.000252   2.40 -0.000127     2.40 #>  5 W10   1999-01-29     2.40  0.000371   2.40 -0.000261     2.40 #>  6 W10   1999-02-05     2.40  0.000397   2.40 -0.000314     2.40 #>  7 W10   1999-02-12     2.40  0.000162   2.40 -0.0000956    2.40 #>  8 W10   1999-02-19     2.40 -0.0000578  2.40  0.000114     2.40 #>  9 W10   1999-02-26     2.40 -0.000171   2.40  0.000235     2.40 #> 10 W10   1999-03-05     2.40 -0.000163   2.40  0.000216     2.40 #> # ℹ 2,285 more rows"},{"path":"https://business-science.github.io/timetk/reference/tk_summary_diagnostics.html","id":null,"dir":"Reference","previous_headings":"","what":"Group-wise Time Series Summary — tk_summary_diagnostics","title":"Group-wise Time Series Summary — tk_summary_diagnostics","text":"tk_summary_diagnostics() returns time series summary one timeseries groups tibble.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_summary_diagnostics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group-wise Time Series Summary — tk_summary_diagnostics","text":"","code":"tk_summary_diagnostics(.data, .date_var)"},{"path":"https://business-science.github.io/timetk/reference/tk_summary_diagnostics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group-wise Time Series Summary — tk_summary_diagnostics","text":".data tibble data.frame time-based column .date_var column containing either date date-time values. missing, attempts auto-detect date date-time column.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_summary_diagnostics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Group-wise Time Series Summary — tk_summary_diagnostics","text":"tibble data.frame timeseries summary features","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_summary_diagnostics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Group-wise Time Series Summary — tk_summary_diagnostics","text":"Applies tk_get_timeseries_summary() group-wise returning summary one time series groups. Respects dplyr groups Returns time series summary time-based feature.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_summary_diagnostics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group-wise Time Series Summary — tk_summary_diagnostics","text":"","code":"library(dplyr)  # ---- NON-GROUPED EXAMPLES ----  # Monthly Data m4_monthly %>%     filter(id == \"M750\") %>%     tk_summary_diagnostics() #> tk_augment_timeseries_signature(): Using the following .date_var variable: date #> # A tibble: 1 × 12 #>   n.obs start      end        units scale tzone diff.minimum diff.q1 diff.median #>   <int> <date>     <date>     <chr> <chr> <chr>        <dbl>   <dbl>       <dbl> #> 1   306 1990-01-01 2015-06-01 days  month UTC        2419200 2592000     2678400 #> # ℹ 3 more variables: diff.mean <dbl>, diff.q3 <dbl>, diff.maximum <dbl>  # ---- GROUPED EXAMPLES ----  # Monthly Data m4_monthly %>%     group_by(id) %>%     tk_summary_diagnostics() #> tk_augment_timeseries_signature(): Using the following .date_var variable: date #> # A tibble: 4 × 13 #> # Groups:   id [4] #>   id    n.obs start      end        units scale tzone diff.minimum diff.q1 #>   <fct> <int> <date>     <date>     <chr> <chr> <chr>        <dbl>   <dbl> #> 1 M1      469 1976-06-01 2015-06-01 days  month UTC        2419200 2592000 #> 2 M2      469 1976-06-01 2015-06-01 days  month UTC        2419200 2592000 #> 3 M750    306 1990-01-01 2015-06-01 days  month UTC        2419200 2592000 #> 4 M1000   330 1988-01-01 2015-06-01 days  month UTC        2419200 2592000 #> # ℹ 4 more variables: diff.median <dbl>, diff.mean <dbl>, diff.q3 <dbl>, #> #   diff.maximum <dbl>"},{"path":"https://business-science.github.io/timetk/reference/tk_tbl.html","id":null,"dir":"Reference","previous_headings":"","what":"Coerce time-series objects to tibble. — tk_tbl","title":"Coerce time-series objects to tibble. — tk_tbl","text":"Coerce time-series objects tibble.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_tbl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coerce time-series objects to tibble. — tk_tbl","text":"","code":"tk_tbl(   data,   preserve_index = TRUE,   rename_index = \"index\",   timetk_idx = FALSE,   silent = FALSE,   ... )"},{"path":"https://business-science.github.io/timetk/reference/tk_tbl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coerce time-series objects to tibble. — tk_tbl","text":"data time-series object. preserve_index Attempts preserve time series index. Default TRUE. rename_index Enables index column renamed. timetk_idx Used return date / datetime index regularized objects contain timetk \"index\" attribute. Refer tk_index() information returning index information regularized timeseries objects (.e. ts). silent Used toggle printing messages warnings. ... Additional parameters passed tibble::as_tibble() function.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_tbl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Coerce time-series objects to tibble. — tk_tbl","text":"Returns tibble object.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_tbl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Coerce time-series objects to tibble. — tk_tbl","text":"tk_tbl designed coerce time series objects (e.g. xts, zoo, ts, timeSeries, etc) tibble objects. main advantage function keeps date / date-time information underlying time-series object. preserve_index = TRUE specified, new column, index, created object coercion, function attempts preserve date date-time information. date / date-time column name can changed using rename_index argument. timetk_idx argument applicable coercing ts objects created using tk_ts() object time base (e.g. tbl, xts, zoo). Setting timetk_idx = TRUE enables returning timetk \"index\" attribute present, original (non-regularized) time-based index.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_tbl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coerce time-series objects to tibble. — tk_tbl","text":"","code":"library(dplyr)  data_tbl <- tibble(     date = seq.Date(from = as.Date(\"2010-01-01\"), by = 1, length.out = 5),     x    = seq(100, 120, by = 5) )   ### ts to tibble: Comparison between as.data.frame() and tk_tbl() data_ts <- tk_ts(data_tbl, start = c(2010,1), freq = 365) #> Warning: Non-numeric columns being dropped: date  # No index as.data.frame(data_ts) #>     x #> 1 100 #> 2 105 #> 3 110 #> 4 115 #> 5 120  # Defualt index returned is regularized numeric index tk_tbl(data_ts) #> # A tibble: 5 × 2 #>   index     x #>   <dbl> <dbl> #> 1 2010    100 #> 2 2010.   105 #> 3 2010.   110 #> 4 2010.   115 #> 5 2010.   120  # Original date index returned (Only possible if original data has time-based index) tk_tbl(data_ts, timetk_idx = TRUE) #> Warning: 'tzone' attributes are inconsistent #> # A tibble: 5 × 2 #>   index          x #>   <date>     <dbl> #> 1 2010-01-01   100 #> 2 2010-01-02   105 #> 3 2010-01-03   110 #> 4 2010-01-04   115 #> 5 2010-01-05   120   ### xts to tibble: Comparison between as.data.frame() and tk_tbl() data_xts <- tk_xts(data_tbl) #> Warning: Non-numeric columns being dropped: date #> Using column `date` for date_var.  # Dates are character class stored in row names as.data.frame(data_xts) #>              x #> 2010-01-01 100 #> 2010-01-02 105 #> 2010-01-03 110 #> 2010-01-04 115 #> 2010-01-05 120  # Dates are appropriate date class and within the data frame tk_tbl(data_xts) #> # A tibble: 5 × 2 #>   index          x #>   <date>     <dbl> #> 1 2010-01-01   100 #> 2 2010-01-02   105 #> 3 2010-01-03   110 #> 4 2010-01-04   115 #> 5 2010-01-05   120   ### zooreg to tibble: Comparison between as.data.frame() and tk_tbl() data_zooreg <- tk_zooreg(1:8, start = zoo::yearqtr(2000), frequency = 4)  # Dates are character class stored in row names as.data.frame(data_zooreg) #>         data_zooreg #> 2000 Q1           1 #> 2000 Q2           2 #> 2000 Q3           3 #> 2000 Q4           4 #> 2001 Q1           5 #> 2001 Q2           6 #> 2001 Q3           7 #> 2001 Q4           8  # Dates are appropriate zoo yearqtr class within the data frame tk_tbl(data_zooreg) #> # A tibble: 8 × 2 #>   index     value #>   <yearqtr> <int> #> 1 2000 Q1       1 #> 2 2000 Q2       2 #> 3 2000 Q3       3 #> 4 2000 Q4       4 #> 5 2001 Q1       5 #> 6 2001 Q2       6 #> 7 2001 Q3       7 #> 8 2001 Q4       8   ### zoo to tibble: Comparison between as.data.frame() and tk_tbl() data_zoo <- zoo::zoo(1:12, zoo::yearmon(2016 + seq(0, 11)/12))  # Dates are character class stored in row names as.data.frame(data_zoo) #>          data_zoo #> Jan 2016        1 #> Feb 2016        2 #> Mar 2016        3 #> Apr 2016        4 #> May 2016        5 #> Jun 2016        6 #> Jul 2016        7 #> Aug 2016        8 #> Sep 2016        9 #> Oct 2016       10 #> Nov 2016       11 #> Dec 2016       12  # Dates are appropriate zoo yearmon class within the data frame tk_tbl(data_zoo) #> # A tibble: 12 × 2 #>    index     value #>    <yearmon> <int> #>  1 Jan 2016      1 #>  2 Feb 2016      2 #>  3 Mar 2016      3 #>  4 Apr 2016      4 #>  5 May 2016      5 #>  6 Jun 2016      6 #>  7 Jul 2016      7 #>  8 Aug 2016      8 #>  9 Sep 2016      9 #> 10 Oct 2016     10 #> 11 Nov 2016     11 #> 12 Dec 2016     12"},{"path":"https://business-science.github.io/timetk/reference/tk_time_scale_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Get and modify the Time Scale Template — set_tk_time_scale_template","title":"Get and modify the Time Scale Template — set_tk_time_scale_template","text":"Get modify Time Scale Template","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_time_scale_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get and modify the Time Scale Template — set_tk_time_scale_template","text":"","code":"set_tk_time_scale_template(.data)  get_tk_time_scale_template()  tk_time_scale_template()"},{"path":"https://business-science.github.io/timetk/reference/tk_time_scale_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get and modify the Time Scale Template — set_tk_time_scale_template","text":".data tibble \"time_scale\", \"frequency\", \"trend\" columns.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_time_scale_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get and modify the Time Scale Template — set_tk_time_scale_template","text":"get_tk_time_scale_template(): Returns tibble containing time scale template information. set_tk_time_scale_template(): Returns nothing.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_time_scale_template.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get and modify the Time Scale Template — set_tk_time_scale_template","text":"Used get set time scale template, used tk_get_frequency() tk_get_trend() period = \"auto\". predefined template stored function tk_time_scale_template(). default used timetk. Changing Default Template can access current template get_tk_time_scale_template(). can modify current template set_tk_time_scale_template().","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_time_scale_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get and modify the Time Scale Template — set_tk_time_scale_template","text":"","code":"get_tk_time_scale_template() #> # A tibble: 8 × 3 #>   time_scale frequency trend    #>   <chr>      <chr>     <chr>    #> 1 second     1 hour    12 hours #> 2 minute     1 day     14 days  #> 3 hour       1 day     1 month  #> 4 day        1 week    3 months #> 5 week       1 quarter 1 year   #> 6 month      1 year    5 years  #> 7 quarter    1 year    10 years #> 8 year       5 years   30 years  set_tk_time_scale_template(tk_time_scale_template())"},{"path":"https://business-science.github.io/timetk/reference/tk_time_series_cv_plan.html","id":null,"dir":"Reference","previous_headings":"","what":"Time Series Resample Plan Data Preparation — tk_time_series_cv_plan","title":"Time Series Resample Plan Data Preparation — tk_time_series_cv_plan","text":"tk_time_series_cv_plan() function provides simple interface prepare time series resample specification (rset) either rolling_origin time_series_cv class.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_time_series_cv_plan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Time Series Resample Plan Data Preparation — tk_time_series_cv_plan","text":"","code":"tk_time_series_cv_plan(.data)"},{"path":"https://business-science.github.io/timetk/reference/tk_time_series_cv_plan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Time Series Resample Plan Data Preparation — tk_time_series_cv_plan","text":".data time series resample specification either rolling_origin time_series_cv class.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_time_series_cv_plan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Time Series Resample Plan Data Preparation — tk_time_series_cv_plan","text":"tibble containing time series crossvalidation plan.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_time_series_cv_plan.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Time Series Resample Plan Data Preparation — tk_time_series_cv_plan","text":"Resample Set resample set output timetk::time_series_cv() function rsample::rolling_origin() function.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_time_series_cv_plan.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Time Series Resample Plan Data Preparation — tk_time_series_cv_plan","text":"","code":"library(dplyr) library(rsample)  FB_tbl <- FANG %>%     filter(symbol == \"FB\") %>%     select(symbol, date, adjusted)  resample_spec <- time_series_cv(     FB_tbl,     initial = 150, assess = 50, skip = 50,     cumulative = FALSE,     lag = 30,     slice_limit = n()) #> Using date_var: date  resample_spec %>% tk_time_series_cv_plan() #> # A tibble: 3,910 × 5 #>    .id     .key     symbol date       adjusted #>    <chr>   <fct>    <chr>  <date>        <dbl> #>  1 Slice01 training FB     2016-03-18     111. #>  2 Slice01 training FB     2016-03-21     112. #>  3 Slice01 training FB     2016-03-22     112. #>  4 Slice01 training FB     2016-03-23     113. #>  5 Slice01 training FB     2016-03-24     113. #>  6 Slice01 training FB     2016-03-28     114. #>  7 Slice01 training FB     2016-03-29     116. #>  8 Slice01 training FB     2016-03-30     115. #>  9 Slice01 training FB     2016-03-31     114. #> 10 Slice01 training FB     2016-04-01     116. #> # ℹ 3,900 more rows"},{"path":"https://business-science.github.io/timetk/reference/tk_ts.html","id":null,"dir":"Reference","previous_headings":"","what":"Coerce time series objects and tibbles with date/date-time columns to ts. — tk_ts","title":"Coerce time series objects and tibbles with date/date-time columns to ts. — tk_ts","text":"Coerce time series objects tibbles date/date-time columns ts.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_ts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coerce time series objects and tibbles with date/date-time columns to ts. — tk_ts","text":"","code":"tk_ts(   data,   select = NULL,   start = 1,   end = numeric(),   frequency = 1,   deltat = 1,   ts.eps = getOption(\"ts.eps\"),   silent = FALSE )  tk_ts_(   data,   select = NULL,   start = 1,   end = numeric(),   frequency = 1,   deltat = 1,   ts.eps = getOption(\"ts.eps\"),   silent = FALSE )"},{"path":"https://business-science.github.io/timetk/reference/tk_ts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coerce time series objects and tibbles with date/date-time columns to ts. — tk_ts","text":"data time-based tibble time-series object. select Applicable tibbles data frames . column set columns coerced ts class. start time first observation.  Either single     number vector two numbers (second integer),     specify natural time     unit (1-based) number samples time unit.  See     examples use second form. end time last observation, specified way     start. frequency number observations per unit time. deltat fraction sampling period successive     observations; e.g., 1/12 monthly data.  one     frequency deltat provided. ts.eps time series comparison tolerance.  Frequencies     considered equal absolute difference less     ts.eps. silent Used toggle printing messages warnings.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_ts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Coerce time series objects and tibbles with date/date-time columns to ts. — tk_ts","text":"Returns ts object.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_ts.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Coerce time series objects and tibbles with date/date-time columns to ts. — tk_ts","text":"tk_ts() wrapper stats::ts() designed coerce tibble objects \"time-base\" (meaning values vary time) ts class objects. two main advantages: Non-numeric columns get removed instead populated NA's. returned ts object retains \"timetk index\" (various attributes) detected. \"timetk index\" can used coerce tbl, xts, zoo, ts data types. select argument used select subsets columns incoming data.frame. columns containing numeric data coerced. minimum, frequency start specified. non-data.frame object classes (e.g. xts, zoo, timeSeries, etc) objects coerced using stats::ts(). tk_ts_ nonstandard evaluation method.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_ts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coerce time series objects and tibbles with date/date-time columns to ts. — tk_ts","text":"","code":"library(dplyr)  ### tibble to ts: Comparison between tk_ts() and stats::ts() data_tbl <- tibble::tibble(     date = seq.Date(as.Date(\"2016-01-01\"), by = 1, length.out = 5),     x    = rep(\"chr values\", 5),     y    = cumsum(1:5),     z    = cumsum(11:15) * rnorm(1))  # as.ts: Character columns introduce NA's; Result does not retain index stats::ts(data_tbl[,-1], start = 2016) #> Time Series: #> Start = 2016  #> End = 2020  #> Frequency = 1  #>      x  y         z #> 2016 1  1  6.639721 #> 2017 1  3 13.883053 #> 2018 1  6 21.729996 #> 2019 1 10 30.180551 #> 2020 1 15 39.234716  # tk_ts: Only numeric columns get coerced; Result retains index in numeric format data_ts <- tk_ts(data_tbl, start = 2016) #> Warning: Non-numeric columns being dropped: date, x data_ts #> Time Series: #> Start = 2016  #> End = 2020  #> Frequency = 1  #>       y         z #> 2016  1  6.639721 #> 2017  3 13.883053 #> 2018  6 21.729996 #> 2019 10 30.180551 #> 2020 15 39.234716 #> attr(,\"index\") #> [1] 1451606400 1451692800 1451779200 1451865600 1451952000 #> attr(,\"index\")attr(,\"tzone\") #> [1] UTC #> attr(,\"index\")attr(,\"tclass\") #> [1] Date  # timetk index tk_index(data_ts, timetk_idx = FALSE)   # Regularized index returned #> [1] 2016 2017 2018 2019 2020 tk_index(data_ts, timetk_idx = TRUE)    # Original date index returned #> Warning: 'tzone' attributes are inconsistent #> [1] \"2016-01-01\" \"2016-01-02\" \"2016-01-03\" \"2016-01-04\" \"2016-01-05\"  # Coerce back to tibble data_ts %>% tk_tbl(timetk_idx = TRUE) #> Warning: 'tzone' attributes are inconsistent #> # A tibble: 5 × 3 #>   index          y     z #>   <date>     <dbl> <dbl> #> 1 2016-01-01     1  6.64 #> 2 2016-01-02     3 13.9  #> 3 2016-01-03     6 21.7  #> 4 2016-01-04    10 30.2  #> 5 2016-01-05    15 39.2    ### Using select tk_ts(data_tbl, select = y) #> Time Series: #> Start = 1  #> End = 5  #> Frequency = 1  #>       y #> [1,]  1 #> [2,]  3 #> [3,]  6 #> [4,] 10 #> [5,] 15 #> attr(,\"index\") #> [1] 1451606400 1451692800 1451779200 1451865600 1451952000 #> attr(,\"index\")attr(,\"tzone\") #> [1] UTC #> attr(,\"index\")attr(,\"tclass\") #> [1] Date   ### NSE: Enables programming select   <- \"y\" tk_ts_(data_tbl, select = select) #> Time Series: #> Start = 1  #> End = 5  #> Frequency = 1  #>       y #> [1,]  1 #> [2,]  3 #> [3,]  6 #> [4,] 10 #> [5,] 15 #> attr(,\"index\") #> [1] 1451606400 1451692800 1451779200 1451865600 1451952000 #> attr(,\"index\")attr(,\"tzone\") #> [1] UTC #> attr(,\"index\")attr(,\"tclass\") #> [1] Date"},{"path":"https://business-science.github.io/timetk/reference/tk_ts_dispatch_.html","id":null,"dir":"Reference","previous_headings":"","what":"S3 methods for ts method dispatch — tk_ts_dispatch_","title":"S3 methods for ts method dispatch — tk_ts_dispatch_","text":"Method dispatch ts","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_ts_dispatch_.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"S3 methods for ts method dispatch — tk_ts_dispatch_","text":"","code":"tk_ts_dispatch_(data, select, start, end, frequency, deltat, ts.eps, silent)  tk_ts_.data.frame(data, select, start, end, frequency, deltat, ts.eps, silent)  tk_ts_.default(data, select, start, end, frequency, deltat, ts.eps, silent)"},{"path":"https://business-science.github.io/timetk/reference/tk_ts_dispatch_.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"S3 methods for ts method dispatch — tk_ts_dispatch_","text":"data time-based tibble time-series object. select Applicable tibbles data frames . column set columns coerced ts class. start time first observation.  Either single     number vector two numbers (second integer),     specify natural time     unit (1-based) number samples time unit.  See     examples use second form. end time last observation, specified way     start. frequency number observations per unit time. deltat fraction sampling period successive     observations; e.g., 1/12 monthly data.  one     frequency deltat provided. ts.eps time series comparison tolerance.  Frequencies     considered equal absolute difference less     ts.eps. silent Used toggle printing messages warnings.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_ts_dispatch_.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"S3 methods for ts method dispatch — tk_ts_dispatch_","text":"character vector","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_tsfeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Time series feature matrix (Tidy) — tk_tsfeatures","title":"Time series feature matrix (Tidy) — tk_tsfeatures","text":"tk_tsfeatures() tidyverse compliant wrapper tsfeatures::tsfeatures(). function computes matrix time series features describes various time series. designed groupwise analysis using dplyr groups.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_tsfeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Time series feature matrix (Tidy) — tk_tsfeatures","text":"","code":"tk_tsfeatures(   .data,   .date_var,   .value,   .period = \"auto\",   .features = c(\"frequency\", \"stl_features\", \"entropy\", \"acf_features\"),   .scale = TRUE,   .trim = FALSE,   .trim_amount = 0.1,   .parallel = FALSE,   .na_action = na.pass,   .prefix = \"ts_\",   .silent = TRUE,   ... )"},{"path":"https://business-science.github.io/timetk/reference/tk_tsfeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Time series feature matrix (Tidy) — tk_tsfeatures","text":".data tibble data.frame time-based column .date_var column containing either date date-time values .value column containing numeric values .period periodicity (frequency) time series data. Values can provided follows: \"auto\" (default) Calculates using tk_get_frequency(). \"2 weeks\": calculate median number observations 2-week window. 7 (numeric): interpret ts frequency 7 observations per cycle (common weekly data) .features Passed features underlying tsfeatures() function. vector function names represent feature aggregation function. Examples: Use one function names tsfeatures R package e.g.(\"lumpiness\", \"stl_features\"). Use function name (e.g. \"mean\" \"median\") Create function provide function name .scale TRUE, time series scaled mean 0 sd 1 features computed. .trim TRUE, time series trimmed trim_amount features computed. Values larger trim_amount absolute value set NA. .trim_amount Default level trimming trim==TRUE. Default: 0.1. .parallel TRUE, multiple cores (multiple sessions) used. speeds things large number time series. .parallel = TRUE, multiprocess = future::multisession. can adjusted setting multiprocess parameter. See tsfeatures::tsfeatures() function mor details. .na_action function handle missing values. Use na.interp estimate missing values. .prefix prefix prefix feature columns. Default: \"ts_\". .silent Whether show messages warnings. ... arguments get passed feature functions.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_tsfeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Time series feature matrix (Tidy) — tk_tsfeatures","text":"tibble data.frame aggregated features describe time series.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_tsfeatures.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Time series feature matrix (Tidy) — tk_tsfeatures","text":"timetk::tk_tsfeatures() function implements tsfeatures package computing aggregated feature matrix time series useful many types analysis clustering time series. timetk version ports tsfeatures::tsfeatures() function tidyverse-compliant format uses tidy data frame containing grouping columns (optional), date column, value column. columns ignored. becomes easy summarize time series group-wise application .features, simply functions evaluate time series return single aggregated value. (Example: \"mean\" return mean time series (note values scaled mean 1 sd 0 first)) Function Internals: Internally, time series converted ts class using tk_ts(.period) period frequency time series. Values can provided .period, used prior convertion ts class. function leverages tsfeatures::tsfeatures() compute feature matrix summarized feature values.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_tsfeatures.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Time series feature matrix (Tidy) — tk_tsfeatures","text":"Rob Hyndman, Yanfei Kang, Pablo Montero-Manso, Thiyanga Talagala, Earo Wang, Yangzhuoran Yang, Mitchell O'Hara-Wild: tsfeatures R package","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_tsfeatures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Time series feature matrix (Tidy) — tk_tsfeatures","text":"","code":"library(dplyr)  walmart_sales_weekly %>%     group_by(id) %>%     tk_tsfeatures(       .date_var = Date,       .value    = Weekly_Sales,       .period   = 52,       .features = c(\"frequency\", \"stl_features\", \"entropy\", \"acf_features\", \"mean\"),       .scale    = TRUE,       .prefix   = \"ts_\"     ) #> # A tibble: 7 × 22 #> # Groups:   id [7] #>   id    ts_frequency ts_nperiods ts_seasonal_period ts_trend    ts_spike #>   <fct>        <dbl>       <dbl>              <dbl>    <dbl>       <dbl> #> 1 1_1             52           1                 52 0.000670 0.0000280   #> 2 1_3             52           1                 52 0.0614   0.00000987  #> 3 1_8             52           1                 52 0.756    0.00000195  #> 4 1_13            52           1                 52 0.354    0.00000475  #> 5 1_38            52           1                 52 0.425    0.0000179   #> 6 1_93            52           1                 52 0.791    0.000000754 #> 7 1_95            52           1                 52 0.639    0.000000567 #> # ℹ 16 more variables: ts_linearity <dbl>, ts_curvature <dbl>, ts_e_acf1 <dbl>, #> #   ts_e_acf10 <dbl>, ts_seasonal_strength <dbl>, ts_peak <dbl>, #> #   ts_trough <dbl>, ts_entropy <dbl>, ts_x_acf1 <dbl>, ts_x_acf10 <dbl>, #> #   ts_diff1_acf1 <dbl>, ts_diff1_acf10 <dbl>, ts_diff2_acf1 <dbl>, #> #   ts_diff2_acf10 <dbl>, ts_seas_acf1 <dbl>, ts_mean <dbl>"},{"path":"https://business-science.github.io/timetk/reference/tk_xts.html","id":null,"dir":"Reference","previous_headings":"","what":"Coerce time series objects and tibbles with date/date-time columns to xts. — tk_xts","title":"Coerce time series objects and tibbles with date/date-time columns to xts. — tk_xts","text":"Coerce time series objects tibbles date/date-time columns xts.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_xts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coerce time series objects and tibbles with date/date-time columns to xts. — tk_xts","text":"","code":"tk_xts(data, select = NULL, date_var = NULL, silent = FALSE, ...)  tk_xts_(data, select = NULL, date_var = NULL, silent = FALSE, ...)"},{"path":"https://business-science.github.io/timetk/reference/tk_xts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coerce time series objects and tibbles with date/date-time columns to xts. — tk_xts","text":"data time-based tibble time-series object. select Applicable tibbles data frames . column set columns coerced ts class. date_var Applicable tibbles data frames . Column name used order.. NULL default. NULL, function find date date-time column. silent Used toggle printing messages warnings. ... Additional parameters passed xts::xts(). Refer xts::xts().","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_xts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Coerce time series objects and tibbles with date/date-time columns to xts. — tk_xts","text":"Returns xts object.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_xts.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Coerce time series objects and tibbles with date/date-time columns to xts. — tk_xts","text":"tk_xts wrapper xts::xts() designed coerce tibble objects \"time-base\" (meaning values vary time) xts class objects. three main advantages: Non-numeric columns removed via select dropped user warned. prevents error coercion issue occurring. date column auto-detected specified date_var. takes effort user assign date vector coercion. ts objects automatically coerced \"timetk index\" present. Refer tk_ts(). select argument can used select subsets columns incoming data.frame. columns containing numeric data coerced. date_var can used specify column date index. date_var = NULL, date / date-time column interpreted. Optionally, order.argument underlying xts::xts() function can used. user must pass vector dates date-times order.used. non-data.frame object classes (e.g. xts, zoo, timeSeries, etc) objects coerced using xts::xts(). tk_xts_ nonstandard evaluation method.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_xts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coerce time series objects and tibbles with date/date-time columns to xts. — tk_xts","text":"","code":"library(dplyr)  ### tibble to xts: Comparison between tk_xts() and xts::xts() data_tbl <- tibble::tibble(     date = seq.Date(as.Date(\"2016-01-01\"), by = 1, length.out = 5),     x    = rep(\"chr values\", 5),     y    = cumsum(1:5),     z    = cumsum(11:15) * rnorm(1))  # xts: Character columns cause coercion issues; order.by must be passed a vector of dates xts::xts(data_tbl[,-1], order.by = data_tbl$date) #>                     x  y         z #> 2016-01-01 chr values  1  2.113644 #> 2016-01-02 chr values  3  4.419437 #> 2016-01-03 chr values  6  6.917379 #> 2016-01-04 chr values 10  9.607471 #> 2016-01-05 chr values 15 12.489712  # tk_xts: Non-numeric columns automatically dropped; No need to specify date column tk_xts(data_tbl) #> Warning: Non-numeric columns being dropped: date, x #> Using column `date` for date_var. #>             y         z #> 2016-01-01  1  2.113644 #> 2016-01-02  3  4.419437 #> 2016-01-03  6  6.917379 #> 2016-01-04 10  9.607471 #> 2016-01-05 15 12.489712  # ts can be coerced back to xts data_tbl %>%     tk_ts(start = 2016, freq = 365) %>%     tk_xts() #> Warning: Non-numeric columns being dropped: date, x #> Warning: 'tzone' attributes are inconsistent #>             y         z #> 2016-01-01  1  2.113644 #> 2016-01-02  3  4.419437 #> 2016-01-03  6  6.917379 #> 2016-01-04 10  9.607471 #> 2016-01-05 15 12.489712  ### Using select and date_var tk_xts(data_tbl, select = y, date_var = date) #>             y #> 2016-01-01  1 #> 2016-01-02  3 #> 2016-01-03  6 #> 2016-01-04 10 #> 2016-01-05 15   ### NSE: Enables programming date_var <- \"date\" select   <- \"y\" tk_xts_(data_tbl, select = select, date_var = date_var) #>             y #> 2016-01-01  1 #> 2016-01-02  3 #> 2016-01-03  6 #> 2016-01-04 10 #> 2016-01-05 15"},{"path":"https://business-science.github.io/timetk/reference/tk_zoo.html","id":null,"dir":"Reference","previous_headings":"","what":"Coerce time series objects and tibbles with date/date-time columns to xts. — tk_zoo","title":"Coerce time series objects and tibbles with date/date-time columns to xts. — tk_zoo","text":"Coerce time series objects tibbles date/date-time columns xts.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_zoo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coerce time series objects and tibbles with date/date-time columns to xts. — tk_zoo","text":"","code":"tk_zoo(data, select = NULL, date_var = NULL, silent = FALSE, ...)  tk_zoo_(data, select = NULL, date_var = NULL, silent = FALSE, ...)"},{"path":"https://business-science.github.io/timetk/reference/tk_zoo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coerce time series objects and tibbles with date/date-time columns to xts. — tk_zoo","text":"data time-based tibble time-series object. select Applicable tibbles data frames . column set columns coerced ts class. date_var Applicable tibbles data frames . Column name used order.. NULL default. NULL, function find date date-time column. silent Used toggle printing messages warnings. ... Additional parameters passed xts::xts(). Refer xts::xts().","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_zoo.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Coerce time series objects and tibbles with date/date-time columns to xts. — tk_zoo","text":"Returns zoo object.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_zoo.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Coerce time series objects and tibbles with date/date-time columns to xts. — tk_zoo","text":"tk_zoo wrapper zoo::zoo() designed coerce tibble objects \"time-base\" (meaning values vary time) zoo class objects. three main advantages: Non-numeric columns removed via select dropped user warned. prevents error coercion issue occurring. date column auto-detected specified date_var. takes effort user assign date vector coercion. ts objects automatically coerced \"timetk index\" present. Refer tk_ts(). select argument can used select subsets columns incoming data.frame. columns containing numeric data coerced. date_var can used specify column date index. date_var = NULL, date / date-time column interpreted. Optionally, order.argument underlying zoo::zoo() function can used. user must pass vector dates date-times order.used. Important Note: ... arguments passed xts::xts(), enables additional information (e.g. time zone) attribute zoo object. non-data.frame object classes (e.g. xts, zoo, timeSeries, etc) objects coerced using zoo::zoo(). tk_zoo_ nonstandard evaluation method.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_zoo.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coerce time series objects and tibbles with date/date-time columns to xts. — tk_zoo","text":"","code":"library(dplyr)  ### tibble to zoo: Comparison between tk_zoo() and zoo::zoo() data_tbl <- dplyr::tibble(     date = seq.Date(as.Date(\"2016-01-01\"), by = 1, length.out = 5),     x    = rep(\"chr values\", 5),     y    = cumsum(1:5),     z    = cumsum(11:15) * rnorm(1))  # zoo: Characters will cause error; order.by must be passed a vector of dates zoo::zoo(data_tbl[,-c(1,2)], order.by = data_tbl$date) #>             y         z #> 2016-01-01  1  9.308031 #> 2016-01-02  3 19.462247 #> 2016-01-03  6 30.462648 #> 2016-01-04 10 42.309233 #> 2016-01-05 15 55.002003  # tk_zoo: Character columns dropped with a warning; No need to specify dates (auto detected) tk_zoo(data_tbl) #> Warning: Non-numeric columns being dropped: date, x #> Using column `date` for date_var. #>             y         z #> 2016-01-01  1  9.308031 #> 2016-01-02  3 19.462247 #> 2016-01-03  6 30.462648 #> 2016-01-04 10 42.309233 #> 2016-01-05 15 55.002003  # ts can be coerced back to zoo data_tbl %>%     tk_ts(start = 2016, freq = 365) %>%     tk_zoo() #> Warning: Non-numeric columns being dropped: date, x #> Warning: 'tzone' attributes are inconsistent #>             y         z #> 2016-01-01  1  9.308031 #> 2016-01-02  3 19.462247 #> 2016-01-03  6 30.462648 #> 2016-01-04 10 42.309233 #> 2016-01-05 15 55.002003   ### Using select and date_var tk_zoo(data_tbl, select = y, date_var = date) #>             y #> 2016-01-01  1 #> 2016-01-02  3 #> 2016-01-03  6 #> 2016-01-04 10 #> 2016-01-05 15   ### NSE: Enables programming date_var <- \"date\" select   <- \"y\" tk_zoo_(data_tbl, select = select, date_var = date_var) #>             y #> 2016-01-01  1 #> 2016-01-02  3 #> 2016-01-03  6 #> 2016-01-04 10 #> 2016-01-05 15"},{"path":"https://business-science.github.io/timetk/reference/tk_zooreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Coerce time series objects and tibbles with date/date-time columns to ts. — tk_zooreg","title":"Coerce time series objects and tibbles with date/date-time columns to ts. — tk_zooreg","text":"Coerce time series objects tibbles date/date-time columns ts.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_zooreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coerce time series objects and tibbles with date/date-time columns to ts. — tk_zooreg","text":"","code":"tk_zooreg(   data,   select = NULL,   date_var = NULL,   start = 1,   end = numeric(),   frequency = 1,   deltat = 1,   ts.eps = getOption(\"ts.eps\"),   order.by = NULL,   silent = FALSE )  tk_zooreg_(   data,   select = NULL,   date_var = NULL,   start = 1,   end = numeric(),   frequency = 1,   deltat = 1,   ts.eps = getOption(\"ts.eps\"),   order.by = NULL,   silent = FALSE )"},{"path":"https://business-science.github.io/timetk/reference/tk_zooreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coerce time series objects and tibbles with date/date-time columns to ts. — tk_zooreg","text":"data time-based tibble time-series object. select Applicable tibbles data frames . column set columns coerced zooreg class. date_var Applicable tibbles data frames . Column name used order.. NULL default. NULL, function find date date-time column. start time first observation.  Either single number     vector two integers, specify natural time unit     (1-based) number samples time unit. end time last observation, specified way     start. frequency number observations per unit time. deltat fraction sampling period successive     observations; e.g., 1/12 monthly data.  one     frequency deltat provided. ts.eps time series comparison tolerance.  Frequencies considered     equal absolute difference less ts.eps. order.vector observations x     ordered. specified arguments start     end ignored zoo(data, order., frequency)     called. See zoo information. silent Used toggle printing messages warnings.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_zooreg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Coerce time series objects and tibbles with date/date-time columns to ts. — tk_zooreg","text":"Returns zooreg object.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_zooreg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Coerce time series objects and tibbles with date/date-time columns to ts. — tk_zooreg","text":"tk_zooreg() wrapper zoo::zooreg() designed coerce tibble objects \"time-base\" (meaning values vary time) zooreg class objects. two main advantages: Non-numeric columns get removed instead causing coercion issues. index present, returned zooreg object retains index retrievable using tk_index(). select argument used select subsets columns incoming data.frame. date_var can used specify column date index. date_var = NULL, date / date-time column interpreted. Optionally, order.argument underlying xts::xts() function can used. user must pass vector dates date-times order.used. columns containing numeric data coerced. minimum, frequency start specified. non-data.frame object classes (e.g. xts, zoo, timeSeries, etc) objects coerced using zoo::zooreg(). tk_zooreg_ nonstandard evaluation method.","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/tk_zooreg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coerce time series objects and tibbles with date/date-time columns to ts. — tk_zooreg","text":"","code":"### tibble to zooreg: Comparison between tk_zooreg() and zoo::zooreg() data_tbl <- tibble::tibble(     date = seq.Date(as.Date(\"2016-01-01\"), by = 1, length.out = 5),     x    = rep(\"chr values\", 5),     y    = cumsum(1:5),     z    = cumsum(11:15) * rnorm(1))  # zoo::zooreg: Values coerced to character; Result does not retain index data_zooreg <- zoo::zooreg(data_tbl[,-1], start = 2016, freq = 365) data_zooreg                # Numeric values coerced to character #>         x          y  z         #> 2016(1) chr values  1 -14.35629 #> 2016(2) chr values  3 -30.01769 #> 2016(3) chr values  6 -46.98421 #> 2016(4) chr values 10 -65.25585 #> 2016(5) chr values 15 -84.83261 rownames(data_zooreg)      # NULL, no dates retained #> NULL  # tk_zooreg: Only numeric columns get coerced; Result retains index as rownames data_tk_zooreg <- tk_zooreg(data_tbl, start = 2016, freq = 365) #> Warning: Non-numeric columns being dropped: date, x data_tk_zooreg             # No inadvertent coercion to character class #>          y         z #> 2016(1)  1 -14.35629 #> 2016(2)  3 -30.01769 #> 2016(3)  6 -46.98421 #> 2016(4) 10 -65.25585 #> 2016(5) 15 -84.83261  # timetk index tk_index(data_tk_zooreg, timetk_idx = FALSE)   # Regularized index returned #> [1] 2016.000 2016.003 2016.005 2016.008 2016.011 tk_index(data_tk_zooreg, timetk_idx = TRUE)    # Original date index returned #> [1] \"2016-01-01\" \"2016-01-02\" \"2016-01-03\" \"2016-01-04\" \"2016-01-05\"  ### Using select and date_var tk_zooreg(data_tbl, select = y, date_var = date, start = 2016, freq = 365) #>          y #> 2016(1)  1 #> 2016(2)  3 #> 2016(3)  6 #> 2016(4) 10 #> 2016(5) 15   ### NSE: Enables programming select   <- \"y\" date_var <- \"date\" tk_zooreg_(data_tbl, select = select, date_var = date_var, start = 2016, freq = 365) #>          y #> 2016(1)  1 #> 2016(2)  3 #> 2016(3)  6 #> 2016(4) 10 #> 2016(5) 15"},{"path":"https://business-science.github.io/timetk/reference/tk_zooreg_dispatch_.html","id":null,"dir":"Reference","previous_headings":"","what":"S3 methods for zooreg method dispatch — tk_zooreg_dispatch_","title":"S3 methods for zooreg method dispatch — tk_zooreg_dispatch_","text":"Method dispatch Zooreg","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_zooreg_dispatch_.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"S3 methods for zooreg method dispatch — tk_zooreg_dispatch_","text":"","code":"tk_zooreg_dispatch_(   data,   select,   date_var,   start,   end,   frequency,   deltat,   ts.eps,   order.by,   silent )  tk_zooreg_.data.frame(   data,   select,   date_var,   start,   end,   frequency,   deltat,   ts.eps,   order.by,   silent )  tk_zooreg_.default(   data,   select,   date_var,   start,   end,   frequency,   deltat,   ts.eps,   order.by,   silent )"},{"path":"https://business-science.github.io/timetk/reference/tk_zooreg_dispatch_.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"S3 methods for zooreg method dispatch — tk_zooreg_dispatch_","text":"data time-based tibble time-series object. select Applicable tibbles data frames . column set columns coerced zooreg class. date_var Applicable tibbles data frames . Column name used order.. NULL default. NULL, function find date date-time column. start time first observation.  Either single number     vector two integers, specify natural time unit     (1-based) number samples time unit. end time last observation, specified way     start. frequency number observations per unit time. deltat fraction sampling period successive     observations; e.g., 1/12 monthly data.  one     frequency deltat provided. ts.eps time series comparison tolerance.  Frequencies considered     equal absolute difference less ts.eps. order.vector observations x     ordered. specified arguments start     end ignored zoo(data, order., frequency)     called. See zoo information. silent Used toggle printing messages warnings.","code":""},{"path":"https://business-science.github.io/timetk/reference/tk_zooreg_dispatch_.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"S3 methods for zooreg method dispatch — tk_zooreg_dispatch_","text":"character vector","code":""},{"path":"https://business-science.github.io/timetk/reference/ts_clean_vec.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace Outliers & Missing Values in a Time Series — ts_clean_vec","title":"Replace Outliers & Missing Values in a Time Series — ts_clean_vec","text":"mainly wrapper outlier cleaning function, tsclean(), forecast R package. ts_clean_vec() function includes arguments applying seasonality numeric vector (non-ts) via period argument.","code":""},{"path":"https://business-science.github.io/timetk/reference/ts_clean_vec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace Outliers & Missing Values in a Time Series — ts_clean_vec","text":"","code":"ts_clean_vec(x, period = 1, lambda = NULL)"},{"path":"https://business-science.github.io/timetk/reference/ts_clean_vec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace Outliers & Missing Values in a Time Series — ts_clean_vec","text":"x numeric vector. period seasonal period use transformation. period = 1, seasonality included supsmu() used fit trend. period > 1, robust STL decomposition first performed linear interpolation applied seasonally adjusted data. lambda box cox transformation parameter. set \"auto\", performs automated lambda selection.","code":""},{"path":"https://business-science.github.io/timetk/reference/ts_clean_vec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Replace Outliers & Missing Values in a Time Series — ts_clean_vec","text":"numeric vector missing values /anomalies transformed imputed values.","code":""},{"path":"https://business-science.github.io/timetk/reference/ts_clean_vec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Replace Outliers & Missing Values in a Time Series — ts_clean_vec","text":"Cleaning Outliers Non-Seasonal (period = 1): Uses stats::supsmu() Seasonal (period > 1): Uses forecast::mstl() robust = TRUE (robust STL decomposition) seasonal series. estimate missing values outlier replacements, linear interpolation used (possibly seasonally adjusted) series. See forecast::tsoutliers() outlier detection method. Box Cox Transformation many circumstances, Box Cox transformation can help. Especially series multiplicative meaning variance grows exponentially. Box Cox transformation can automated setting lambda = \"auto\" can specified setting lambda = numeric value.","code":""},{"path":"https://business-science.github.io/timetk/reference/ts_clean_vec.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Replace Outliers & Missing Values in a Time Series — ts_clean_vec","text":"Forecast R Package Forecasting Principles & Practices: Dealing missing values outliers","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/ts_clean_vec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Replace Outliers & Missing Values in a Time Series — ts_clean_vec","text":"","code":"library(dplyr)   # --- VECTOR ----  values <- c(1,2,3, 4*2, 5,6,7, NA, 9,10,11, 12*2) values #>  [1]  1  2  3  8  5  6  7 NA  9 10 11 24  # Linear interpolation + Outlier Cleansing ts_clean_vec(values, period = 1, lambda = NULL) #>  [1] 1 2 3 4 5 6 7 8 9 9 9 9  # Seasonal Interpolation: set period = 4 ts_clean_vec(values, period = 4, lambda = NULL) #>  [1]  1.00000  2.00000  3.00000  8.00000  5.00000  6.00000  7.00000 11.25703 #>  [9]  9.00000 10.00000 10.00000 14.00000  # Seasonal Interpolation with Box Cox Transformation (internal) ts_clean_vec(values, period = 4, lambda = \"auto\") #>  [1]  1.000000  2.000000  3.000000  8.444127  3.832690  6.000000  7.000000 #>  [8] 15.895521  9.000000 10.000000 11.000000 24.000000"},{"path":"https://business-science.github.io/timetk/reference/ts_impute_vec.html","id":null,"dir":"Reference","previous_headings":"","what":"Missing Value Imputation for Time Series — ts_impute_vec","title":"Missing Value Imputation for Time Series — ts_impute_vec","text":"mainly wrapper Seasonally Adjusted Missing Value using Linear Interpolation function, na.interp(), forecast R package. ts_impute_vec() function includes arguments applying seasonality numeric vector (non-ts) via period argument.","code":""},{"path":"https://business-science.github.io/timetk/reference/ts_impute_vec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Missing Value Imputation for Time Series — ts_impute_vec","text":"","code":"ts_impute_vec(x, period = 1, lambda = NULL)"},{"path":"https://business-science.github.io/timetk/reference/ts_impute_vec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Missing Value Imputation for Time Series — ts_impute_vec","text":"x numeric vector. period seasonal period use transformation. period = 1, linear interpolation performed. period > 1, robust STL decomposition first performed linear interpolation applied seasonally adjusted data. lambda box cox transformation parameter. set \"auto\", performs automated lambda selection.","code":""},{"path":"https://business-science.github.io/timetk/reference/ts_impute_vec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Missing Value Imputation for Time Series — ts_impute_vec","text":"numeric vector missing values imputed.","code":""},{"path":"https://business-science.github.io/timetk/reference/ts_impute_vec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Missing Value Imputation for Time Series — ts_impute_vec","text":"Imputation using Linear Interpolation Three circumstances cause strictly linear interpolation: Period 1: period = 1, seasonality interpreted therefore linear used. Number Non-Missing Values less 2-Periods: Insufficient values exist detect seasonality. Number Total Values less 3-Periods: Insufficient values exist detect seasonality. Seasonal Imputation using Linear Interpolation seasonal series period > 1, robust Seasonal Trend Loess (STL) decomposition first computed. linear interpolation applied seasonally adjusted data, seasonal component added back. Box Cox Transformation many circumstances, Box Cox transformation can help. Especially series multiplicative meaning variance grows exponentially. Box Cox transformation can automated setting lambda = \"auto\" can specified setting lambda = numeric value.","code":""},{"path":"https://business-science.github.io/timetk/reference/ts_impute_vec.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Missing Value Imputation for Time Series — ts_impute_vec","text":"Forecast R Package Forecasting Principles & Practices: Dealing missing values outliers","code":""},{"path":[]},{"path":"https://business-science.github.io/timetk/reference/ts_impute_vec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Missing Value Imputation for Time Series — ts_impute_vec","text":"","code":"library(dplyr)   # --- VECTOR ----  values <- c(1,2,3, 4*2, 5,6,7, NA, 9,10,11, 12*2) values #>  [1]  1  2  3  8  5  6  7 NA  9 10 11 24  # Linear interpolation ts_impute_vec(values, period = 1, lambda = NULL) #>  [1]  1  2  3  8  5  6  7  8  9 10 11 24  # Seasonal Interpolation: set period = 4 ts_impute_vec(values, period = 4, lambda = NULL) #>  [1]  1  2  3  8  5  6  7  8  9 10 11 24  # Seasonal Interpolation with Box Cox Transformation (internal) ts_impute_vec(values, period = 4, lambda = \"auto\") #>  [1]  1.000000  2.000000  3.000000  8.000000  5.000000  6.000000  7.000000 #>  [8]  7.960572  9.000000 10.000000 11.000000 24.000000"},{"path":"https://business-science.github.io/timetk/reference/walmart_sales_weekly.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample Time Series Retail Data from the Walmart Recruiting Store Sales Forecasting Competition — walmart_sales_weekly","title":"Sample Time Series Retail Data from the Walmart Recruiting Store Sales Forecasting Competition — walmart_sales_weekly","text":"Kaggle \"Walmart Recruiting - Store Sales Forecasting\" Competition used retail data combinations stores departments within store. competition began February 20th, 2014 ended May 5th, 2014. competition included data 45 retail stores located different regions. dataset included various external features including Holiday information, Temperature, Fuel Price, Markdown. dataset includes Sample 7 departments Store ID 1 (7 total time series).","code":""},{"path":"https://business-science.github.io/timetk/reference/walmart_sales_weekly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample Time Series Retail Data from the Walmart Recruiting Store Sales Forecasting Competition — walmart_sales_weekly","text":"","code":"walmart_sales_weekly"},{"path":"https://business-science.github.io/timetk/reference/walmart_sales_weekly.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sample Time Series Retail Data from the Walmart Recruiting Store Sales Forecasting Competition — walmart_sales_weekly","text":"tibble: 9,743 x 3 id Factor. Unique series identifier (4 total) Store Numeric. Store ID. Dept Numeric. Department ID. Date Date. Weekly timestamp. Weekly_Sales Numeric. Sales given department given store. IsHoliday Logical. Whether week \"special\" holiday store. Type Character. Type identifier store. Size Numeric. Store square-footage Temperature Numeric. Average temperature region. Fuel_Price Numeric. Cost fuel region. MarkDown1, MarkDown2, MarkDown3, MarkDown4, MarkDown5 Numeric. Anonymized data related promotional markdowns Walmart running. MarkDown data available Nov 2011, available stores time. missing value marked NA. CPI Numeric. consumer price index. Unemployment Numeric. unemployment rate region.","code":""},{"path":"https://business-science.github.io/timetk/reference/walmart_sales_weekly.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Sample Time Series Retail Data from the Walmart Recruiting Store Sales Forecasting Competition — walmart_sales_weekly","text":"Kaggle Competition Website","code":""},{"path":"https://business-science.github.io/timetk/reference/walmart_sales_weekly.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sample Time Series Retail Data from the Walmart Recruiting Store Sales Forecasting Competition — walmart_sales_weekly","text":"sample 7 Weekly data sets Kaggle Walmart Recruiting Store Sales Forecasting competition. Holiday Features four holidays fall within following weeks dataset (holidays data): Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13 Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13 Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13 Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13","code":""},{"path":"https://business-science.github.io/timetk/reference/walmart_sales_weekly.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample Time Series Retail Data from the Walmart Recruiting Store Sales Forecasting Competition — walmart_sales_weekly","text":"","code":"walmart_sales_weekly #> # A tibble: 1,001 × 17 #>    id    Store  Dept Date       Weekly_Sales IsHoliday Type    Size Temperature #>    <fct> <dbl> <dbl> <date>            <dbl> <lgl>     <chr>  <dbl>       <dbl> #>  1 1_1       1     1 2010-02-05       24924. FALSE     A     151315        42.3 #>  2 1_1       1     1 2010-02-12       46039. TRUE      A     151315        38.5 #>  3 1_1       1     1 2010-02-19       41596. FALSE     A     151315        39.9 #>  4 1_1       1     1 2010-02-26       19404. FALSE     A     151315        46.6 #>  5 1_1       1     1 2010-03-05       21828. FALSE     A     151315        46.5 #>  6 1_1       1     1 2010-03-12       21043. FALSE     A     151315        57.8 #>  7 1_1       1     1 2010-03-19       22137. FALSE     A     151315        54.6 #>  8 1_1       1     1 2010-03-26       26229. FALSE     A     151315        51.4 #>  9 1_1       1     1 2010-04-02       57258. FALSE     A     151315        62.3 #> 10 1_1       1     1 2010-04-09       42961. FALSE     A     151315        65.9 #> # ℹ 991 more rows #> # ℹ 8 more variables: Fuel_Price <dbl>, MarkDown1 <dbl>, MarkDown2 <dbl>, #> #   MarkDown3 <dbl>, MarkDown4 <dbl>, MarkDown5 <dbl>, CPI <dbl>, #> #   Unemployment <dbl>"},{"path":"https://business-science.github.io/timetk/reference/wikipedia_traffic_daily.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample Daily Time Series Data from the Web Traffic Forecasting (Wikipedia) Competition — wikipedia_traffic_daily","title":"Sample Daily Time Series Data from the Web Traffic Forecasting (Wikipedia) Competition — wikipedia_traffic_daily","text":"Kaggle \"Web Traffic Forecasting\" (Wikipedia) Competition used Google Analytics Web Traffic Data 145,000 websites. time series represent number daily views different Wikipedia articles. competition began July 13th, 2017 ended November 15th, 2017. dataset includes Sample 10 article pages (10 total time series).","code":""},{"path":"https://business-science.github.io/timetk/reference/wikipedia_traffic_daily.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample Daily Time Series Data from the Web Traffic Forecasting (Wikipedia) Competition — wikipedia_traffic_daily","text":"","code":"wikipedia_traffic_daily"},{"path":"https://business-science.github.io/timetk/reference/wikipedia_traffic_daily.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sample Daily Time Series Data from the Web Traffic Forecasting (Wikipedia) Competition — wikipedia_traffic_daily","text":"tibble: 9,743 x 3 Page Character. Page information. date Date. Daily timestamp. value Numeric. Daily views wikipedia article.","code":""},{"path":"https://business-science.github.io/timetk/reference/wikipedia_traffic_daily.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Sample Daily Time Series Data from the Web Traffic Forecasting (Wikipedia) Competition — wikipedia_traffic_daily","text":"Kaggle Competition Website","code":""},{"path":"https://business-science.github.io/timetk/reference/wikipedia_traffic_daily.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sample Daily Time Series Data from the Web Traffic Forecasting (Wikipedia) Competition — wikipedia_traffic_daily","text":"sample 10 Daily data sets Kaggle Web Traffic Forecasting (Wikipedia) Competition","code":""},{"path":"https://business-science.github.io/timetk/reference/wikipedia_traffic_daily.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample Daily Time Series Data from the Web Traffic Forecasting (Wikipedia) Competition — wikipedia_traffic_daily","text":"","code":"wikipedia_traffic_daily #> # A tibble: 5,500 × 3 #>    Page                                                         date       value #>    <chr>                                                        <date>     <dbl> #>  1 Death_of_Freddie_Gray_en.wikipedia.org_mobile-web_all-agents 2015-07-01   791 #>  2 Death_of_Freddie_Gray_en.wikipedia.org_mobile-web_all-agents 2015-07-02   704 #>  3 Death_of_Freddie_Gray_en.wikipedia.org_mobile-web_all-agents 2015-07-03   903 #>  4 Death_of_Freddie_Gray_en.wikipedia.org_mobile-web_all-agents 2015-07-04   732 #>  5 Death_of_Freddie_Gray_en.wikipedia.org_mobile-web_all-agents 2015-07-05   558 #>  6 Death_of_Freddie_Gray_en.wikipedia.org_mobile-web_all-agents 2015-07-06   504 #>  7 Death_of_Freddie_Gray_en.wikipedia.org_mobile-web_all-agents 2015-07-07   543 #>  8 Death_of_Freddie_Gray_en.wikipedia.org_mobile-web_all-agents 2015-07-08  1156 #>  9 Death_of_Freddie_Gray_en.wikipedia.org_mobile-web_all-agents 2015-07-09  1196 #> 10 Death_of_Freddie_Gray_en.wikipedia.org_mobile-web_all-agents 2015-07-10   701 #> # ℹ 5,490 more rows"},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-290","dir":"Changelog","previous_headings":"","what":"timetk 2.9.0","title":"timetk 2.9.0","text":"CRAN release: 2023-10-31","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"anomalize-integration-2-9-0","dir":"Changelog","previous_headings":"","what":"Anomalize Integration:","title":"timetk 2.9.0","text":"anomalize R package now available timetk: anomlize(): 1 function breaks , identifies, cleans anomalies plot_anomalies(): Visualize anomalies anomaly bands plot_anomalies_decomp(): Visualize time series decomposition. Make adjustments needed. plot_anomalies_cleaned(): Visualize /cleaning anomalies. Note - anomalize(.method): .method = \"stl\" supported time. \"twitter\" method also planned.","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"other-changes-2-9-0","dir":"Changelog","previous_headings":"","what":"Other Changes:","title":"timetk 2.9.0","text":"Removed dependency tidymodels. (#154, @olivroy).","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-284","dir":"Changelog","previous_headings":"","what":"timetk 2.8.4","title":"timetk 2.8.4","text":"CRAN release: 2023-09-22 Update forecasting vignette: Use glmnet time series forecasting. CRAN Fixes: - tzdata time zone fixes: - GB -> Europe/London - NZ -> Pacific/Auckland - US/Eastern -> America/New_York - US/Pacific -> America/Los_Angeles - Add @aliases timetk-package","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-283","dir":"Changelog","previous_headings":"","what":"timetk 2.8.3","title":"timetk 2.8.3","text":"CRAN release: 2023-03-30 remove support robets remove tidyquant examples remove tidyverse examples add FANG dataset timetk (port tidyquant) cran: fix return, dontrun -> donttest, options(max.print)","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-282","dir":"Changelog","previous_headings":"","what":"timetk 2.8.2","title":"timetk 2.8.2","text":"CRAN release: 2022-11-17 New Features plot_time_series(): Gets new arguments specify .x_intercept .x_intercept_color. #131 Fixes Fix error plot_time_series() .group_names found. #121 Merge variable checking update needed recipes >= 1.0.3 #132","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-281","dir":"Changelog","previous_headings":"","what":"timetk 2.8.1","title":"timetk 2.8.1","text":"CRAN release: 2022-05-31","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"trelliscope-plotting-2-8-1","dir":"Changelog","previous_headings":"","what":"Trelliscope Plotting","title":"timetk 2.8.1","text":"plot_time_series() plot_time_series_boxplot() plot_anomaly_diagnostics()","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-280","dir":"Changelog","previous_headings":"","what":"timetk 2.8.0","title":"timetk 2.8.0","text":"CRAN release: 2022-04-07 New Features Many plotting functions upgraded use trelliscopejs easier visualization many time series. Gets new argument trelliscope: Used visualizing many time series. Gets new argument .facet_strip_remove remove facet strips since trelliscope automatically labeled. Gets new argument .facet_nrow adjust grid trelliscope. default argument facet_collapse = TRUE changed FALSE better compatibility Trelliscope JS. may cause plots multiple groups take extra space strip. Gets new argument trelliscope: Used visualizing many time series. Gets new argument .facet_strip_remove remove facet strips since trelliscope automatically labeled. Gets new argument .facet_nrow adjust grid trelliscope. default argument .facet_collapse = TRUE changed FALSE better compatibility Trelliscope JS. may cause plots multiple groups take extra space strip. Gets new argument trelliscope: Used visualizing many time series. Gets new argument .facet_strip_remove remove facet strips since trelliscope automatically labeled. Gets new argument .facet_nrow adjust grid trelliscope. default argument .facet_collapse = TRUE changed FALSE better compatibility Trelliscope JS. may cause plots multiple groups take extra space strip. Updates & Bug Fixes Recipes steps (e.g. step_timeseries_signature()) use new recipes::print_step() function. Requires recipes >= 0.2.0. #110 Offset parameter step_log_interval() working properly. Now works. #103 Potential Breaking Changes default argument .facet_collapse = TRUE changed FALSE better compatibility Trelliscope JS. may cause plots multiple groups take extra space strip.","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-270","dir":"Changelog","previous_headings":"","what":"timetk 2.7.0","title":"timetk 2.7.0","text":"CRAN release: 2022-01-19 New Features tk_tsfeatures(): new function makes easy generate time series feature matrix using tsfeatures. main benefit can pipe time series data tibbles dplyr groups. features produced group. #95 #84 plot_time_series_boxplot(): new function makes plotting time series boxplots simple using .period argument time series aggregation. New Vignettes Time Series Clustering: Uses new tk_tsfeatures() function perform time series clustering. #95 #84 Time Series Visualization: Updated include plot_time_series_boxplot() plot_time_series_regression(). Improvements Improvements point forecasting target n-periods future. time_series_cv(), time_series_split(): New parameter point_forecast. useful testing / assessing n-th prediction future. set TRUE, return single point returns last value assess. Fixes Updates rlang > 0.4.11 (dev version) #98 plot_time_series(): Smoother longer fails time series 1 observation #106","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-262","dir":"Changelog","previous_headings":"","what":"timetk 2.6.2","title":"timetk 2.6.2","text":"CRAN release: 2021-11-16 Improvements summarize_by_time(): Added .week_start argument allow specifying .week_start = 1 Monday start. Default 7 Sunday Start. can also changed lubridate setting lubridate.week.start option. Plotting Functions: Several plotting functions gain new .facet_dir argument adjusting direction facet_wrap(dir). #94 Plot ACF Diagnostics (plot_acf_diagnostics()): Change default parameter .show_white_noise_bars = TRUE. #85 plot_timeseries_regression(): Can now show_summary group-wise models visualizing groups Time Series CV (time_series_cv()): Add Label tune_results Improve speed pad_by_time(). #93 Bug Fixes tk_make_timeseries() tk_make_future_timeseries() now able handle end months. #72 tk_tbl.zoo(): Fix issue readr::type_convert() produces warning messages character columns inputs. #89 plot_time_series_regression(): Fixed issue lags added .formula. Pads lags NA. step_fourier() fourier_vec(): Fixed issue step_fourier failing one observation. Added scale_factor argument override date sequences stored scale factor. #77","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-261","dir":"Changelog","previous_headings":"","what":"timetk 2.6.1","title":"timetk 2.6.1","text":"CRAN release: 2021-01-18 Improvements tk_augment_slidify(), tk_augment_lags(), tk_augment_leads(), tk_augment_differences(): Now works multiple columns (passed via .value) tidyselect (e.g. contains()). Fixes Reduce “New names” messages. Remove dependency lazyeval. #24 Fix deprecated functions: select_() used tk_xts_(). #52","code":"#> New names: #> * NA -> ...1"},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-260","dir":"Changelog","previous_headings":"","what":"timetk 2.6.0","title":"timetk 2.6.0","text":"CRAN release: 2020-11-21 New Functions filter_period() (#64): Applies filtering expressions within time-based periods (windows). slice_period() (#64): Applies slices within time-based periods (windows). condense_period() (#64): Converts periodicity higher (e.g. daily) lower (e.g. monthly) frequency. Similar xts::.period() tibbletime::as_period(). tk_augment_leads() lead_vec() (#65): Added make easier / obvious create leads. Fixes time_series_cv(): Fix bug Panel Data. Train/Test Splits returning 1st observation final time stamp. return observations. future_frame() tk_make_future_timeseries(): Now sort incoming index ensure dates returned go future. tk_augment_lags() tk_augment_slidify(): Now overwrite column names match behavior tk_augment_fourier() tk_augment_differences().","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-250","dir":"Changelog","previous_headings":"","what":"timetk 2.5.0","title":"timetk 2.5.0","text":"CRAN release: 2020-10-22 Improvements time_series_cv(): Now works time series groups. great working panel data. future_frame(): Gets new argument called .bind_data. set TRUE, performs data binding operation incoming data future frame. Miscellaneous Tune startup messages (#63)","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-240","dir":"Changelog","previous_headings":"","what":"timetk 2.4.0","title":"timetk 2.4.0","text":"CRAN release: 2020-10-08 step_slidify_augment() - variant step slidify adds multiple rolling columns inside recipe. Bug Fixes Add warning %+time% %-time% return missing values Fix issues tk_make_timeseries() tk_make_future_timeseries() providing odd results regular time series. GitHub Issue 60","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-230","dir":"Changelog","previous_headings":"","what":"timetk 2.3.0","title":"timetk 2.3.0","text":"CRAN release: 2020-09-29 New Functionality tk_time_series_cv_plan() - Now works k-fold cross validation objects vfold_cv() function. pad_by_time() - Added new argument .fill_na_direction specify tidyr::fill() strategy filling missing data. Bug Fixes Augment functions (e.g. tk_augment_lags()) - Fix bug grouped functions exported Vectorized Functions - Compatabiliy ts class","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-221","dir":"Changelog","previous_headings":"","what":"timetk 2.2.1","title":"timetk 2.2.1","text":"CRAN release: 2020-09-01 New Functions step_log_interval_vec() - Extends log_interval_vec() recipes preprocessing. Parallel Processing Parallel backend use tune recipes Bug Fixes log_interval_vec() - Correct messaging complement.ts_cv_split - Helper show time series cross validation splits list explorer.","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-220","dir":"Changelog","previous_headings":"","what":"timetk 2.2.0","title":"timetk 2.2.0","text":"CRAN release: 2020-07-18 New Functions mutate_by_time(): applying mutates time windows log_interval_vec() & log_interval_inv_vec(): constrained interval forecasting. Improvements plot_acf_diagnostics(): new argument, .show_white_noise_bars adding white noise bars ACF / PACF Plot. pad_by_time(): New arguments .start_date .end_date expanding/contracting padding windows.","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-210","dir":"Changelog","previous_headings":"","what":"timetk 2.1.0","title":"timetk 2.1.0","text":"CRAN release: 2020-07-03 New Functions plot_time_series_regression(): Convenience function visualize & explore features using Linear Regression (stats::lm() formula). time_series_split(): convenient way return single split time_series_cv(). Returns split format rsample::initial_time_split(). Improvements Auto-detect date date-time: Affects summarise_by_time(), filter_by_time(), tk_summary_diagnostics tk_time_series_cv_plan(): Allow single resample rsample::initial_time_split timetk::time_series_split Updated Vignette: vignette, “Forecasting Using Time Series Signature”, updated modeltime tidymodels. Plotting Improvements plotting functions now support Tab Completion (minor breaking change needed , see breaking changes ) Add .legend_show toggle /legends. Permit numeric index (fix issue smoother failing) Breaking Changes plot_time_series() plot_acf_diagnostics() plot_anomaly_diagnostics() plot_seasonal_diagnostics() plot_stl_diagnostics() Bug Fixes fourier_vec() step_fourier_vec(): Add error observations zero difference. Issue #40.","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-200","dir":"Changelog","previous_headings":"","what":"timetk 2.0.0","title":"timetk 2.0.0","text":"CRAN release: 2020-05-31 New Interactive Plotting Functions plot_anomaly_diagnostics(): Visualize Anomalies One Time Series New Data Wrangling Functions future_frame(): Make future tibble existing time-based tibble. New Diagnostic / Data Processing Functions tk_anomaly_diagnostics() - Group-wise anomaly detection diagnostics. wrapper anomalize R package functions without importing anomalize. New Vectorized Functions: ts_clean_vec() - Replace Outliers & Missing Values Time Series standardize_vec() - Centers scales time series mean 0, standard deviation 1 normalize_vec() - Normalizes time series Range: (0, 1) New Recipes Preprocessing Steps: step_ts_pad() - Preprocessing padding time series data. Adds rows fill gaps can used step_ts_impute() interpolate going low high frequency! step_ts_clean() - Preprocessing step cleaning outliers imputing missing values time series. New Parsing Functions parse_date2() parse_datetime2(): similar readr::parse_date() lubridate::as_date() parse character vectors date datetimes. key advantage SPEED. parse_date2() uses anytime package process using C++ Boost.Date_Time library. Improvements: plot_acf_diagnostics(): .lags argument now handles time-based phrases (e.g. .lags = \"1 month\"). time_series_cv(): Implements time-based phrases (e.g. initial = \"5 years\" assess = \"1 year\") tk_make_future_timeseries(): n_future argument deprecated new length_out argument accepts numeric input (e.g. length_out = 12) time-based phrases (e.g. length_out = \"12 months\"). major improvement numeric values define number timestamps returned even weekends removed holidays removed. Thus, can always anticipate length. (Issue #19). diff_vec: Now reports initial values used differencing calculation. Bug Fixes: Fix name collision .value = .value. Respect timezones Fix incorrect calculation starts/stops Make skip = 1 default. skip = 0 make sense. Fix issue skip adding 1 stops. Fix printing method Prevent name collisions underlying data column “id” “splits” Fix bug day month doesn’t exist. Lubridate period() returns NA. Fix implemented ceiling_date(). Fix pad_value inserts pad values new row inserted. Fix issue lambda = NULL Breaking Changes: major impact since 1.0.0 version just released. Renamed impute_ts_vec() ts_impute_vec() consistency ts_clean_vec() Renamed step_impute_ts() step_ts_impute() consistency underlying function Renamed roll_apply_vec() slidify_vec() consistency slidify() & relationship slider R package Renamed step_roll_apply step_slidify() consistency slidify() & relationship slider R package Renamed tk_augment_roll_apply tk_augment_slidify() consistency slidify() & relationship slider R package plot_time_series_cv_plan() tk_time_series_cv_plan(): Changed argument .rset .data.","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-100","dir":"Changelog","previous_headings":"","what":"timetk 1.0.0","title":"timetk 1.0.0","text":"CRAN release: 2020-04-19 New Interactive Plotting Functions: plot_time_series() - workhorse time-series plotting function generates interactive plotly plots, consolidates 20+ lines ggplot2 code, scales well many time series using dplyr groups. plot_acf_diagnostics() - Visualize ACF, PACF, number CCFs one plot Multiple Time Series. Interactive plotly default. plot_seasonal_diagnostics() - Visualize Multiple Seasonality Features One Time Series. Interactive plotly default. plot_stl_diagnostics() - Visualize STL Decomposition Features One Time Series. plot_time_series_cv_plan() - Visualize Time Series Cross Validation plan made time_series_cv(). New Time Series Data Wrangling: summarise_by_time() - time-based variant dplyr::summarise() flexible summarization using common time-based criteria. filter_by_time() - time-based variant dplyr::filter() flexible filtering time-ranges. pad_by_time() - Insert time series rows regularly spaced timestamps. slidify() - Make function rolling / sliding function. between_time() - time-based variant dplyr::() flexible time-range detection. add_time() - Add time series index. Shifts index period. New Recipe Functions: Feature Generators: step_holiday_signature() - New recipe step adding 130 holiday features based individual holidays, locales, stock exchanges / business holidays. step_fourier() - New recipe step adding fourier transforms adding seasonal features time series data step_roll_apply() - New recipe step adding rolling summary functions. Similar recipes::step_window() flexible enabling application summary function. step_smooth() - New recipe step adding Local Polynomial Regression (LOESS) smoothing noisy time series step_diff() - New recipe adding multiple differenced columns. Similar recipes::step_lag(). step_box_cox() - New recipe transforming predictors. Similar step_BoxCox() improvements forecasting including “guerrero” method lambda selection handling negative data. step_impute_ts() - New recipe imputing time series. New Rsample Functions time_series_cv() - Create rsample cross validation sets time series. function produces sampling plan starting recent time series observations, rolling backwards. New Vector Functions: functions useful inside mutate() power many new plotting recipes functions. roll_apply_vec() - Vectorized rolling apply function - wraps slider::slide_vec() smooth_vec() - Vectorized smoothing function - Applies Local Polynomial Regression (LOESS) diff_vec() diff_inv_vec() - Vectorized differencing function. Pads NA’s default (unlike stats::diff). lag_vec() - Vectorized lag functions. Returns lags leads (negative lags) adjusting .lag argument. box_cox_vec(), box_cox_inv_vec(), & auto_lambda() - Vectorized Box Cox transformation. Leverages forecast::BoxCox.lambda() automatic lambda selection. fourier_vec() - Vectorized Fourier Series calculation. impute_ts_vec() - Vectorized imputation missing values time series. Leverages forecast::na.interp(). New Augment Functions: functions designed scale. respect dplyr::group_by(). tk_augment_holiday_signature() - Add holiday features data.frame using time-series index. tk_augment_roll_apply() - Add multiple columns rolling window calculations data.frame. tk_augment_differences() - Add multiple columns differences data.frame. tk_augment_lags() - Add multiple columns lags data.frame. tk_augment_fourier() - Add multiple columns fourier series data.frame. New Make Functions: Make date date-time sequences start end dates. tk_make_timeseries() - Super flexible function creating daily sub-daily time series. tk_make_weekday_sequence() - Weekday sequence accounts stripping weekends holidays tk_make_holiday_sequence() - Makes sequence dates corresponding business holidays calendars timeDate (common non-working days) tk_make_weekend_sequence() - Weekday sequence dates Saturday Sunday (common non-working days) New Get Functions: tk_get_holiday_signature() - Get 100+ holiday features using time-series index. tk_get_frequency() tk_get_trend() - Automatic frequency trend calculation time series index. New Diagnostic / Data Processing Functions tk_summary_diagnostics() - Group-wise time series summary. tk_acf_diagnostics() - data preparation function plot_acf_diagnostics() tk_seasonal_diagnostics() - data preparation function plot_seasonal_diagnostics() tk_stl_diagnostics() - Group-wise STL Decomposition (Season, Trend, Remainder). Data prep plot_stl_diagnostics(). tk_time_series_cv_plan - data preparation function plot_time_series_cv_plan() New Datasets M4 Competition - Sample “economic” datasets hourly, daily, weekly, monthly, quarterly, yearly. Walmart Recruiting Retail Sales Forecasting Competition - Sample 7 retail time series Web Traffic Forecasting (Wikipedia) Competition - Sample 10 website time series Taylor’s Energy Demand - Single time series 30-minute interval energy demand UCI Bike Sharing Daily - time series consisting Capital Bikesharing Transaction Counts related time-based features. Improvements: * tk_make_future_timeseries() - Now accepts n_future time-based phrase like “12 seconds” “1 year”. Bug Fixes: Don’t set timezone date - Accommodate recent changes lubridate::tz<- now returns POSIXct used Date objects. Fixed PR32 @vspinu. Potential Breaking Changes: tk_augment_timeseries_signature() - Changed data .data prevent name collisions piping.","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-013","dir":"Changelog","previous_headings":"","what":"timetk 0.1.3","title":"timetk 0.1.3","text":"CRAN release: 2020-03-18 New Features: step_timeseries_signature() - New step_timeseries_signature() adding date date-time features. New Vignette - “Time Series Machine Learning” (previously forecasting using time series signature) Bug Fixes: xts::indexTZ deprecated. Use tzone instead. Replace arrange_ arrange. Fix failing tests due tidyquant 1.0.0 upagrade (single stocks now return extra symbol column).","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-012","dir":"Changelog","previous_headings":"","what":"timetk 0.1.2","title":"timetk 0.1.2","text":"CRAN release: 2019-09-25 Compatability tidyquant v0.5.7 - Removed dependency tidyverse Dependency cleanup - removed devtools unncessary dependencies.","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-011","dir":"Changelog","previous_headings":"","what":"timetk 0.1.1","title":"timetk 0.1.1","text":"CRAN release: 2018-05-14 Added timeSeries Suggests satisfy CRAN issue.","code":""},{"path":"https://business-science.github.io/timetk/news/index.html","id":"timetk-010","dir":"Changelog","previous_headings":"","what":"timetk 0.1.0","title":"timetk 0.1.0","text":"CRAN release: 2017-07-25 Renamed package timetk. formerly timekit. Fixed issue back-ticked date columns Update pkgdown support robets","code":""}]
